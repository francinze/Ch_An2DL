{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ffa1054",
   "metadata": {},
   "source": [
    "# Kaggle & Colab Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e4c0e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# KAGGLE IMPORTS\n",
    "# Clone repo\n",
    "!git clone https://github.com/francinze/Ch_An2DL.git /kaggle/working/ch2\n",
    "\n",
    "# Install kaggle API\n",
    "!pip install -q kaggle\n",
    "\n",
    "# Configure kaggle.json\n",
    "!mkdir -p /root/.config/kaggle\n",
    "\n",
    "# Copy your kaggle.json there\n",
    "!cp /kaggle/working/ch2/kaggle.json /root/.config/kaggle/\n",
    "\n",
    "# Set correct permissions\n",
    "!chmod 600 /root/.config/kaggle/kaggle.json\n",
    "\n",
    "# Move into the working directory\n",
    "%cd /kaggle/working/ch2/\n",
    "\n",
    "!mkdir -p data\n",
    "!mkdir -p models\n",
    "\n",
    "# Download competition files WITH CORRECT PATH\n",
    "!kaggle competitions download -c an2dl2526c2v2 -p ./data/\n",
    "\n",
    "# Unzip dataset WITH CORRECT PATH\n",
    "!unzip -o ./data/an2dl2526c2v2.zip -d ./data/\n",
    "\n",
    "# Verify download\n",
    "!ls -la ./data/\n",
    "!echo \"Download complete!\"\n",
    "\n",
    "rm -rf ./data/an2dl2526c2v2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b1cef6",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%capture\n",
    "# COLAB IMPORTS\n",
    "!git clone https://github.com/francinze/Ch_An2DL.git\n",
    "! pip install -q kaggle\n",
    "! mkdir ~/.kaggle\n",
    "! cp Ch_An2DL/kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "%cd /content/Ch_An2DL/\n",
    "!mkdir data\n",
    "!mkdir models\n",
    "!kaggle competitions download -c an2dl2526c2v2 -p /data\n",
    "!unzip -o /data/an2dl2526c2v2.zip -d /data/\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687201fe",
   "metadata": {},
   "source": [
    "#  Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8eea66",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ccb0d",
   "metadata": {},
   "source": [
    "## Organize Data by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a05d7a",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Detect environment and set appropriate path prefix\n",
    "if os.path.exists('./data/train_data'):\n",
    "    PATH_PREFIX = './'\n",
    "    print(\"âœ“ Found ./data/train_data (Local or Colab)\")\n",
    "elif os.path.exists('/data/train_data'):\n",
    "    PATH_PREFIX = '/'\n",
    "    print(\"âœ“ Found /data/train_data (Kaggle)\")\n",
    "elif os.path.exists('data/train_data'):\n",
    "    PATH_PREFIX = ''\n",
    "    print(\"âœ“ Found data/train_data (Current directory)\")\n",
    "else:\n",
    "    print(\"âœ— Data not found in expected locations!\")\n",
    "    PATH_PREFIX = '/'\n",
    "\n",
    "print(f\"Using PATH_PREFIX: {PATH_PREFIX}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ORGANIZING DATA INTO SEPARATE DIRECTORIES BY TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define source directories\n",
    "train_data_dir = PATH_PREFIX + 'data/train_data/'\n",
    "test_data_dir = PATH_PREFIX + 'data/test_data/'\n",
    "\n",
    "# Define target directories for organized data\n",
    "train_img_dir = PATH_PREFIX + 'data/train_img/'\n",
    "train_mask_dir = PATH_PREFIX + 'data/train_mask/'\n",
    "test_img_dir = PATH_PREFIX + 'data/test_img/'\n",
    "test_mask_dir = PATH_PREFIX + 'data/test_mask/'\n",
    "\n",
    "train_labels = pd.read_csv(PATH_PREFIX + 'data/train_labels.csv')\n",
    "\n",
    "# Create target directories if they don't exist\n",
    "for directory in [train_img_dir, train_mask_dir, test_img_dir, test_mask_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Function to organize files by type\n",
    "def organize_data_by_type(source_dir, img_dir, mask_dir):\n",
    "    \"\"\"\n",
    "    Move image and mask files from source directory to separate directories.\n",
    "    Only moves files if they don't already exist in the target directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(source_dir):\n",
    "        print(f\"âš  Warning: Source directory not found: {source_dir}\")\n",
    "        return 0, 0\n",
    "    \n",
    "    files = os.listdir(source_dir)\n",
    "    img_count = 0\n",
    "    mask_count = 0\n",
    "    \n",
    "    for filename in files:\n",
    "        source_path = os.path.join(source_dir, filename)\n",
    "        \n",
    "        # Skip if not a file\n",
    "        if not os.path.isfile(source_path):\n",
    "            continue\n",
    "        \n",
    "        # Determine target directory based on filename prefix\n",
    "        if filename.startswith('img_'):\n",
    "            target_path = os.path.join(img_dir, filename)\n",
    "            if not os.path.exists(target_path):\n",
    "                shutil.copy2(source_path, target_path)\n",
    "                img_count += 1\n",
    "        elif filename.startswith('mask_'):\n",
    "            target_path = os.path.join(mask_dir, filename)\n",
    "            if not os.path.exists(target_path):\n",
    "                shutil.copy2(source_path, target_path)\n",
    "                mask_count += 1\n",
    "    \n",
    "    return img_count, mask_count\n",
    "\n",
    "# Organize training data\n",
    "print(\"\\nOrganizing training data...\")\n",
    "train_img_moved, train_mask_moved = organize_data_by_type(\n",
    "    train_data_dir, train_img_dir, train_mask_dir\n",
    ")\n",
    "print(f\"  Images: {train_img_moved} files copied to {train_img_dir}\")\n",
    "print(f\"  Masks: {train_mask_moved} files copied to {train_mask_dir}\")\n",
    "\n",
    "# Organize test data\n",
    "print(\"\\nOrganizing test data...\")\n",
    "test_img_moved, test_mask_moved = organize_data_by_type(\n",
    "    test_data_dir, test_img_dir, test_mask_dir\n",
    ")\n",
    "print(f\"  Images: {test_img_moved} files copied to {test_img_dir}\")\n",
    "print(f\"  Masks: {test_mask_moved} files copied to {test_mask_dir}\")\n",
    "\n",
    "# Verify organization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA ORGANIZATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train images: {len(os.listdir(train_img_dir)) if os.path.exists(train_img_dir) else 0} files in {train_img_dir}\")\n",
    "print(f\"Train masks: {len(os.listdir(train_mask_dir)) if os.path.exists(train_mask_dir) else 0} files in {train_mask_dir}\")\n",
    "print(f\"Test images: {len(os.listdir(test_img_dir)) if os.path.exists(test_img_dir) else 0} files in {test_img_dir}\")\n",
    "print(f\"Test masks: {len(os.listdir(test_mask_dir)) if os.path.exists(test_mask_dir) else 0} files in {test_mask_dir}\")\n",
    "print(\"=\"*80)\n",
    "print(\"Data organization complete!\")\n",
    "print(\"  - Organized copies are in train_img/, train_mask/, test_img/, test_mask/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af21e5c",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af5838",
   "metadata": {},
   "source": [
    "## Remove Shrek & Slimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef36cc",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parse the contaminated indices from the text file\n",
    "contaminated_indices = []\n",
    "with open('shrek_and_slimes.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and line.isdigit():\n",
    "            contaminated_indices.append(int(line))\n",
    "\n",
    "print(f\"Found {len(contaminated_indices)} contaminated samples to remove\")\n",
    "\n",
    "# Define directories to clean (both img and mask directories)\n",
    "train_img_dir_clean = PATH_PREFIX + 'data/train_img/'\n",
    "train_mask_dir_clean = PATH_PREFIX + 'data/train_mask/'\n",
    "\n",
    "# Remove corresponding image and mask files from both directories\n",
    "removed_count = 0\n",
    "for idx in contaminated_indices:\n",
    "    img_name = f'img_{idx:04d}.png'\n",
    "    mask_name = f'mask_{idx:04d}.png'\n",
    "    \n",
    "    # Remove from train_img directory\n",
    "    img_path = os.path.join(train_img_dir_clean, img_name)\n",
    "    if os.path.exists(img_path):\n",
    "        os.remove(img_path)\n",
    "        removed_count += 1\n",
    "    \n",
    "    # Remove from train_mask directory\n",
    "    mask_path = os.path.join(train_mask_dir_clean, mask_name)\n",
    "    if os.path.exists(mask_path):\n",
    "        os.remove(mask_path)\n",
    "        removed_count += 1\n",
    "\n",
    "print(f\"Removed {removed_count} files from organized directories\")\n",
    "\n",
    "# Update train_labels by removing contaminated indices\n",
    "train_labels = train_labels[~train_labels['sample_index'].str.extract(r'(\\d+)')[0].astype(int).isin(contaminated_indices)]\n",
    "print(f\"Training labels updated: {len(train_labels)} samples remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4e14e-b5c7-4134-a63a-108cccf383df",
   "metadata": {},
   "source": [
    "## Masks as Focus Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4165b80-ee19-4d1f-80e0-c61896dc68fb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# === FUNZIONE CHE APPLICA LA MASK ALLâ€™IMMAGINE ===\n",
    "def apply_mask(image, mask):\n",
    "    \"\"\"\n",
    "    image: PIL RGB image\n",
    "    mask: PIL grayscale mask (0 = nero, 255 = bianco)\n",
    "    return: masked image (PIL)\n",
    "    \"\"\"\n",
    "\n",
    "    # Converti in numpy\n",
    "    img_np = np.array(image).astype(np.uint8)\n",
    "    mask_np = np.array(mask).astype(np.uint8)\n",
    "\n",
    "    # Normalizza la mask a 0â€“1\n",
    "    mask_np = mask_np / 255.0\n",
    "\n",
    "    # Se lâ€™immagine ha 3 canali, estendi la mask\n",
    "    if img_np.ndim == 3:\n",
    "        mask_np = np.expand_dims(mask_np, axis=-1)\n",
    "\n",
    "    # Moltiplica â†’ le zone nere diventano 0 (nero)\n",
    "    masked_img_np = (img_np * mask_np).astype(np.uint8)\n",
    "\n",
    "    return Image.fromarray(masked_img_np)\n",
    "\n",
    "# === VISUALIZZA A VIDEO ALCUNI ESEMPI ===\n",
    "samples = sorted(os.listdir(train_img_dir))[:4]  # primi 4 esempi\n",
    "\n",
    "fig, axes = plt.subplots(len(samples), 2, figsize=(12, 2 * len(samples)))\n",
    "\n",
    "for i, img_name in enumerate(samples):\n",
    "\n",
    "    # Carica immagine e mask corrispondente\n",
    "    img_path = os.path.join(train_img_dir, img_name)\n",
    "    mask_path = os.path.join(train_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "    # Applica la mask\n",
    "    masked_image = apply_mask(image, mask)\n",
    "\n",
    "    # --- Plot ---\n",
    "    axes[i, 0].imshow(image)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    axes[i, 1].imshow(masked_image)\n",
    "    axes[i, 1].set_title(\"Masked Image\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed302cc",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Save masked images to a new directory\n",
    "masked_train_img_dir = PATH_PREFIX + 'data/train_img_masked/'\n",
    "os.makedirs(masked_train_img_dir, exist_ok=True)\n",
    "masked_test_img_dir = PATH_PREFIX + 'data/test_img_masked/'\n",
    "os.makedirs(masked_test_img_dir, exist_ok=True)\n",
    "\n",
    "# Apply masking to all training images and save\n",
    "train_img_files = sorted(os.listdir(train_img_dir))\n",
    "\n",
    "for img_name in tqdm(train_img_files, desc=\"Processing training images\"):\n",
    "    img_path = os.path.join(train_img_dir, img_name)\n",
    "    mask_path = os.path.join(train_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "    masked_image = apply_mask(image, mask)\n",
    "    masked_image.save(os.path.join(masked_train_img_dir, img_name))\n",
    "print(\"Masked training images saved.\")\n",
    "\n",
    "# Apply masking to all test images and save\n",
    "test_img_files = sorted(os.listdir(test_img_dir))\n",
    "for img_name in tqdm(test_img_files, desc=\"Processing test images\"):\n",
    "    img_path = os.path.join(test_img_dir, img_name)\n",
    "    mask_path = os.path.join(test_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "    masked_image = apply_mask(image, mask)\n",
    "    masked_image.save(os.path.join(masked_test_img_dir, img_name))\n",
    "\n",
    "print(\"Masked test images saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f213e",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b4216",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Analyze class distribution after removal\n",
    "class_distribution = train_labels['label'].value_counts().sort_index()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Class Distribution After Removal of Contaminated Images\")\n",
    "print(\"=\"*60)\n",
    "print(class_distribution)\n",
    "print(f\"\\nTotal samples: {len(train_labels)}\")\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICS FOR AUGMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Class with the most samples (majority)\n",
    "max_class = class_distribution.max()\n",
    "max_class_name = class_distribution.idxmax()\n",
    "print(f\"\\nClass with the most samples (Majority): {max_class_name} ({max_class} samples)\")\n",
    "\n",
    "# Class with the fewest samples (minority)\n",
    "min_class = class_distribution.min()\n",
    "min_class_name = class_distribution.idxmin()\n",
    "print(f\"Class with the fewest samples (Minority): {min_class_name} ({min_class} samples)\")\n",
    "\n",
    "# Imbalance ratio\n",
    "imbalance_ratio = max_class / min_class\n",
    "print(f\"\\nImbalance ratio (Max/Min): {imbalance_ratio:.2f}x\")\n",
    "\n",
    "# Augmentation proposal\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDED AUGMENTATION STRATEGY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAugmentations to apply (as suggested by the professor):\")\n",
    "print(\"  1. Horizontal Flip (p=0.5)\")\n",
    "print(\"  2. Vertical Flip (p=0.5)\")\n",
    "print(\"  3. Random Translation (0.2, 0.2)\")\n",
    "print(\"  4. Random Zoom/Scale (0.8, 1.2)\")\n",
    "print(\"  [EXCLUDE: Random Rotation - would change dimensions]\\n\")\n",
    "\n",
    "# STRATEGY: All classes grow until reaching the same target number for ALL\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BALANCED STRATEGY: ALL CLASSES GROW TO A FIXED AND EQUAL NUMBER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ===== MODIFY THE TARGET NUMBER OF SAMPLES HERE =====\n",
    "target_samples = 250  # Desired number of samples for EACH class\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\nTarget: {target_samples} samples for EACH class\")\n",
    "\n",
    "augmentation_strategy_balanced = {}\n",
    "total_to_generate = 0\n",
    "\n",
    "for class_name in class_distribution.index:\n",
    "    n_samples = class_distribution[class_name]\n",
    "    n_needed = target_samples - n_samples\n",
    "    n_augmentations = max(0, n_needed)  # We cannot have negative augmentations\n",
    "    \n",
    "    augmentation_strategy_balanced[class_name] = {\n",
    "        'original': n_samples,\n",
    "        'target': target_samples,\n",
    "        'augment_count': n_augmentations,\n",
    "        'ratio_multiplier': n_augmentations / n_samples if n_samples > 0 else 0\n",
    "    }\n",
    "    \n",
    "    total_to_generate += n_augmentations\n",
    "\n",
    "# Projection of the dataset after augmentation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET AFTER BALANCED AUGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Class':<20} {'Original':<15} {'New Augment':<15} {'Augmentations per image':<25} {'Total':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "total_original = 0\n",
    "total_augmented = 0\n",
    "for class_name in class_distribution.index:\n",
    "    n_original = class_distribution[class_name]\n",
    "    n_aug = augmentation_strategy_balanced[class_name]['augment_count']\n",
    "    n_total = n_original + n_aug\n",
    "    \n",
    "    total_original += n_original\n",
    "    total_augmented += n_total\n",
    "    \n",
    "    print(f\"{class_name:<20} {n_original:<15} {n_aug:<15} {augmentation_strategy_balanced[class_name]['ratio_multiplier']:<25.2f} {n_total:<15}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'TOTAL':<20} {total_original:<15} {total_to_generate:<15} {np.mean([augmentation_strategy_balanced[class_name]['ratio_multiplier'] for class_name in class_distribution.index]):<25.2f} {total_augmented:<15}\")\n",
    "\n",
    "# Visualize the distribution before and after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before\n",
    "class_distribution.plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Class Distribution - BEFORE Augmentation', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of samples')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].axhline(y=target_samples, color='red', linestyle='--', linewidth=2, label=f'Target: {target_samples}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# After\n",
    "after_augmentation_balanced = {}\n",
    "for class_name in class_distribution.index:\n",
    "    after_augmentation_balanced[class_name] = augmentation_strategy_balanced[class_name]['target']\n",
    "\n",
    "after_series = pd.Series(after_augmentation_balanced)\n",
    "after_series.plot(kind='bar', ax=axes[1], color='seagreen')\n",
    "axes[1].set_title('Class Distribution - AFTER Balanced Augmentation', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of samples')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].axhline(y=target_samples, color='red', linestyle='--', linewidth=2, label=f'Target: {target_samples}')\n",
    "axes[1].set_ylim([0, max_class * 1.1])\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc789e54",
   "metadata": {},
   "source": [
    "## Generative Mask Conditioning\n",
    "\n",
    "**Philosophy**: *\"Few images you have? But the shapes (masks) you possess. Use a generative model, conditioned on these masks, to dream new textures into old forms.\"*\n",
    "\n",
    "### Strategy: Synthesize New Textures from Existing Masks\n",
    "\n",
    "We'll use **Stable Diffusion + ControlNet** to generate synthetic images:\n",
    "1. Use **existing masks** as spatial conditioning\n",
    "2. Generate **new texture variations** while preserving anatomy (mask shape)\n",
    "3. Create diverse \"skins\" for the same underlying structure\n",
    "4. Then apply geometric augmentation to both real + synthetic images\n",
    "\n",
    "**Pipeline**:\n",
    "```\n",
    "Original Images + Masks\n",
    "    â†“\n",
    "[GENERATIVE] Generate synthetic images from masks\n",
    "    â†“\n",
    "Real + Synthetic Images (with masks)\n",
    "    â†“\n",
    "[AUGMENTATION] Geometric transforms on all\n",
    "    â†“\n",
    "Maximum Diversity Dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c018cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "GENERATE_SYNTHETIC = True  # Set to False to skip generation (if already done)\n",
    "SYNTHETIC_MULTIPLIER = 3   # Generate N synthetic images per real image\n",
    "# ===== MULTI-GPU SETUP =====\n",
    "# Check for multiple GPUs and set up DataParallel\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f\"Found {num_gpus} GPU(s) available:\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\" GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    if num_gpus > 1:\n",
    "        print(f\"Multi-GPU training enabled: Will use {num_gpus} GPUs with DataParallel\")\n",
    "    else:\n",
    "        print(f\"Single GPU training\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    num_gpus = 0\n",
    "    print(\"No GPU available, using CPU\")\n",
    "# ===========================\n",
    "\n",
    "\n",
    "# Create directory for synthetic images and masks\n",
    "synthetic_img_dir = PATH_PREFIX + 'data/train_img_synthetic/'\n",
    "synthetic_mask_dir = PATH_PREFIX + 'data/train_mask_synthetic/'\n",
    "\n",
    "os.makedirs(synthetic_img_dir, exist_ok=True)\n",
    "os.makedirs(synthetic_mask_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATIVE MASK CONDITIONING SETUP (Pix2Pix GAN)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Synthetic image directory: {synthetic_img_dir}\")\n",
    "print(f\"Synthetic mask directory: {synthetic_mask_dir}\")\n",
    "print(f\"Synthetic multiplier: {SYNTHETIC_MULTIPLIER}x per image\")\n",
    "print(f\"Using GPU: {num_gpus > 0}\")\n",
    "print(f\"Method: Lightweight Pix2Pix GAN (~50MB)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c6ab4",
   "metadata": {},
   "source": [
    "### Lightweight GAN Approach (Pix2Pix)\n",
    "\n",
    "**Why Pix2Pix instead of Stable Diffusion?**\n",
    "\n",
    "- **Size**: ~50MB vs ~4GB (80x smaller!)\n",
    "- **Speed**: Much faster inference\n",
    "- **No downloads**: No need to download huge pretrained models\n",
    "- **Works on CPU**: Can run without GPU if needed\n",
    "- **Same goal**: Generate new textures from mask guidance\n",
    "\n",
    "The GAN learns to translate masks â†’ realistic tissue textures, achieving the organizer's vision of \"dreaming new textures into old forms.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_SYNTHETIC:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LIGHTWEIGHT PIX2PIX GAN SETUP\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Building simple conditional GAN for mask-to-image translation\")\n",
    "    print(\"Benefits:\")\n",
    "    print(\"  - No large model downloads (~50MB vs 4GB)\")\n",
    "    print(\"  - Runs on CPU if needed\")\n",
    "    print(\"  - Fast inference\")\n",
    "    print(\"  - Uses texture/color augmentation guided by masks\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Define Pix2Pix Generator (U-Net architecture)\n",
    "    class UNetGenerator(nn.Module):\n",
    "        \"\"\"Lightweight U-Net generator for Pix2Pix\"\"\"\n",
    "        def __init__(self, in_channels=1, out_channels=3):\n",
    "            super(UNetGenerator, self).__init__()\n",
    "            \n",
    "            # Encoder (downsampling)\n",
    "            self.enc1 = self.conv_block(in_channels, 64, normalize=False)\n",
    "            self.enc2 = self.conv_block(64, 128)\n",
    "            self.enc3 = self.conv_block(128, 256)\n",
    "            self.enc4 = self.conv_block(256, 512)\n",
    "            \n",
    "            # Decoder (upsampling) with skip connections\n",
    "            self.dec1 = self.upconv_block(512, 256)\n",
    "            self.dec2 = self.upconv_block(512, 128)  # 512 = 256 + 256 from skip\n",
    "            self.dec3 = self.upconv_block(256, 64)   # 256 = 128 + 128 from skip\n",
    "            self.dec4 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(128, out_channels, 4, 2, 1),  # 128 = 64 + 64 from skip\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        \n",
    "        def conv_block(self, in_ch, out_ch, normalize=True):\n",
    "            layers = [nn.Conv2d(in_ch, out_ch, 4, 2, 1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_ch))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "        def upconv_block(self, in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Encoder\n",
    "            e1 = self.enc1(x)\n",
    "            e2 = self.enc2(e1)\n",
    "            e3 = self.enc3(e2)\n",
    "            e4 = self.enc4(e3)\n",
    "            \n",
    "            # Decoder with skip connections\n",
    "            d1 = self.dec1(e4)\n",
    "            d2 = self.dec2(torch.cat([d1, e3], 1))\n",
    "            d3 = self.dec3(torch.cat([d2, e2], 1))\n",
    "            d4 = self.dec4(torch.cat([d3, e1], 1))\n",
    "            \n",
    "            return d4\n",
    "    \n",
    "    # Initialize generator\n",
    "    generator = UNetGenerator(in_channels=1, out_channels=3)\n",
    "    if num_gpus > 0:\n",
    "        generator = generator.cuda()\n",
    "    \n",
    "    # Initialize with random weights (or load pretrained if available)\n",
    "    generator.eval()  # Set to eval mode for inference\n",
    "    \n",
    "    print(\"âœ“ Pix2Pix Generator initialized\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "    print(f\"  Size: ~{sum(p.numel() for p in generator.parameters()) * 4 / 1024 / 1024:.1f} MB\")\n",
    "    print(f\"  Device: {'GPU' if num_gpus > 0 else 'CPU'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"GENERATE_SYNTHETIC=False, skipping model loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ea19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color/texture variations for synthetic generation\n",
    "# Since GAN starts with random weights, we enhance with classical augmentations\n",
    "class_color_profiles = {\n",
    "    'Triple negative': {\n",
    "        'hue_range': (-10, 10),\n",
    "        'saturation_mult': (0.9, 1.2),\n",
    "        'brightness_mult': (0.85, 1.15),\n",
    "        'contrast_mult': (0.9, 1.1)\n",
    "    },\n",
    "    'Luminal A': {\n",
    "        'hue_range': (-15, 15),\n",
    "        'saturation_mult': (0.8, 1.1),\n",
    "        'brightness_mult': (0.9, 1.1),\n",
    "        'contrast_mult': (0.95, 1.05)\n",
    "    },\n",
    "    'Luminal B': {\n",
    "        'hue_range': (-12, 12),\n",
    "        'saturation_mult': (0.85, 1.15),\n",
    "        'brightness_mult': (0.88, 1.12),\n",
    "        'contrast_mult': (0.92, 1.08)\n",
    "    },\n",
    "    'HER2(+)': {\n",
    "        'hue_range': (-8, 8),\n",
    "        'saturation_mult': (0.95, 1.25),\n",
    "        'brightness_mult': (0.9, 1.1),\n",
    "        'contrast_mult': (0.95, 1.1)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nâœ“ Class-specific color profiles defined\")\n",
    "print(f\"  Classes: {list(class_color_profiles.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mask_for_gan(mask_path, target_size=256):\n",
    "    \"\"\"\n",
    "    Prepare mask for GAN input.\n",
    "    Converts to tensor format and normalizes.\n",
    "    \"\"\"\n",
    "    mask = Image.open(mask_path).convert('L')\n",
    "    mask = mask.resize((target_size, target_size), Image.BILINEAR)\n",
    "    mask_array = np.array(mask, dtype=np.float32) / 255.0\n",
    "    mask_tensor = torch.from_numpy(mask_array).unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]\n",
    "    \n",
    "    if USE_GPU:\n",
    "        mask_tensor = mask_tensor.cuda()\n",
    "    \n",
    "    return mask_tensor\n",
    "\n",
    "def apply_color_jitter(img, color_profile):\n",
    "    \"\"\"Apply color jittering based on class-specific profile\"\"\"\n",
    "    img_hsv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "    \n",
    "    # Hue shift\n",
    "    hue_shift = np.random.uniform(*color_profile['hue_range'])\n",
    "    img_hsv[:, :, 0] = (img_hsv[:, :, 0] + hue_shift) % 180\n",
    "    \n",
    "    # Saturation multiplication\n",
    "    sat_mult = np.random.uniform(*color_profile['saturation_mult'])\n",
    "    img_hsv[:, :, 1] = np.clip(img_hsv[:, :, 1] * sat_mult, 0, 255)\n",
    "    \n",
    "    # Convert back to RGB\n",
    "    img_rgb = cv2.cvtColor(img_hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    # Brightness and contrast\n",
    "    brightness_mult = np.random.uniform(*color_profile['brightness_mult'])\n",
    "    contrast_mult = np.random.uniform(*color_profile['contrast_mult'])\n",
    "    \n",
    "    img_rgb = np.clip(img_rgb * brightness_mult, 0, 255).astype(np.uint8)\n",
    "    img_rgb = np.clip((img_rgb - 128) * contrast_mult + 128, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return Image.fromarray(img_rgb)\n",
    "\n",
    "def generate_synthetic_image_gan(mask_tensor, original_img, class_label, generator):\n",
    "    \"\"\"\n",
    "    Generate synthetic image using GAN + color augmentation\n",
    "    \n",
    "    Args:\n",
    "        mask_tensor: Tensor of mask [1, 1, H, W]\n",
    "        original_img: PIL Image of original (for reference)\n",
    "        class_label: Cancer subtype label\n",
    "        generator: UNetGenerator model\n",
    "    \n",
    "    Returns:\n",
    "        PIL Image of generated synthetic image\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Generate base image from mask using GAN\n",
    "        generated = generator(mask_tensor)  # Output: [-1, 1]\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        generated_np = generated.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        generated_np = ((generated_np + 1) / 2 * 255).astype(np.uint8)  # [-1,1] -> [0,255]\n",
    "        synthetic_img = Image.fromarray(generated_np)\n",
    "        \n",
    "        # Apply class-specific color augmentation\n",
    "        color_profile = class_color_profiles[class_label]\n",
    "        synthetic_img = apply_color_jitter(synthetic_img, color_profile)\n",
    "        \n",
    "        # Blend with original for better texture (optional, helps with untrained GAN)\n",
    "        alpha = np.random.uniform(0.3, 0.7)  # Random blending\n",
    "        synthetic_img = Image.blend(original_img.resize(synthetic_img.size), synthetic_img, alpha)\n",
    "    \n",
    "    return synthetic_img\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")\n",
    "print(\"  - prepare_mask_for_gan(): Convert masks to GAN tensor format\")\n",
    "print(\"  - apply_color_jitter(): Class-specific color augmentation\")\n",
    "print(\"  - generate_synthetic_image_gan(): GAN-based generation with blending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a236e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_SYNTHETIC:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING SYNTHETIC IMAGES FROM MASKS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Generating {SYNTHETIC_MULTIPLIER} synthetic images per real image\")\n",
    "    print(f\"Total real images: {len(train_labels)}\")\n",
    "    print(f\"Expected synthetic images: {len(train_labels) * SYNTHETIC_MULTIPLIER}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    total_generated = 0\n",
    "    \n",
    "    for idx, row in train_labels.iterrows():\n",
    "        img_name = row['sample_index']\n",
    "        class_label = row['label']\n",
    "        \n",
    "        # Load original mask and image\n",
    "        mask_name = img_name.replace('img_', 'mask_')\n",
    "        mask_path = os.path.join(train_mask_dir, mask_name)\n",
    "        img_path = os.path.join(train_img_dir, img_name)\n",
    "        \n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"  âš  Mask not found: {mask_name}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"  âš  Image not found: {img_name}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Load original image\n",
    "        original_img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Prepare mask for GAN\n",
    "        mask_tensor = prepare_mask_for_gan(mask_path)\n",
    "        \n",
    "        # Generate multiple synthetic versions\n",
    "        for syn_idx in range(SYNTHETIC_MULTIPLIER):\n",
    "            try:\n",
    "                # Generate synthetic image using GAN\n",
    "                synthetic_img = generate_synthetic_image_gan(\n",
    "                    generator=generator,\n",
    "                    mask_tensor=mask_tensor,\n",
    "                    original_img=original_img,\n",
    "                    class_label=class_label,\n",
    "                )\n",
    "                \n",
    "                # Save synthetic image\n",
    "                base_name = img_name.replace('.png', '')\n",
    "                synthetic_img_name = f\"{base_name}_syn_{syn_idx}.png\"\n",
    "                synthetic_img_path = os.path.join(synthetic_img_dir, synthetic_img_name)\n",
    "                synthetic_img.save(synthetic_img_path)\n",
    "                \n",
    "                # Copy mask for this synthetic image (same mask, new image)\n",
    "                synthetic_mask_name = f\"mask_{img_name.split('_')[1].replace('.png', '')}_syn_{syn_idx}.png\"\n",
    "                synthetic_mask_path = os.path.join(synthetic_mask_dir, synthetic_mask_name)\n",
    "                Image.open(mask_path).save(synthetic_mask_path)\n",
    "                \n",
    "                total_generated += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Error generating image at index {syn_idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Progress update\n",
    "        if (idx + 1) % 10 == 0 or idx == len(train_labels) - 1:\n",
    "            print(f\"  Progress: {idx + 1}/{len(train_labels)} images processed ({total_generated} synthetic images generated)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SYNTHETIC IMAGE GENERATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total synthetic images generated: {total_generated}\")\n",
    "    print(f\"Saved to: {synthetic_img_dir}\")\n",
    "    print(f\"Corresponding masks saved to: {synthetic_mask_dir}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    if num_gpus > 0:\n",
    "        del generator\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"\\nâœ“ GPU memory cleared\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nGENERATE_SYNTHETIC=False, skipping generation\")\n",
    "    print(\"To generate synthetic images, set GENERATE_SYNTHETIC=True and run this cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e021505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some synthetic examples (if they exist)\n",
    "if os.path.exists(synthetic_img_dir) and len(os.listdir(synthetic_img_dir)) > 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Get a few synthetic images\n",
    "    synthetic_files = sorted(os.listdir(synthetic_img_dir))[:6]\n",
    "    \n",
    "    if len(synthetic_files) > 0:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, syn_img_name in enumerate(synthetic_files):\n",
    "            if idx >= 6:\n",
    "                break\n",
    "            \n",
    "            # Load synthetic image\n",
    "            syn_img_path = os.path.join(synthetic_img_dir, syn_img_name)\n",
    "            syn_img = Image.open(syn_img_path)\n",
    "            \n",
    "            # Load corresponding mask\n",
    "            syn_mask_name = syn_img_name.replace('img_', 'mask_')\n",
    "            syn_mask_path = os.path.join(synthetic_mask_dir, syn_mask_name)\n",
    "            syn_mask = Image.open(syn_mask_path)\n",
    "            \n",
    "            # Find original image for comparison\n",
    "            base_name = syn_img_name.split('_syn_')[0] + '.png'\n",
    "            orig_img_path = os.path.join(train_img_dir, base_name)\n",
    "            \n",
    "            # Create composite visualization\n",
    "            composite = Image.new('RGB', (syn_img.width * 3, syn_img.height))\n",
    "            \n",
    "            if os.path.exists(orig_img_path):\n",
    "                orig_img = Image.open(orig_img_path)\n",
    "                composite.paste(orig_img, (0, 0))\n",
    "            \n",
    "            composite.paste(syn_mask.convert('RGB'), (syn_img.width, 0))\n",
    "            composite.paste(syn_img, (syn_img.width * 2, 0))\n",
    "            \n",
    "            axes[idx].imshow(composite)\n",
    "            axes[idx].set_title(f\"Original | Mask | Synthetic\\n{base_name}\")\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(\"Generative Mask Conditioning Examples\", fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nâœ“ Showing {min(6, len(synthetic_files))} synthetic examples\")\n",
    "        print(\"  Left: Original image | Center: Mask (guide) | Right: Synthetic image\")\n",
    "else:\n",
    "    print(\"No synthetic images to visualize. Generate them first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1b076",
   "metadata": {},
   "source": [
    "### Update Dataset with Synthetic Images\n",
    "\n",
    "Now we'll combine real + synthetic images before applying geometric augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ca9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create expanded dataset including synthetic images\n",
    "train_labels_expanded = train_labels.copy()\n",
    "\n",
    "# Add synthetic images if they exist\n",
    "if os.path.exists(synthetic_img_dir) and len(os.listdir(synthetic_img_dir)) > 0:\n",
    "    synthetic_img_files = os.listdir(synthetic_img_dir)\n",
    "    print(f\"Found {len(synthetic_img_files)} synthetic images\")\n",
    "    \n",
    "    synthetic_rows = []\n",
    "    for syn_img_name in synthetic_img_files:\n",
    "        # Extract original file name: img_XXXX_syn_N.png -> img_XXXX.png\n",
    "        base_name = syn_img_name.split('_syn_')[0] + '.png'\n",
    "        \n",
    "        # Find corresponding class label\n",
    "        original_row = train_labels[train_labels['sample_index'] == base_name]\n",
    "        if not original_row.empty:\n",
    "            class_label = original_row.iloc[0]['label']\n",
    "            synthetic_rows.append({'sample_index': syn_img_name, 'label': class_label, 'source': 'synthetic'})\n",
    "    \n",
    "    synthetic_df = pd.DataFrame(synthetic_rows)\n",
    "    train_labels.loc[:, 'source'] = 'real'  # Mark original images\n",
    "    train_labels_expanded = pd.concat([train_labels, synthetic_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nDataset expansion complete:\")\n",
    "    print(f\"  Real images: {len(train_labels)}\")\n",
    "    print(f\"  Synthetic images: {len(synthetic_df)}\")\n",
    "    print(f\"  Total: {len(train_labels_expanded)}\")\n",
    "    print(f\"  Expansion ratio: {len(train_labels_expanded) / len(train_labels):.2f}x\")\n",
    "    \n",
    "    print(f\"\\nClass distribution (Real + Synthetic):\")\n",
    "    print(train_labels_expanded['label'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"No synthetic images found. Using only real images.\")\n",
    "    print(\"Generate synthetic images first by setting GENERATE_SYNTHETIC=True\")\n",
    "\n",
    "# This expanded dataset will be used for augmentation\n",
    "# So we get: Real + Synthetic images â†’ Then all augmented geometrically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24828c3e",
   "metadata": {},
   "source": [
    "### Complete Data Augmentation Pipeline\n",
    "\n",
    "The notebook now implements **both** organizer's advice strategies:\n",
    "\n",
    "#### ðŸŽ¨ **ADVICE 11/12: Generative Mask Conditioning**\n",
    "- Uses Stable Diffusion + ControlNet to generate synthetic images\n",
    "- Masks guide the generation â†’ same anatomy, different textures\n",
    "- Creates `SYNTHETIC_MULTIPLIER` variations per real image\n",
    "- Result: ~3-4x dataset expansion with appearance diversity\n",
    "\n",
    "#### ðŸ”„ **Geometric Augmentation** \n",
    "- Applies to **both real AND synthetic** images\n",
    "- Flips, rotations maintain geometric diversity\n",
    "- Result: Further ~3-4x expansion\n",
    "\n",
    "#### ðŸ“Š **Combined Effect**:\n",
    "```\n",
    "Original: 100 images\n",
    "    â†“ Generative (3x)\n",
    "Expanded: 400 images (100 real + 300 synthetic)\n",
    "    â†“ Augmentation (balanced strategy)\n",
    "Final: ~1000+ images\n",
    "```\n",
    "\n",
    "This achieves the organizer's vision: *\"If the model can see the same object in a thousand new skins, robust its vision becomes.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722ec8b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# Create folders for augmented data if they don't exist\n",
    "# We need SEPARATE folders for augmented images and augmented masks (dual-path architecture)\n",
    "augmented_img_dir = PATH_PREFIX + f'data/train_img_augmented/'\n",
    "augmented_mask_dir = PATH_PREFIX + f'data/train_mask_augmented/'\n",
    "\n",
    "for aug_dir in [augmented_img_dir, augmented_mask_dir]:\n",
    "    if not os.path.exists(aug_dir):\n",
    "        os.makedirs(aug_dir)\n",
    "        print(f\"Created directory: {aug_dir}\")\n",
    "    else:\n",
    "        existing_files = len(os.listdir(aug_dir))\n",
    "        print(f\"Directory already exists: {aug_dir}\")\n",
    "        print(f\"Found {existing_files} existing augmented files\")\n",
    "\n",
    "# For backward compatibility (old augmented_dir variable)\n",
    "augmented_dir = augmented_img_dir\n",
    "\n",
    "# Define augmentations for each class\n",
    "# IMPORTANT: We apply the SAME transformations to BOTH image AND mask\n",
    "# This ensures geometric consistency for dual-path architecture\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING DUAL-PATH AUGMENTATION PROCESS\")\n",
    "print(\"Augmenting both IMAGES and MASKS with identical transforms\")\n",
    "print(\"Augmenting REAL + SYNTHETIC images\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Loop through each class and generate augmentations\n",
    "total_augmented = 0\n",
    "\n",
    "for class_name in sorted(augmentation_strategy_balanced.keys()):\n",
    "    info = augmentation_strategy_balanced[class_name]\n",
    "    n_augment = info['augment_count']\n",
    "    \n",
    "    if n_augment == 0:\n",
    "        print(f\"\\n{class_name}: No augmentation needed (already at target)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Class: {class_name}\")\n",
    "    print(f\"Augmentations to generate: {n_augment}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Get ALL images of this class (real + synthetic)\n",
    "    class_samples = train_labels_expanded[train_labels_expanded['label'] == class_name]['sample_index'].tolist()\n",
    "    n_total = len(class_samples)\n",
    "    \n",
    "    # Calculate how many augmentations per image (real + synthetic)\n",
    "    aug_per_img = n_augment / n_total\n",
    "    \n",
    "    # For each image (real or synthetic)\n",
    "    aug_count = 0\n",
    "    for img_idx, img_name in enumerate(class_samples):\n",
    "        # Determine source directory (real or synthetic)\n",
    "        is_synthetic = '_syn_' in img_name\n",
    "        \n",
    "        if is_synthetic:\n",
    "            img_source_dir = synthetic_img_dir\n",
    "            mask_source_dir = synthetic_mask_dir\n",
    "        else:\n",
    "            img_source_dir = train_img_dir\n",
    "            mask_source_dir = train_mask_dir\n",
    "        \n",
    "        # Load BOTH the image AND mask\n",
    "        img_file = img_name\n",
    "        mask_file = img_name.replace('img_', 'mask_')\n",
    "        \n",
    "        img_path = os.path.join(img_source_dir, img_file)\n",
    "        mask_path = os.path.join(mask_source_dir, mask_file)\n",
    "        \n",
    "        if not os.path.exists(img_path) or not os.path.exists(mask_path):\n",
    "            print(f\"  File not found: {img_file} or {mask_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Load image and mask\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # Grayscale mask\n",
    "        \n",
    "        # Generate augmentations for this image-mask pair\n",
    "        n_to_generate = int(np.ceil(aug_per_img)) if img_idx < n_augment % n_total else int(np.floor(aug_per_img))\n",
    "        \n",
    "        for aug_num in range(n_to_generate):\n",
    "            if aug_count <= n_augment:\n",
    "                base_name = img_file.replace('.png', '')\n",
    "                mask_base_name = mask_file.replace('.png', '')\n",
    "                \n",
    "                # Set random seed to ensure SAME transformations for image and mask\n",
    "                seed = np.random.randint(2147483647)\n",
    "                \n",
    "                # Apply identical transformations to both image and mask\n",
    "                # Horizontal flip\n",
    "                torch.manual_seed(seed)\n",
    "                img_augmented = transforms.RandomHorizontalFlip(p=0.5)(img)\n",
    "                torch.manual_seed(seed)\n",
    "                mask_augmented = transforms.RandomHorizontalFlip(p=0.5)(mask)\n",
    "                \n",
    "                # Vertical flip\n",
    "                torch.manual_seed(seed)\n",
    "                img_augmented = transforms.RandomVerticalFlip(p=0.5)(img_augmented)\n",
    "                torch.manual_seed(seed)\n",
    "                mask_augmented = transforms.RandomVerticalFlip(p=0.5)(mask_augmented)\n",
    "                \n",
    "                # Random rotation (0, 90, 180, 270 degrees only to avoid interpolation artifacts)\n",
    "                rotation_angle = np.random.choice([0, 90, 180, 270])\n",
    "                img_augmented = transforms.functional.rotate(img_augmented, angle=int(rotation_angle), expand=True, fill=0)\n",
    "                mask_augmented = transforms.functional.rotate(mask_augmented, angle=int(rotation_angle), expand=True, fill=0)\n",
    "                \n",
    "                # Save augmented image and mask\n",
    "                augmented_img_name = f\"{base_name}_aug_{aug_num}.png\"\n",
    "                augmented_mask_name = f\"{mask_base_name}_aug_{aug_num}.png\"\n",
    "                \n",
    "                augmented_img_path = os.path.join(augmented_img_dir, augmented_img_name)\n",
    "                augmented_mask_path = os.path.join(augmented_mask_dir, augmented_mask_name)\n",
    "                \n",
    "                img_augmented.save(augmented_img_path)\n",
    "                mask_augmented.save(augmented_mask_path)\n",
    "                \n",
    "            aug_count += 1\n",
    "        \n",
    "        # Progress update\n",
    "        if (img_idx + 1) % max(1, n_total // 5) == 0 or img_idx == n_total - 1:\n",
    "            print(f\"  Processed {img_idx + 1}/{n_total} samples (real+synthetic) ({aug_count} augmentations generated)\")\n",
    "    \n",
    "    total_augmented += aug_count\n",
    "    print(f\"  {class_name}: Completed! {aug_count} image-mask pairs generated\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"AUGMENTATION COMPLETED!\")\n",
    "print(f\"Total augmented image-mask pairs generated: {total_augmented}\")\n",
    "print(f\"Images saved to: {augmented_img_dir}\")\n",
    "print(f\"Masks saved to: {augmented_mask_dir}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verify file count\n",
    "augmented_img_files = os.listdir(augmented_img_dir)\n",
    "augmented_mask_files = os.listdir(augmented_mask_dir)\n",
    "print(f\"\\nAugmented images: {len(augmented_img_files)}\")\n",
    "print(f\"Augmented masks: {len(augmented_mask_files)}\")\n",
    "\n",
    "# For backward compatibility\n",
    "augmented_files = augmented_img_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c11365",
   "metadata": {},
   "source": [
    "## Crop Masked Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3ab60",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Crop masked images to the extent of information\n",
    "# Now that we have applied the masks, we can crop the images to the bounding box of the non-zero regions in the masks.\n",
    "\n",
    "# Define minimum image size (will be used later in preprocessing)\n",
    "MIN_IMG_SIZE = (256, 256)\n",
    "def crop_to_mask(image, min_size=MIN_IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Crop the image to the bounding box of the non-zero regions.\n",
    "    Since the mask has already been applied to the image, we detect non-zero pixels directly from the image.\n",
    "    If the cropped image is smaller than min_size, add zero-padding to reach min_size.\n",
    "    \n",
    "    image: PIL RGB image (already masked, with black background)\n",
    "    mask: PIL grayscale mask (not used, kept for compatibility)\n",
    "    min_size: tuple (width, height) - minimum size for the output image\n",
    "    return: cropped and padded image (PIL)\n",
    "    \"\"\"\n",
    "    # Convert image to numpy array\n",
    "    img_np = np.array(image).astype(np.uint8)\n",
    "    \n",
    "    # Find non-zero pixels in any channel (R, G, or B)\n",
    "    # Sum across color channels and check where sum > 0\n",
    "    non_zero_mask = np.sum(img_np, axis=2) > 0\n",
    "    coords = np.column_stack(np.where(non_zero_mask))\n",
    "    \n",
    "    if coords.size == 0:\n",
    "        return image  # No cropping if image is completely black\n",
    "    \n",
    "    y_min, x_min = coords.min(axis=0)\n",
    "    y_max, x_max = coords.max(axis=0) + 1  # add 1 to include the max pixel\n",
    "    \n",
    "    cropped = image.crop((x_min, y_min, x_max, y_max))\n",
    "    \n",
    "    # Check if padding is needed\n",
    "    width, height = cropped.size\n",
    "    min_width, min_height = min_size\n",
    "    \n",
    "    if width < min_width or height < min_height:\n",
    "        # Calculate padding needed\n",
    "        pad_width = max(0, min_width - width)\n",
    "        pad_height = max(0, min_height - height)\n",
    "        \n",
    "        # Create new image with black padding\n",
    "        padded = Image.new('RGB', (max(width, min_width), max(height, min_height)), (0, 0, 0))\n",
    "        \n",
    "        # Paste cropped image in the center\n",
    "        paste_x = pad_width // 2\n",
    "        paste_y = pad_height // 2\n",
    "        padded.paste(cropped, (paste_x, paste_y))\n",
    "        \n",
    "        return padded\n",
    "    \n",
    "    return cropped\n",
    "\n",
    "# === VISUALIZZA A VIDEO ALCUNI ESEMPI DI CROP ===\n",
    "samples = sorted(os.listdir(masked_train_img_dir))[:4]  # primi 4 esempi\n",
    "fig, axes = plt.subplots(len(samples), 3, figsize=(12, 3 * len(samples)))\n",
    "\n",
    "for i, img_name in enumerate(samples):\n",
    "    # Carica immagine e mask corrispondente\n",
    "    img_path = os.path.join(masked_train_img_dir, img_name)\n",
    "    mask_path = os.path.join(train_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "    # Applica la mask\n",
    "    masked_image = apply_mask(image, mask)\n",
    "\n",
    "    # Esegui il crop con padding se necessario\n",
    "    cropped_image = crop_to_mask(masked_image)\n",
    "\n",
    "    # --- Plot ---\n",
    "    axes[i, 0].imshow(masked_image)\n",
    "    axes[i, 0].set_title(\"Masked Image\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    axes[i, 1].imshow(mask, cmap=\"gray\")\n",
    "    axes[i, 1].set_title(\"Mask\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "    axes[i, 2].imshow(cropped_image)\n",
    "    axes[i, 2].set_title(f\"Cropped Image\\n{cropped_image.size[0]}x{cropped_image.size[1]}\")\n",
    "    axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473d6cb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Crop all images and save to new directories\n",
    "# Define new directories for cropped images\n",
    "cropped_train_img_dir = PATH_PREFIX + 'data/train_img_cropped/'\n",
    "cropped_test_img_dir = PATH_PREFIX + 'data/test_img_cropped/'\n",
    "os.makedirs(cropped_train_img_dir, exist_ok=True)\n",
    "os.makedirs(cropped_test_img_dir, exist_ok=True)\n",
    "\n",
    "# Process training images\n",
    "for img_name in tqdm(os.listdir(masked_train_img_dir), desc=\"Processing training images\"):\n",
    "    img_path = os.path.join(masked_train_img_dir, img_name)\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    cropped_image = crop_to_mask(image)\n",
    "\n",
    "    cropped_image.save(os.path.join(cropped_train_img_dir, img_name))\n",
    "print(\"\\nCropped training images saved.\")\n",
    "# Process test images\n",
    "for img_name in tqdm(os.listdir(masked_test_img_dir), desc=\"Processing test images\"):\n",
    "    img_path = os.path.join(masked_test_img_dir, img_name)\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    cropped_image = crop_to_mask(image)\n",
    "\n",
    "    cropped_image.save(os.path.join(cropped_test_img_dir, img_name))\n",
    "print(\"\\nCropped test images saved.\")\n",
    "\n",
    "# Crop also all augmented images\n",
    "for img_name in tqdm(os.listdir(augmented_dir), desc=\"Processing augmented images\"):\n",
    "    img_path = os.path.join(augmented_dir, img_name)\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    cropped_image = crop_to_mask(image)\n",
    "\n",
    "    cropped_image.save(os.path.join(cropped_train_img_dir, img_name))\n",
    "print(\"\\nCropped augmented images saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48843507",
   "metadata": {},
   "source": [
    "# Dataloaders - Dual-Path Version\n",
    "\n",
    "For the dual-path architecture, we need datasets that load:\n",
    "1. RGB images (original + augmented)\n",
    "2. Grayscale masks (original + augmented)\n",
    "\n",
    "Both are loaded separately and processed through their own paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475c767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class DualPathDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for dual-path architecture that loads images and masks separately.\n",
    "    Supports both original and augmented data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, labels_df, img_dir, mask_dir, augmented_img_dir=None, augmented_mask_dir=None, \n",
    "                 transform_img=None, transform_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels_df: DataFrame with columns ['sample_index', 'label']\n",
    "            img_dir: Directory with original images\n",
    "            mask_dir: Directory with original masks\n",
    "            augmented_img_dir: Directory with augmented images (optional)\n",
    "            augmented_mask_dir: Directory with augmented masks (optional)\n",
    "            transform_img: Transformations for images\n",
    "            transform_mask: Transformations for masks\n",
    "        \"\"\"\n",
    "        self.labels_df = labels_df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.augmented_img_dir = augmented_img_dir\n",
    "        self.augmented_mask_dir = augmented_mask_dir\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_mask = transform_mask\n",
    "        \n",
    "        # Label mapping\n",
    "        self.label_map = {'Triple negative': 0, 'Luminal A': 1, 'Luminal B': 2, 'HER2(+)': 3}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        img_name = row['sample_index']\n",
    "        label = self.label_map[row['label']]\n",
    "        \n",
    "        # Determine if this is an augmented sample\n",
    "        is_augmented = '_aug_' in img_name\n",
    "        \n",
    "        if is_augmented:\n",
    "            # Load from augmented directories\n",
    "            img_path = os.path.join(self.augmented_img_dir, img_name)\n",
    "            mask_name = img_name.replace('img_', 'mask_')\n",
    "            mask_path = os.path.join(self.augmented_mask_dir, mask_name)\n",
    "        else:\n",
    "            # Load from original directories\n",
    "            img_path = os.path.join(self.img_dir, img_name)\n",
    "            mask_name = img_name.replace('img_', 'mask_')\n",
    "            mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        \n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # Grayscale\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform_img:\n",
    "            image = self.transform_img(image)\n",
    "        if self.transform_mask:\n",
    "            mask = self.transform_mask(mask)\n",
    "        \n",
    "        return image, mask, label\n",
    "\n",
    "# ===== DEFINE TRANSFORMATIONS =====\n",
    "# Image transformations (RGB)\n",
    "transform_img = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Pretrained models expect 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Mask transformations (Grayscale, no normalization)\n",
    "transform_mask = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()  # Converts to [0, 1] range, shape [1, H, W]\n",
    "])\n",
    "\n",
    "print(\"âœ“ Transforms defined\")\n",
    "print(\"  Image: Resize â†’ ToTensor â†’ ImageNet Normalize\")\n",
    "print(\"  Mask: Resize â†’ ToTensor (no normalization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e403093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualPathTestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Test dataset for dual-path architecture that loads test images and masks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, mask_dir, transform_img=None, transform_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir: Directory with test images\n",
    "            mask_dir: Directory with test masks\n",
    "            transform_img: Transformations for images\n",
    "            transform_mask: Transformations for masks\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_mask = transform_mask\n",
    "        \n",
    "        # Get all test images\n",
    "        self.image_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        mask_name = img_name.replace('img_', 'mask_')\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        \n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # Grayscale\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform_img:\n",
    "            image = self.transform_img(image)\n",
    "        if self.transform_mask:\n",
    "            mask = self.transform_mask(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "print(\"âœ“ DualPathTestDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f5de5c",
   "metadata": {},
   "source": [
    "## Dual-Path Patch-Based Processing (Advanced)\n",
    "\n",
    "Combining dual-path architecture with patch-based processing for best of both worlds:\n",
    "- **Dual-path**: Separate learning for color (image) and geometry (mask)\n",
    "- **Patch-based**: Higher resolution, more training samples, better for large images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f13e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualPathPatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Combined dual-path + patch-based dataset with DISK CACHING.\n",
    "    Extracts patches from BOTH images and masks, saves to disk to avoid RAM overload.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, labels_df, img_dir, mask_dir, augmented_img_dir=None, augmented_mask_dir=None,\n",
    "                 patch_size=256, stride=224, transform_img=None, transform_mask=None, min_variance=0.001,\n",
    "                 cache_dir=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels_df: DataFrame with columns ['sample_index', 'label']\n",
    "            img_dir: Directory with original images\n",
    "            mask_dir: Directory with original masks\n",
    "            augmented_img_dir: Directory with augmented images (optional)\n",
    "            augmented_mask_dir: Directory with augmented masks (optional)\n",
    "            patch_size: Size of patches to extract\n",
    "            stride: Stride for patch extraction\n",
    "            transform_img: Transformations for image patches\n",
    "            transform_mask: Transformations for mask patches\n",
    "            min_variance: Minimum variance threshold for non-blank patches\n",
    "        \"\"\"\n",
    "        self.labels_df = labels_df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.augmented_img_dir = augmented_img_dir\n",
    "        self.augmented_mask_dir = augmented_mask_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_mask = transform_mask\n",
    "        self.min_variance = min_variance\n",
    "        \n",
    "        # Label mapping\n",
    "        self.label_map = {'Triple negative': 0, 'Luminal A': 1, 'Luminal B': 2, 'HER2(+)': 3}\n",
    "        # Setup cache directory\n",
    "        if cache_dir is None:\n",
    "            import os\n",
    "            cache_dir = os.path.join(PATH_PREFIX if 'PATH_PREFIX' in globals() else '.', 'data', 'dual_path_patches_cache')\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Metadata file\n",
    "        self.metadata_file = os.path.join(cache_dir, f\"metadata_ps{patch_size}_stride{stride}.json\")\n",
    "        \n",
    "        # Check if patches are already cached\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            print(f\"âœ“ Found cached patches: {self.metadata_file}\")\n",
    "            self._load_from_cache()\n",
    "        else:\n",
    "            print(f\"Extracting and caching dual-path patches (patch_size={patch_size}, stride={stride})...\")\n",
    "            self._extract_and_cache_patches(labels_df)\n",
    "        \n",
    "        print(f\"âœ“ Dataset ready with {len(self)} patches\")\n",
    "    \n",
    "    def _extract_and_cache_patches(self, labels_df):\n",
    "        \"\"\"Extract patches and save to disk in batches\"\"\"\n",
    "        self.batch_files = []\n",
    "        self.batch_sizes = []\n",
    "        self.cumulative_sizes = [0]\n",
    "        \n",
    "        batch_patches_img = []\n",
    "        batch_patches_mask = []\n",
    "        batch_labels = []\n",
    "        batch_num = 0\n",
    "        PATCHES_PER_BATCH = 1000  # Save every 1000 patches\n",
    "        \n",
    "        for idx, row in labels_df.iterrows():\n",
    "            img_name = row['sample_index']\n",
    "            label = self.label_map[row['label']]\n",
    "            \n",
    "            # Determine if augmented\n",
    "            is_augmented = '_aug_' in img_name\n",
    "            \n",
    "            if is_augmented:\n",
    "                img_path = os.path.join(self.augmented_img_dir, img_name)\n",
    "                mask_name = img_name.replace('img_', 'mask_')\n",
    "                mask_path = os.path.join(self.augmented_mask_dir, mask_name)\n",
    "            else:\n",
    "                img_path = os.path.join(self.img_dir, img_name)\n",
    "                mask_name = img_name.replace('img_', 'mask_')\n",
    "                mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "            \n",
    "            # Extract paired patches\n",
    "            paired_patches = self._extract_paired_patches_from_file(img_path, mask_path)\n",
    "            \n",
    "            for img_patch, mask_patch in paired_patches:\n",
    "                batch_patches_img.append(img_patch)\n",
    "                batch_patches_mask.append(mask_patch)\n",
    "                batch_labels.append(label)\n",
    "                \n",
    "                # Save batch when full\n",
    "                if len(batch_patches_img) >= PATCHES_PER_BATCH:\n",
    "                    self._save_batch(batch_num, batch_patches_img, batch_patches_mask, batch_labels)\n",
    "                    batch_num += 1\n",
    "                    batch_patches_img = []\n",
    "                    batch_patches_mask = []\n",
    "                    batch_labels = []\n",
    "            \n",
    "            if (idx + 1) % 50 == 0:\n",
    "                total_patches = sum(self.batch_sizes) + len(batch_patches_img)\n",
    "                print(f\"  Processed {idx + 1}/{len(labels_df)} images ({total_patches} patches)\")\n",
    "        \n",
    "        # Save remaining patches\n",
    "        if len(batch_patches_img) > 0:\n",
    "            self._save_batch(batch_num, batch_patches_img, batch_patches_mask, batch_labels)\n",
    "        \n",
    "        # Save metadata\n",
    "        import json\n",
    "        metadata = {\n",
    "            'patch_size': self.patch_size,\n",
    "            'stride': self.stride,\n",
    "            'batch_files': self.batch_files,\n",
    "            'batch_sizes': self.batch_sizes,\n",
    "            'total_patches': sum(self.batch_sizes)\n",
    "        }\n",
    "        with open(self.metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ“ Cached {sum(self.batch_sizes)} patches in {len(self.batch_files)} batch files\")\n",
    "    \n",
    "    def _save_batch(self, batch_num, img_patches, mask_patches, labels):\n",
    "        \"\"\"Save a batch of patches to disk\"\"\"\n",
    "        batch_file = os.path.join(self.cache_dir, f\"batch_{batch_num:04d}.pt\\\")\n",
    "        \n",
    "        # Stack into tensors\n",
    "        img_tensor = torch.stack([torch.from_numpy(p.transpose(2, 0, 1) / 255.0).float() for p in img_patches])\n",
    "        mask_tensor = torch.stack([torch.from_numpy(p / 255.0).float().unsqueeze(0) for p in mask_patches])\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Save to disk\n",
    "        torch.save({\n",
    "            'images': img_tensor,\n",
    "            'masks': mask_tensor,\n",
    "            'labels': labels_tensor\n",
    "        }, batch_file)\n",
    "        \n",
    "        self.batch_files.append(batch_file)\n",
    "        self.batch_sizes.append(len(img_patches))\n",
    "        self.cumulative_sizes.append(self.cumulative_sizes[-1] + len(img_patches))\n",
    "    \n",
    "    def _load_from_cache(self):\n",
    "        \"\"\"Load metadata and patch batches from cache\"\"\"\n",
    "        import json\n",
    "        with open(self.metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        self.batch_files = metadata['batch_files']\n",
    "        self.batch_sizes = metadata['batch_sizes']\n",
    "        self.cumulative_sizes = [0]\n",
    "        for size in self.batch_sizes:\n",
    "            self.cumulative_sizes.append(self.cumulative_sizes[-1] + size)\n",
    "        \n",
    "        # Pre-load all batches into cache (RAM)\n",
    "        self._cache = {}\n",
    "        print(f\"  Loading {len(self.batch_files)} batch files into RAM...\")\n",
    "        for idx, batch_file in enumerate(self.batch_files):\n",
    "            self._cache[idx] = torch.load(batch_file)\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                print(f\"    Loaded {idx + 1}/{len(self.batch_files)} batches\")\n",
    "        \n",
    "        print(f\"  âœ“ All patches loaded into cache\")\n",
    "    \n",
    "    def _extract_paired_patches_from_file(self, img_path, mask_path):\n",
    "        \"\"\"Extract patches from both image and mask at the same locations\"\"\"\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        \n",
    "        img_array = np.array(img, dtype=np.float32)\n",
    "        mask_array = np.array(mask, dtype=np.float32)\n",
    "        \n",
    "        h, w = img_array.shape[:2]\n",
    "        patches = []\n",
    "        \n",
    "        # Handle small images\n",
    "        if h < self.patch_size or w < self.patch_size:\n",
    "            # Pad both image and mask\n",
    "            pad_h = max(0, self.patch_size - h)\n",
    "            pad_w = max(0, self.patch_size - w)\n",
    "            img_array = np.pad(img_array, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "            mask_array = np.pad(mask_array, ((0, pad_h), (0, pad_w)), mode='reflect')\n",
    "            pad_h = max(0, self.patch_size - h)\n",
    "            pad_w = max(0, self.patch_size - w)\n",
    "            img_array = np.pad(img_array, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "            mask_array = np.pad(mask_array, ((0, pad_h), (0, pad_w)), mode='reflect')\n",
    "            \n",
    "            img_patch = img_array[:self.patch_size, :self.patch_size, :]\n",
    "            mask_patch = mask_array[:self.patch_size, :self.patch_size]\n",
    "            \n",
    "            # Check variance\n",
    "            if np.var(img_patch / 255.0) > self.min_variance:\n",
    "                patches.append((img_patch, mask_patch))\n",
    "        else:\n",
    "            # Extract regular grid of patches\n",
    "            n_patches_h = max(1, (h - self.patch_size) // self.stride + 1)\n",
    "            n_patches_w = max(1, (w - self.patch_size) // self.stride + 1)\n",
    "            \n",
    "            for row_idx in range(n_patches_h):\n",
    "                for col_idx in range(n_patches_w):\n",
    "                    start_h = min(row_idx * self.stride, h - self.patch_size)\n",
    "                    start_w = min(col_idx * self.stride, w - self.patch_size)\n",
    "                    \n",
    "                    img_patch = img_array[start_h:start_h+self.patch_size, start_w:start_w+self.patch_size, :]\n",
    "                    mask_patch = mask_array[start_h:start_h+self.patch_size, start_w:start_w+self.patch_size]\n",
    "                    \n",
    "                    # Filter blank patches\n",
    "                    if np.var(img_patch / 255.0) > self.min_variance:\n",
    "                        patches.append((img_patch, mask_patch))\n",
    "        \n",
    "        return patches\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.cumulative_sizes[-1]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Find which batch contains this index\n",
    "        batch_idx = np.searchsorted(self.cumulative_sizes[1:], idx, side='right')\n",
    "        local_idx = idx - self.cumulative_sizes[batch_idx]\n",
    "        \n",
    "        # Get from cache\n",
    "        batch_data = self._cache[batch_idx]\n",
    "        img_patch = batch_data['images'][local_idx]\n",
    "        mask_patch = batch_data['masks'][local_idx]\n",
    "        label = batch_data['labels'][local_idx]\n",
    "        \n",
    "        # Apply runtime transformations if needed (transforms already applied during caching)\n",
    "        return img_patch, mask_patch, label\n",
    "\n",
    "print(\"âœ“ DualPathPatchDataset class defined\")\n",
    "print(\"  Combines dual-path architecture with patch-based processing\")\n",
    "print(\"  Uses disk caching to avoid RAM overload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a692c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualPathTestPatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Test dataset for dual-path + patch-based inference.\n",
    "    Extracts patches from both test images and masks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, mask_dir, patch_size=256, stride=224, \n",
    "                 transform_img=None, transform_mask=None, min_variance=0.001):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir: Directory with test images\n",
    "            mask_dir: Directory with test masks\n",
    "            patch_size: Size of patches\n",
    "            stride: Stride for extraction\n",
    "            transform_img: Image transformations\n",
    "            transform_mask: Mask transformations\n",
    "            min_variance: Minimum variance for non-blank patches\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_mask = transform_mask\n",
    "        self.min_variance = min_variance\n",
    "        \n",
    "        # Get test images\n",
    "        self.image_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
    "        \n",
    "        # Extract patches\n",
    "        print(f\"Extracting test patches (patch_size={patch_size}, stride={stride})...\")\n",
    "        self.patches = []  # List of (img_patch, mask_patch, source_image_idx)\n",
    "        self.patch_to_image = []  # Maps patch index to image index\n",
    "        \n",
    "        for idx, img_name in enumerate(self.image_files):\n",
    "            mask_name = img_name.replace('img_', 'mask_')\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            mask_path = os.path.join(mask_dir, mask_name)\n",
    "            \n",
    "            paired_patches = self._extract_paired_patches(img_path, mask_path, idx)\n",
    "            self.patches.extend(paired_patches)\n",
    "            self.patch_to_image.extend([idx] * len(paired_patches))\n",
    "            \n",
    "            if (idx + 1) % 50 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(self.image_files)} images ({len(self.patches)} patches)\")\n",
    "        \n",
    "        print(f\"âœ“ Extracted {len(self.patches)} test patches from {len(self.image_files)} images\")\n",
    "    \n",
    "    def _extract_paired_patches(self, img_path, mask_path, source_idx):\n",
    "        \"\"\"Extract patches from both image and mask\"\"\"\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        \n",
    "        img_array = np.array(img, dtype=np.float32)\n",
    "        mask_array = np.array(mask, dtype=np.float32)\n",
    "        \n",
    "        h, w = img_array.shape[:2]\n",
    "        patches = []\n",
    "        \n",
    "        if h < self.patch_size or w < self.patch_size:\n",
    "            pad_h = max(0, self.patch_size - h)\n",
    "            pad_w = max(0, self.patch_size - w)\n",
    "            img_array = np.pad(img_array, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "            mask_array = np.pad(mask_array, ((0, pad_h), (0, pad_w)), mode='reflect')\n",
    "            \n",
    "            img_patch = img_array[:self.patch_size, :self.patch_size, :]\n",
    "            mask_patch = mask_array[:self.patch_size, :self.patch_size]\n",
    "            \n",
    "            if np.var(img_patch / 255.0) > self.min_variance:\n",
    "                patches.append((img_patch, mask_patch))\n",
    "        else:\n",
    "            n_patches_h = max(1, (h - self.patch_size) // self.stride + 1)\n",
    "            n_patches_w = max(1, (w - self.patch_size) // self.stride + 1)\n",
    "            \n",
    "            for row_idx in range(n_patches_h):\n",
    "                for col_idx in range(n_patches_w):\n",
    "                    start_h = min(row_idx * self.stride, h - self.patch_size)\n",
    "                    start_w = min(col_idx * self.stride, w - self.patch_size)\n",
    "                    \n",
    "                    img_patch = img_array[start_h:start_h+self.patch_size, start_w:start_w+self.patch_size, :]\n",
    "                    mask_patch = mask_array[start_h:start_h+self.patch_size, start_w:start_w+self.patch_size]\n",
    "                    \n",
    "                    if np.var(img_patch / 255.0) > self.min_variance:\n",
    "                        patches.append((img_patch, mask_patch))\n",
    "        \n",
    "        return patches\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_patch, mask_patch = self.patches[idx]\n",
    "        \n",
    "        # Convert to PIL\n",
    "        img_patch_pil = Image.fromarray(img_patch.astype('uint8'), mode='RGB')\n",
    "        mask_patch_pil = Image.fromarray(mask_patch.astype('uint8'), mode='L')\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform_img:\n",
    "            img_patch = self.transform_img(img_patch_pil)\n",
    "        else:\n",
    "            img_patch = transforms.ToTensor()(img_patch_pil)\n",
    "        \n",
    "        if self.transform_mask:\n",
    "            mask_patch = self.transform_mask(mask_patch_pil)\n",
    "        else:\n",
    "            mask_patch = transforms.ToTensor()(mask_patch_pil)\n",
    "        \n",
    "        return img_patch, mask_patch\n",
    "\n",
    "print(\"âœ“ DualPathTestPatchDataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cba65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CREATE AUGMENTED LABELS DATAFRAME =====\n",
    "# Combine original + augmented samples\n",
    "train_labels_dual = train_labels.copy()\n",
    "\n",
    "# Add augmented samples if they exist\n",
    "if os.path.exists(augmented_img_dir) and os.path.exists(augmented_mask_dir):\n",
    "    augmented_img_files = os.listdir(augmented_img_dir)\n",
    "    print(f\"Found {len(augmented_img_files)} augmented images\")\n",
    "    \n",
    "    augmented_rows = []\n",
    "    for aug_img_name in augmented_img_files:\n",
    "        # Extract original file name: img_XXXX_aug_N.png -> img_XXXX.png\n",
    "        base_name = aug_img_name.split('_aug_')[0] + '.png'\n",
    "        \n",
    "        original_row = train_labels[train_labels['sample_index'] == base_name]\n",
    "        if not original_row.empty:\n",
    "            class_label = original_row.iloc[0]['label']\n",
    "            augmented_rows.append({'sample_index': aug_img_name, 'label': class_label})\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    train_labels_dual = pd.concat([train_labels_dual, augmented_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"Total samples (original + augmented): {len(train_labels_dual)}\")\n",
    "else:\n",
    "    print(\"No augmented data found. Using only original images.\")\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(train_labels_dual['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8dcd7",
   "metadata": {},
   "source": [
    "## Choose Your Approach\n",
    "\n",
    "You now have THREE options:\n",
    "\n",
    "1. **Simple Dual-Path** (cells 34-35): Process full images, faster, good for quick iteration\n",
    "2. **Dual-Path + Patches** (cell below): Best accuracy, higher resolution, more data\n",
    "3. **Patches only** (old cell 36, disabled): Single-path baseline\n",
    "\n",
    "For maximum performance, use **option 2** (dual-path + patches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== OPTION 2: DUAL-PATH + PATCH-BASED (RECOMMENDED) =====\n",
    "# Uncomment this section to use the combined approach\n",
    "\n",
    "USE_PATCHES = True  # Set to False to use simple dual-path (option 1)\n",
    "PATCH_SIZE = 256\n",
    "PATCH_STRIDE = 224\n",
    "\n",
    "if USE_PATCHES:\n",
    "    print(\"=\"*80)\n",
    "    print(\"USING DUAL-PATH + PATCH-BASED APPROACH\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create patch-based datasets\n",
    "    train_dataset_patches = DualPathPatchDataset(\n",
    "        labels_df=train_df,\n",
    "        img_dir=train_img_dir,\n",
    "        mask_dir=train_mask_dir,\n",
    "        augmented_img_dir=augmented_img_dir,\n",
    "        augmented_mask_dir=augmented_mask_dir,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        stride=PATCH_STRIDE,\n",
    "        transform_img=transform_img,\n",
    "        transform_mask=transform_mask\n",
    "    )\n",
    "    \n",
    "    val_dataset_patches = DualPathPatchDataset(\n",
    "        labels_df=val_df,\n",
    "        img_dir=train_img_dir,\n",
    "        mask_dir=train_mask_dir,\n",
    "        augmented_img_dir=augmented_img_dir,\n",
    "        augmented_mask_dir=augmented_mask_dir,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        stride=PATCH_STRIDE,\n",
    "        transform_img=transform_img,\n",
    "        transform_mask=transform_mask\n",
    "    )\n",
    "    \n",
    "    # Override the simple datasets\n",
    "    train_dataset = train_dataset_patches\n",
    "    val_dataset = val_dataset_patches\n",
    "    \n",
    "    print(f\"\\nâœ“ Dual-path patch datasets created\")\n",
    "    print(f\"  Train patches: {len(train_dataset)}\")\n",
    "    print(f\"  Val patches: {len(val_dataset)}\")\n",
    "else:\n",
    "    print(\"Using simple dual-path approach (full images)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CREATE TRAIN/VAL SPLIT =====\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "train_df, val_df = train_test_split(\n",
    "    train_labels_dual, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=train_labels_dual['label']\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_df)} samples\")\n",
    "print(f\"Val set: {len(val_df)} samples\")\n",
    "\n",
    "# ===== CREATE DATASETS =====\n",
    "train_dataset = DualPathDataset(\n",
    "    labels_df=train_df,\n",
    "    img_dir=train_img_dir,\n",
    "    mask_dir=train_mask_dir,\n",
    "    augmented_img_dir=augmented_img_dir,\n",
    "    augmented_mask_dir=augmented_mask_dir,\n",
    "    transform_img=transform_img,\n",
    "    transform_mask=transform_mask\n",
    ")\n",
    "\n",
    "val_dataset = DualPathDataset(\n",
    "    labels_df=val_df,\n",
    "    img_dir=train_img_dir,\n",
    "    mask_dir=train_mask_dir,\n",
    "    augmented_img_dir=augmented_img_dir,\n",
    "    augmented_mask_dir=augmented_mask_dir,\n",
    "    transform_img=transform_img,\n",
    "    transform_mask=transform_mask\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Datasets created\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578afcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CREATE DATALOADERS =====\n",
    "BATCH_SIZE = 64  # Adjust based on GPU memory\n",
    "\n",
    "# GPU optimization settings\n",
    "if torch.cuda.is_available():\n",
    "    NUM_WORKERS = 2 * torch.cuda.device_count()\n",
    "    PIN_MEMORY = True\n",
    "    PERSISTENT_WORKERS = True\n",
    "    print(f\"GPU detected: {torch.cuda.device_count()} GPU(s) - using {NUM_WORKERS} workers\")\n",
    "else:\n",
    "    NUM_WORKERS = 0\n",
    "    PIN_MEMORY = False\n",
    "    PERSISTENT_WORKERS = False\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=PERSISTENT_WORKERS if NUM_WORKERS > 0 else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    persistent_workers=PERSISTENT_WORKERS if NUM_WORKERS > 0 else False\n",
    ")\n",
    "\n",
    "test_loader = \n",
    "\n",
    "print(f\"\\nâœ“ DataLoaders created\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n",
    "print(f\"  Pin memory: {PIN_MEMORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6b729",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_shape = (3, PATCH_SIZE, PATCH_SIZE) if PATCH_SIZE else (3, IMG_SIZE[0], IMG_SIZE[1])\n",
    "num_classes = len(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85cd3aa",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00880a6d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Number of training epochs\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 300\n",
    "PATIENCE = 50\n",
    "\n",
    "# Regularisation\n",
    "DROPOUT_RATE = 0.5       # Dropout probability\n",
    "L1_LAMBDA = 0.001           # L1 penalty\n",
    "L2_LAMBDA = 0.01         # L2 penalty\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", EPOCHS)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Learning Rate:\", LEARNING_RATE)\n",
    "print(\"Dropout Rate:\", DROPOUT_RATE)\n",
    "print(\"L1 Penalty:\", L1_LAMBDA)\n",
    "print(\"L2 Penalty:\", L2_LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb42d0",
   "metadata": {},
   "source": [
    "# Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b60f5-a3e4-4214-843d-a0c2eec77448",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights, ResNet50_Weights, EfficientNet_B0_Weights, EfficientNet_B3_Weights, VGG16_Weights\n",
    "\n",
    "# ===== UNCOMMENT THE MODEL YOU WANT TO USE =====\n",
    "\n",
    "# ResNet-18 (Smaller, faster)\n",
    "model_pretrained = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "MODEL_NAME = \"resnet18\"\n",
    "\n",
    "# ResNet-50 (Deeper, more powerful)\n",
    "# model_pretrained = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "# MODEL_NAME = \"resnet50\"\n",
    "\n",
    "# EfficientNet-B0 (Efficient, good balance)\n",
    "# model_pretrained = models.efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "# MODEL_NAME = \"efficientnet_b0\"\n",
    "\n",
    "# EfficientNet-B3 (More powerful EfficientNet)\n",
    "# model_pretrained = models.efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
    "# MODEL_NAME = \"efficientnet_b3\"\n",
    "\n",
    "# VGG-16 (Classic architecture)\n",
    "# model_pretrained = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "# MODEL_NAME = \"vgg16\"\n",
    "\n",
    "# ===============================================\n",
    "\n",
    "print(f\"Loaded pretrained model: {MODEL_NAME}\")\n",
    "print(f\"Model architecture:\\n{model_pretrained}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b2d99",
   "metadata": {},
   "source": [
    "## Dual-Path Architecture (Following Organizer's Advice)\n",
    "\n",
    "**Philosophy**: *\"The image is the chaotic world; the mask is the silent intent. Walk two roads, and meet only at the summit.\"*\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "Instead of concatenating mask at input (crude approach), we:\n",
    "\n",
    "1. **Two Parallel Paths**:\n",
    "   - **Image Path**: Pretrained backbone (ResNet/EfficientNet) processes RGB color â†’ Frozen weights (transfer learning)\n",
    "   - **Mask Path**: Trainable backbone processes pure geometry â†’ Learns from scratch\n",
    "\n",
    "2. **Separate Augmentation**:\n",
    "   - Apply **identical** transformations to both image AND mask\n",
    "   - Saves to `train_img_augmented/` and `train_mask_augmented/`\n",
    "   - Maintains geometric consistency\n",
    "\n",
    "3. **Late Fusion**:\n",
    "   - Both paths extract features independently\n",
    "   - Concatenate in latent space (after feature extraction)\n",
    "   - Final classification head processes combined features\n",
    "\n",
    "4. **Key Changes**:\n",
    "   - âœ… Augmentation generates paired image-mask files\n",
    "   - âœ… Dataset loads images and masks separately  \n",
    "   - âœ… Model accepts dual inputs: `model(image, mask)`\n",
    "   - âœ… Training/validation loops updated for dual inputs\n",
    "\n",
    "This follows the organizer's advice to keep paths parallel and only fuse when \"both have learned their language\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights, ResNet50_Weights, EfficientNet_B0_Weights\n",
    "\n",
    "class DualPathModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-Path Architecture following organizer's advice:\n",
    "    - Image Path: Processes raw RGB color information  \n",
    "    - Mask Path: Encodes pure geometry\n",
    "    - Late Fusion: Combine in latent space after both have learned their representations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, dropout_rate=0.5, pretrained_model='resnet18'):\n",
    "        super(DualPathModel, self).__init__()\n",
    "        \n",
    "        # ===== IMAGE PATH: Process RGB color =====\n",
    "        if pretrained_model == 'resnet18':\n",
    "            self.image_backbone = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "            backbone_features = 512\n",
    "        elif pretrained_model == 'resnet50':\n",
    "            self.image_backbone = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "            backbone_features = 2048\n",
    "        elif pretrained_model == 'efficientnet_b0':\n",
    "            self.image_backbone = models.efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "            backbone_features = 1280\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {pretrained_model}\")\n",
    "        \n",
    "        # Remove the final classification layer (keep only feature extractor)\n",
    "        if pretrained_model.startswith('resnet'):\n",
    "            self.image_backbone = nn.Sequential(*list(self.image_backbone.children())[:-1])\n",
    "        elif pretrained_model.startswith('efficientnet'):\n",
    "            self.image_backbone = nn.Sequential(*list(self.image_backbone.children())[:-1])\n",
    "        \n",
    "        # Freeze image backbone (transfer learning)\n",
    "        for param in self.image_backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # ===== MASK PATH: Process pure geometry =====\n",
    "        # Smaller network for masks (1-channel input, simpler features)\n",
    "        if pretrained_model.startswith('resnet'):\n",
    "            self.mask_backbone = models.resnet18(weights=None)  # Random init for mask\n",
    "            # Modify first conv to accept 1-channel input (grayscale mask)\n",
    "            self.mask_backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            self.mask_backbone = nn.Sequential(*list(self.mask_backbone.children())[:-1])\n",
    "            mask_features = 512\n",
    "        elif pretrained_model.startswith('efficientnet'):\n",
    "            self.mask_backbone = models.efficientnet_b0(weights=None)\n",
    "            # Modify first conv for 1-channel input\n",
    "            self.mask_backbone.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "            self.mask_backbone = nn.Sequential(*list(self.mask_backbone.children())[:-1])\n",
    "            mask_features = 1280\n",
    "        \n",
    "        # Mask backbone is trainable (learns geometry from scratch)\n",
    "        for param in self.mask_backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # ===== LATE FUSION: Combine paths in latent space =====\n",
    "        fusion_features = backbone_features + mask_features\n",
    "        \n",
    "        self.fusion_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(fusion_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, image, mask):\n",
    "        \"\"\"\n",
    "        Forward pass through dual paths\n",
    "        \n",
    "        Args:\n",
    "            image: RGB image tensor [B, 3, H, W]\n",
    "            mask: Grayscale mask tensor [B, 1, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            logits: Class predictions [B, num_classes]\n",
    "        \"\"\"\n",
    "        # Process image path (chaotic world)\n",
    "        image_features = self.image_backbone(image)  # [B, 512, 1, 1] or [B, 1280, 1, 1]\n",
    "        \n",
    "        # Process mask path (silent intent)\n",
    "        mask_features = self.mask_backbone(mask)     # [B, 512, 1, 1] or [B, 1280, 1, 1]\n",
    "        \n",
    "        # Concatenate in latent space\n",
    "        combined_features = torch.cat([image_features, mask_features], dim=1)  # [B, fusion_features, 1, 1]\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.fusion_head(combined_features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"âœ“ DualPathModel class defined\")\n",
    "print(\"  - Image path: Pretrained backbone (frozen)\")\n",
    "print(\"  - Mask path: Trainable geometry encoder\")\n",
    "print(\"  - Fusion: Late concatenation in latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edcbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SELECT BACKBONE FOR DUAL-PATH MODEL =====\n",
    "\n",
    "# ResNet-18 (Smaller, faster) - RECOMMENDED for dual-path\n",
    "MODEL_NAME = \"resnet18\"\n",
    "\n",
    "# ResNet-50 (Deeper, more powerful)\n",
    "# MODEL_NAME = \"resnet50\"\n",
    "\n",
    "# EfficientNet-B0 (Efficient, good balance)\n",
    "# MODEL_NAME = \"efficientnet_b0\"\n",
    "\n",
    "# ===============================================\n",
    "\n",
    "# Create dual-path model\n",
    "model_pretrained = DualPathModel(\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    pretrained_model=MODEL_NAME\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Created DualPathModel with {MODEL_NAME} backbones\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  Dropout rate: {DROPOUT_RATE}\")\n",
    "\n",
    "# Move model to device FIRST\n",
    "model_pretrained = model_pretrained.to(device)\n",
    "\n",
    "# Then wrap with DataParallel if multiple GPUs are available\n",
    "if num_gpus > 1:\n",
    "    model_pretrained = nn.DataParallel(model_pretrained)\n",
    "    print(f\"  Model wrapped with DataParallel for {num_gpus} GPUs\")\n",
    "\n",
    "print(f\"\\nâœ“ Model ready for dual-path training on {device} ({num_gpus} GPU(s))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea301fbb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure correct trainability settings for dual-path model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURING TRAINABILITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get the actual model (unwrap DataParallel if needed)\n",
    "actual_model = model_pretrained.module if hasattr(model_pretrained, 'module') else model_pretrained\n",
    "\n",
    "# Freeze image backbone (transfer learning)\n",
    "for param in actual_model.image_backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"âœ“ Image backbone: FROZEN (pretrained weights)\")\n",
    "\n",
    "# Unfreeze mask backbone (learn geometry)\n",
    "for param in actual_model.mask_backbone.parameters():\n",
    "    param.requires_grad = True\n",
    "print(\"âœ“ Mask backbone: TRAINABLE (learns from scratch)\")\n",
    "\n",
    "# Unfreeze fusion head (classification)\n",
    "for param in actual_model.fusion_head.parameters():\n",
    "    param.requires_grad = True\n",
    "print(\"âœ“ Fusion head: TRAINABLE (classification)\")\n",
    "\n",
    "# Display architecture and verify trainability\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DUAL-PATH MODEL ARCHITECTURE\")\n",
    "print(\"=\"*80)\n",
    "print(actual_model)\n",
    "\n",
    "# Count trainable vs frozen parameters\n",
    "total_params = sum(p.numel() for p in model_pretrained.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_pretrained.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARAMETER STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters (mask backbone + fusion head): {trainable_params:,}\")\n",
    "print(f\"Frozen parameters (image backbone): {frozen_params:,}\")\n",
    "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nâœ“ Model moved to {device} after trainability configuration\")\n",
    "\n",
    "# CRITICAL: Ensure model is on GPU after trainability changesmodel_pretrained = model_pretrained.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d3174",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define optimizer - train mask backbone + fusion head (image backbone is frozen)\n",
    "# Get only trainable parameters\n",
    "trainable_params = [p for p in model_pretrained.parameters() if p.requires_grad]\n",
    "\n",
    "print(f\"\\nâœ“ Found {len(trainable_params)} trainable parameter tensors\")\n",
    "print(f\"  Total trainable params: {sum(p.numel() for p in trainable_params):,}\")\n",
    "\n",
    "optimizer = torch.optim.RMSprop(trainable_params, lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "\n",
    "# Enable mixed precision training for GPU acceleration\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "print(f\"\\nâœ“ Optimizer configured (RMSprop)\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay (L2): {L2_LAMBDA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268bf50",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GPU Memory and Utilization Monitoring\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GPU STATUS BEFORE TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory Reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Max Memory Allocated: {torch.cuda.max_memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f5f56",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a4724",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize best model tracking variables\n",
    "best_model = None\n",
    "best_performance = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e4dd6",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    \"\"\"\n",
    "    Perform one complete training epoch through the entire training dataset (DUAL-PATH VERSION).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train (DualPathModel)\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches (images, masks, labels)\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): Lambda for L1 regularization\n",
    "        l2_lambda (float): Lambda for L2 regularization\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Iterate through training batches - DUAL-PATH: images, masks, labels\n",
    "    for batch_idx, (images, masks, targets) in enumerate(train_loader):\n",
    "        # Move data to device (GPU/CPU)\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Clear gradients from previous step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with mixed precision (if CUDA available)\n",
    "        # DUAL-PATH: Pass both images and masks\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(images, masks)  # <-- Dual inputs\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Add L1 and L2 regularization\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters() if p.requires_grad)\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters() if p.requires_grad)\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b99110",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Perform one complete validation epoch through the entire validation dataset (DUAL-PATH VERSION).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to evaluate (DualPathModel)\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches (images, masks, labels)\n",
    "        criterion (nn.Module): Loss function used to calculate validation loss\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
    "\n",
    "    Note:\n",
    "        This function automatically sets the model to evaluation mode and disables\n",
    "        gradient computation for efficiency during validation.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        for images, masks, targets in val_loader:  # <-- Dual inputs\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision (if CUDA available)\n",
    "            # DUAL-PATH: Pass both images and masks\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(images, masks)  # <-- Dual inputs\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05834891",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Train the neural network model on the training data and validate on the validation data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        epochs (int): Number of training epochs\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
    "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
    "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
    "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
    "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
    "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
    "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
    "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
    "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, training_history) - Trained model and metrics history\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize metrics tracking\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "\n",
    "    # Configure early stopping if patience is set\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    # Main training loop: iterate through epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        # Forward pass through training data, compute gradients, update weights\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        # Evaluate model on validation data without updating weights\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        # Store metrics for plotting and analysis\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        # Print progress every N epochs or on first epoch\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
    "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping logic: monitor metric and save best model\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(),\"models/\"+experiment_name+'_model.pt')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    # Restore best model weights if early stopping was used\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
    "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "\n",
    "    # Save final model if no early stopping\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history, best_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8b6f7",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0c487",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Set experiment name for this run\n",
    "EXPERIMENT_NAME = f\"pretrained_{MODEL_NAME}_augmented\"\n",
    "\n",
    "# Train with augmented (balanced) dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TRAINING WITH PRETRAINED {MODEL_NAME.upper()} - TRANSFER LEARNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")\n",
    "print(f\"Strategy: Frozen feature extractor + Trainable classifier\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Train model and track training history using AUGMENTED dataset\n",
    "model_pretrained, history, best_perf = fit(\n",
    "    model=model_pretrained,\n",
    "    train_loader=train_loader,  # â† USE AUGMENTED LOADER\n",
    "    val_loader=val_loader,      # â† USE AUGMENTED LOADER\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    verbose=10,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    patience=PATIENCE\n",
    ")\n",
    "\n",
    "# Update best model if current performance is superior\n",
    "if best_perf > best_performance:\n",
    "    best_model = model_pretrained    \n",
    "    best_performance = best_perf\n",
    "    print(f\"\\nNew best model saved with F1 Score: {best_performance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd146d25",
   "metadata": {},
   "source": [
    "# Identify High-Loss Samples (Data Quality Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67724fa1",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_per_sample_loss(model, dataset, criterion, device):\n",
    "    \"\"\"\n",
    "    Calculate loss for each individual sample in the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        losses: numpy array of per-sample losses\n",
    "        predictions: numpy array of predicted labels\n",
    "        targets: numpy array of true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            inputs, target = dataset[i]\n",
    "            inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension\n",
    "            target_tensor = torch.tensor([target]).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, target_tensor)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            predictions.append(logits.argmax(dim=1).cpu().item())\n",
    "            targets.append(target)\n",
    "    \n",
    "    return np.array(losses), np.array(predictions), np.array(targets)\n",
    "\n",
    "print(\"Calculating per-sample losses on training set...\")\n",
    "train_losses, train_preds, train_targets = calculate_per_sample_loss(\n",
    "    model_pretrained, train_dataset, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\nLoss statistics:\")\n",
    "print(f\"Mean loss: {train_losses.mean():.4f}\")\n",
    "print(f\"Median loss: {np.median(train_losses):.4f}\")\n",
    "print(f\"Max loss: {train_losses.max():.4f}\")\n",
    "print(f\"Min loss: {train_losses.min():.4f}\")\n",
    "print(f\"Std loss: {train_losses.std():.4f}\")\n",
    "\n",
    "# Identify high-loss samples\n",
    "top_k = 50  # Number of worst samples to examine\n",
    "worst_indices = np.argsort(train_losses)[-top_k:][::-1]  # Highest losses first\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOP {top_k} HIGHEST LOSS SAMPLES (Potential Data Quality Issues)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Index':<10} {'Loss':<12} {'True Label':<20} {'Predicted':<20} {'Correct':<10}\")\n",
    "print('-' * 80)\n",
    "\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "problematic_samples = []\n",
    "\n",
    "for rank, idx in enumerate(worst_indices, 1):\n",
    "    loss = train_losses[idx]\n",
    "    true_label = reverse_label_map[train_targets[idx]]\n",
    "    pred_label = reverse_label_map[train_preds[idx]]\n",
    "    is_correct = train_targets[idx] == train_preds[idx]\n",
    "    \n",
    "    problematic_samples.append({\n",
    "        'dataset_index': idx,\n",
    "        'loss': loss,\n",
    "        'true_label': true_label,\n",
    "        'predicted_label': pred_label,\n",
    "        'correct': is_correct\n",
    "    })\n",
    "    \n",
    "    if rank <= 20:  # Print top 20\n",
    "        print(f\"{idx:<10} {loss:<12.4f} {true_label:<20} {pred_label:<20} {str(is_correct):<10}\")\n",
    "\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773e6da",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize the worst samples\n",
    "fig, axes = plt.subplots(5, 5, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(f\"\\nVisualizing top 25 highest-loss samples...\")\n",
    "\n",
    "for i in range(min(25, len(worst_indices))):\n",
    "    idx = worst_indices[i]\n",
    "    loss = train_losses[idx]\n",
    "    true_label = reverse_label_map[train_targets[idx]]\n",
    "    pred_label = reverse_label_map[train_preds[idx]]\n",
    "    \n",
    "    # Get the image tensor\n",
    "    img_tensor, _ = train_dataset[idx]\n",
    "    \n",
    "    # Convert tensor to displayable image (C, H, W) -> (H, W, C)\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Display image\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(\n",
    "        f\"Rank {i+1}: Loss={loss:.3f}\\n\"\n",
    "        f\"True: {true_label}\\n\"\n",
    "        f\"Pred: {pred_label}\",\n",
    "        fontsize=9,\n",
    "        color='red' if true_label != pred_label else 'green'\n",
    "    )\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Top 25 Highest Loss Training Samples', fontsize=16, y=1.001)\n",
    "plt.show()\n",
    "\n",
    "# Plot loss distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram of losses\n",
    "axes[0].hist(train_losses, bins=100, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(train_losses.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {train_losses.mean():.3f}')\n",
    "axes[0].axvline(np.median(train_losses), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(train_losses):.3f}')\n",
    "axes[0].set_xlabel('Loss')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].set_title('Distribution of Per-Sample Losses')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Sorted losses\n",
    "sorted_losses = np.sort(train_losses)\n",
    "axes[1].plot(sorted_losses, color='steelblue', linewidth=2)\n",
    "axes[1].axhline(train_losses.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {train_losses.mean():.3f}')\n",
    "axes[1].set_xlabel('Sample Rank (sorted)')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Sorted Per-Sample Losses')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94029c4",
   "metadata": {},
   "source": [
    "## Remove High-Loss Samples (Data Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e0d33",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define threshold for removing high-loss samples\n",
    "# Option 1: Remove top N samples with highest loss\n",
    "REMOVE_TOP_N = 100  # Adjust this value based on visual inspection\n",
    "\n",
    "# Option 2: Remove samples above a certain loss percentile\n",
    "LOSS_PERCENTILE_THRESHOLD = 95  # Remove top 5% highest losses\n",
    "\n",
    "# Choose method (uncomment one)\n",
    "METHOD = \"top_n\"  # Remove top N samples\n",
    "# METHOD = \"percentile\"  # Remove by percentile\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"REMOVING HIGH-LOSS SAMPLES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if METHOD == \"top_n\":\n",
    "    # First, we need to get the actual worst indices (not limited by top_k)\n",
    "    # Re-calculate worst_indices for removal (up to REMOVE_TOP_N)\n",
    "    all_worst_indices = np.argsort(train_losses)[::-1]  # All samples sorted by loss (highest first)\n",
    "    n_to_remove = min(REMOVE_TOP_N, len(train_losses))  # Don't try to remove more than available\n",
    "    samples_to_remove = all_worst_indices[:n_to_remove]\n",
    "    threshold_loss = train_losses[samples_to_remove[-1]]\n",
    "    print(f\"Method: Remove top {n_to_remove} samples\")\n",
    "    print(f\"Loss threshold: {threshold_loss:.4f}\")\n",
    "else:\n",
    "    # Remove samples above percentile threshold\n",
    "    threshold_loss = np.percentile(train_losses, LOSS_PERCENTILE_THRESHOLD)\n",
    "    samples_to_remove = np.where(train_losses > threshold_loss)[0]\n",
    "    print(f\"Method: Remove samples above {LOSS_PERCENTILE_THRESHOLD}th percentile\")\n",
    "    print(f\"Loss threshold: {threshold_loss:.4f}\")\n",
    "\n",
    "print(f\"Samples to remove: {len(samples_to_remove)}\")\n",
    "print(f\"Original training set size: {len(train_dataset)}\")\n",
    "print(f\"New training set size: {len(train_dataset) - len(samples_to_remove)}\")\n",
    "\n",
    "# Create mask for samples to keep\n",
    "keep_mask = np.ones(len(train_dataset), dtype=bool)\n",
    "keep_mask[samples_to_remove] = False\n",
    "\n",
    "# Filter the datasets\n",
    "# Create new indices list excluding samples to remove\n",
    "keep_indices = [idx for idx in train_indices if idx not in samples_to_remove]\n",
    "\n",
    "# Filter using the keep_indices - use Subset to avoid loading all data into memory\n",
    "train_dataset_cleaned = Subset(full_dataset, keep_indices)\n",
    "\n",
    "# Check class distribution after cleaning\n",
    "print(f\"\\nClass distribution after cleaning:\")\n",
    "y_cleaned = [full_dataset[idx][1] for idx in keep_indices]\n",
    "unique, counts = np.unique(y_cleaned, return_counts=True)\n",
    "for label_idx, count in zip(unique, counts):\n",
    "    label_name = reverse_label_map[label_idx]\n",
    "    print(f\"  {label_name}: {count} samples\")\n",
    "\n",
    "# Note: Using Subset instead of TensorDataset to save memory\n",
    "# Data will be loaded on-demand during training\n",
    "\n",
    "train_loader_kwargs = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY\n",
    "}\n",
    "if NUM_WORKERS > 0:\n",
    "    train_loader_kwargs['persistent_workers'] = PERSISTENT_WORKERS\n",
    "\n",
    "train_loader_cleaned = DataLoader(train_dataset_cleaned, **train_loader_kwargs)\n",
    "\n",
    "print(f\"\\nNew DataLoader created:\")\n",
    "print(f\"Train batches: {len(train_loader_cleaned)}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec31b2a",
   "metadata": {},
   "source": [
    "## Retrain with Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11035fa",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# ===== DUAL-PATH MODEL: No manual classifier replacement needed =====\n",
    "# The DualPathModel already has fusion_head configured in __init__\n",
    "# We just need to verify trainable parameters and set up optimizer\n",
    "\n",
    "# Get base model (unwrap DataParallel if needed)\n",
    "is_parallel = isinstance(model_pretrained, nn.DataParallel)\n",
    "base_model = model_pretrained.module if is_parallel else model_pretrained\n",
    "\n",
    "# Count trainable parameters\n",
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DUAL-PATH MODEL PARAMETER STATUS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  - Mask backbone (trainable)\")\n",
    "print(f\"  - Fusion head (trainable)\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "print(f\"  - Image backbone (frozen, pretrained)\")\n",
    "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Reinitialize optimizer and scaler for cleaned training\n",
    "# Only optimize trainable parameters\n",
    "trainable_param_list = filter(lambda p: p.requires_grad, model_pretrained.parameters())\n",
    "optimizer_cleaned = torch.optim.AdamW(trainable_param_list, lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "scaler_cleaned = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# Set experiment name for cleaned model\n",
    "EXPERIMENT_NAME_CLEANED = f\"{EXPERIMENT_NAME}_cleaned\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING WITH CLEANED DATASET (High-loss samples removed)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train loader: {len(train_loader_cleaned)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches (unchanged)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Train model with cleaned data\n",
    "model_pretrained_cleaned, history_cleaned, best_perf = fit(\n",
    "    model=model_pretrained,\n",
    "    train_loader=train_loader_cleaned,  # â† CLEANED LOADER\n",
    "    val_loader=val_loader,              # Validation set unchanged\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer_cleaned,\n",
    "    scaler=scaler_cleaned,\n",
    "    device=device,\n",
    "    verbose=10,\n",
    "    experiment_name=EXPERIMENT_NAME_CLEANED,\n",
    "    patience=PATIENCE\n",
    ")\n",
    "\n",
    "# Update best model if current performance is superior\n",
    "if best_perf > best_performance:\n",
    "    best_model = model_pretrained_cleaned\n",
    "    best_performance = best_perf\n",
    "    print(f\"\\nâœ“ New best model saved with F1 Score: {best_performance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f41e21",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compare original vs cleaned training\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0, 0].plot(history['train_loss'], label='Original - Train', alpha=0.6, linestyle='--', color='#1f77b4')\n",
    "axes[0, 0].plot(history['val_loss'], label='Original - Val', alpha=0.8, color='#1f77b4')\n",
    "axes[0, 0].plot(history_cleaned['train_loss'], label='Cleaned - Train', alpha=0.6, linestyle='--', color='#ff7f0e')\n",
    "axes[0, 0].plot(history_cleaned['val_loss'], label='Cleaned - Val', alpha=0.8, color='#ff7f0e')\n",
    "axes[0, 0].set_title('Loss Comparison: Original vs Cleaned Data')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# F1 Score comparison\n",
    "axes[0, 1].plot(history['train_f1'], label='Original - Train', alpha=0.6, linestyle='--', color='#1f77b4')\n",
    "axes[0, 1].plot(history['val_f1'], label='Original - Val', alpha=0.8, color='#1f77b4')\n",
    "axes[0, 1].plot(history_cleaned['train_f1'], label='Cleaned - Train', alpha=0.6, linestyle='--', color='#ff7f0e')\n",
    "axes[0, 1].plot(history_cleaned['val_f1'], label='Cleaned - Val', alpha=0.8, color='#ff7f0e')\n",
    "axes[0, 1].set_title('F1 Score Comparison: Original vs Cleaned Data')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Training loss only (zoomed)\n",
    "axes[1, 0].plot(history['train_loss'], label='Original', alpha=0.8, color='#1f77b4')\n",
    "axes[1, 0].plot(history_cleaned['train_loss'], label='Cleaned', alpha=0.8, color='#ff7f0e')\n",
    "axes[1, 0].set_title('Training Loss: Original vs Cleaned Data')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Training Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Validation F1 only (zoomed)\n",
    "axes[1, 1].plot(history['val_f1'], label='Original', alpha=0.8, color='#1f77b4', marker='o')\n",
    "axes[1, 1].plot(history_cleaned['val_f1'], label='Cleaned', alpha=0.8, color='#ff7f0e', marker='s')\n",
    "axes[1, 1].set_title('Validation F1 Score: Original vs Cleaned Data')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Validation F1 Score')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOriginal Dataset:\")\n",
    "print(f\"  Best Val F1: {max(history['val_f1']):.4f}\")\n",
    "print(f\"  Final Val F1: {history['val_f1'][-1]:.4f}\")\n",
    "print(f\"  Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nCleaned Dataset (removed {len(samples_to_remove)} high-loss samples):\")\n",
    "print(f\"  Best Val F1: {max(history_cleaned['val_f1']):.4f}\")\n",
    "print(f\"  Final Val F1: {history_cleaned['val_f1'][-1]:.4f}\")\n",
    "print(f\"  Final Train Loss: {history_cleaned['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {history_cleaned['val_loss'][-1]:.4f}\")\n",
    "\n",
    "improvement = max(history_cleaned['val_f1']) - max(history['val_f1'])\n",
    "print(f\"\\nImprovement: {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829b683",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe5e407",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Get validation predictions\n",
    "val_preds = []\n",
    "val_targets = []\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        logits = best_model(inputs)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        val_preds.append(preds)\n",
    "        val_targets.append(targets.numpy())\n",
    "\n",
    "val_preds = np.concatenate(val_preds)\n",
    "val_targets = np.concatenate(val_targets)\n",
    "\n",
    "# Calculate overall validation set metrics\n",
    "val_acc = accuracy_score(val_targets, val_preds)\n",
    "val_prec = precision_score(val_targets, val_preds, average='weighted')\n",
    "val_rec = recall_score(val_targets, val_preds, average='weighted')\n",
    "val_f1 = f1_score(val_targets, val_preds, average='weighted')\n",
    "\n",
    "print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n",
    "print(f\"Precision over the validation set: {val_prec:.4f}\")\n",
    "print(f\"Recall over the validation set: {val_rec:.4f}\")\n",
    "print(f\"F1 score over the validation set: {val_f1:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(val_targets, val_preds)\n",
    "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix â€” Validation Set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0b71c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two side-by-side subplots (two columns)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot of training and validation loss on the first axis\n",
    "ax1.plot(history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax1.plot(history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot of training and validation accuracy on the second axis\n",
    "ax2.plot(history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax2.plot(history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
    "ax2.set_title('F1 Score')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea326ad9",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336b736-e6db-4b1f-aae1-8b68c9c2e8a6",
   "metadata": {},
   "source": [
    "## Emergency load weights if lost variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0032ab7-9d6b-43dc-ac5a-1657313b11a8",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_weights_name = 'pretrained_resnet18_augmented_model.pt'\n",
    "# import torchvision.models as models\n",
    "# from torchvision.models import ResNet18_Weights, ResNet50_Weights, EfficientNet_B0_Weights, EfficientNet_B3_Weights, VGG16_Weights\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # Number of training epochs\n",
    "# LEARNING_RATE = 1e-3\n",
    "# EPOCHS = 300\n",
    "# PATIENCE = 50\n",
    "\n",
    "# # Regularisation\n",
    "# DROPOUT_RATE = 0.5       # Dropout probability\n",
    "# L1_LAMBDA = 0.001           # L1 penalty\n",
    "# L2_LAMBDA = 0.01         # L2 penalty\n",
    "\n",
    "# # Set up loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# checkpoint = torch.load(f\"models/{model_weights_name}\", map_location=device)\n",
    "\n",
    "# # Remove \"module.\" prefix if present\n",
    "# from collections import OrderedDict\n",
    "# new_state_dict = OrderedDict()\n",
    "\n",
    "# for k, v in checkpoint.items():\n",
    "#     name = k.replace(\"module.\", \"\")  # remove module.\n",
    "#     new_state_dict[name] = v\n",
    "# # ===== UNCOMMENT THE MODEL YOU WANT TO USE =====\n",
    "\n",
    "# # ResNet-18 (Smaller, faster)\n",
    "# best_model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "# MODEL_NAME = \"resnet18\"\n",
    "\n",
    "# # ResNet-50 (Deeper, more powerful)\n",
    "# # model_pretrained = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "# # MODEL_NAME = \"resnet50\"\n",
    "\n",
    "# # EfficientNet-B0 (Efficient, good balance)\n",
    "# # model_pretrained = models.efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "# # MODEL_NAME = \"efficientnet_b0\"\n",
    "\n",
    "# # EfficientNet-B3 (More powerful EfficientNet)\n",
    "# # model_pretrained = models.efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
    "# # MODEL_NAME = \"efficientnet_b3\"\n",
    "\n",
    "# # VGG-16 (Classic architecture)\n",
    "# # model_pretrained = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "# # MODEL_NAME = \"vgg16\"\n",
    "\n",
    "# # ===============================================\n",
    "\n",
    "# print(f\"Loaded pretrained model: {MODEL_NAME}\")\n",
    "# print(f\"Model architecture:\\n{best_model}\")\n",
    "\n",
    "# # ===== STEP 1: Freeze all layers in the feature extractor =====\n",
    "# for param in best_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# print(\"\\nAll feature extractor weights frozen\")\n",
    "\n",
    "# # ===== STEP 2: Replace the classifier head =====\n",
    "# # Different models have different classifier layer names\n",
    "# if MODEL_NAME.startswith('resnet'):\n",
    "#     # ResNet has 'fc' as final layer\n",
    "#     num_features = best_model.fc.in_features\n",
    "#     best_model.fc = nn.Sequential(\n",
    "#         nn.Dropout(DROPOUT_RATE),\n",
    "#         nn.Linear(num_features, 256),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Dropout(DROPOUT_RATE),\n",
    "#         nn.Linear(256, num_classes)\n",
    "#     )\n",
    "#     print(f\"Replaced ResNet classifier: {num_features} -> 256 -> {num_classes}\")\n",
    "    \n",
    "# elif MODEL_NAME.startswith('efficientnet'):\n",
    "#     # EfficientNet has 'classifier' as final layer\n",
    "#     num_features = best_model.classifier[1].in_features\n",
    "#     best_model.classifier = nn.Sequential(\n",
    "#         nn.Dropout(DROPOUT_RATE),\n",
    "#         nn.Linear(num_features, 256),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Dropout(DROPOUT_RATE),\n",
    "#         nn.Linear(256, num_classes)\n",
    "#     )\n",
    "#     print(f\"Replaced EfficientNet classifier: {num_features} -> 256 -> {num_classes}\")\n",
    "    \n",
    "# elif MODEL_NAME.startswith('vgg'):\n",
    "#     # VGG has 'classifier' as a sequential module\n",
    "#     num_features = best_model.classifier[0].in_features\n",
    "#     best_model.classifier = nn.Sequential(\n",
    "#         nn.Dropout(DROPOUT_RATE),\n",
    "#         nn.Linear(num_features, 512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Dropout(DROPOUT_RATE),\n",
    "#         nn.Linear(512, 256),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Dropout(DROPOUT_RATE),\n",
    "#         nn.Linear(256, num_classes)\n",
    "#     )\n",
    "#     print(f\"Replaced VGG classifier: {num_features} -> 512 -> 256 -> {num_classes}\")\n",
    "\n",
    "# best_model.load_state_dict(new_state_dict)\n",
    "\n",
    "# # Move model to device FIRST\n",
    "# best_model = best_model.to(device)\n",
    "\n",
    "# # Then wrap with DataParallel if multiple GPUs are available\n",
    "# if num_gpus > 1:\n",
    "#     best_model = nn.DataParallel(best_model)\n",
    "#     print(f\"Model wrapped with DataParallel for {num_gpus} GPUs\")\n",
    "\n",
    "# EXPERIMENT_NAME = MODEL_NAME+'_recovered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229c1f4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== DUAL-PATH INFERENCE =====\n",
    "\n",
    "if USE_PATCHES:\n",
    "    print(\"=\"*80)\n",
    "    print(\"DUAL-PATH + PATCH-BASED INFERENCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create patch-based test dataset\n",
    "    test_dataset = DualPathTestPatchDataset(\n",
    "        img_dir=test_img_dir,\n",
    "        mask_dir=test_mask_dir,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        stride=PATCH_STRIDE,\n",
    "        transform_img=transform_img,\n",
    "        transform_mask=transform_mask\n",
    "    )\n",
    "    \n",
    "    test_filenames = test_dataset.image_files\n",
    "    patch_to_image = test_dataset.patch_to_image\n",
    "    \n",
    "    # Create test dataloader\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Test dataset created\")\n",
    "    print(f\"  Total test images: {len(test_filenames)}\")\n",
    "    print(f\"  Total test patches: {len(test_dataset)}\")\n",
    "    print(f\"  Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Run inference on patches\n",
    "    patch_preds = []\n",
    "    patch_probs = []\n",
    "    best_model.eval()\n",
    "    \n",
    "    print(f\"\\nRunning dual-path inference on {len(test_dataset)} patches...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            # Move to device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass through dual-path model\n",
    "            logits = best_model(images, masks)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            \n",
    "            patch_preds.append(preds)\n",
    "            patch_probs.append(probs)\n",
    "    \n",
    "    # Combine patch predictions\n",
    "    patch_preds = np.concatenate(patch_preds)\n",
    "    patch_probs = np.concatenate(patch_probs)\n",
    "    \n",
    "    print(f\"âœ“ Got {len(patch_preds)} patch predictions\")\n",
    "    \n",
    "    # Aggregate patches to image-level using soft voting\n",
    "    print(f\"\\nAggregating patches to {len(test_filenames)} images...\")\n",
    "    image_preds = []\n",
    "    \n",
    "    for img_idx in range(len(test_filenames)):\n",
    "        # Find patches for this image\n",
    "        patch_indices = [i for i, pid in enumerate(patch_to_image) if pid == img_idx]\n",
    "        \n",
    "        # Average probabilities (soft voting)\n",
    "        image_patch_probs = patch_probs[patch_indices]\n",
    "        avg_probs = image_patch_probs.mean(axis=0)\n",
    "        final_pred = avg_probs.argmax()\n",
    "        \n",
    "        image_preds.append(final_pred)\n",
    "    \n",
    "    test_preds = np.array(image_preds)\n",
    "    print(f\"âœ“ Aggregated to {len(test_preds)} image predictions\")\n",
    "\n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"SIMPLE DUAL-PATH INFERENCE (FULL IMAGES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Simple dual-path (no patches)\n",
    "    test_dataset = DualPathTestDataset(\n",
    "        img_dir=test_img_dir,\n",
    "        mask_dir=test_mask_dir,\n",
    "        transform_img=transform_img,\n",
    "        transform_mask=transform_mask\n",
    "    )\n",
    "    \n",
    "    test_filenames = test_dataset.image_files\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Test dataset: {len(test_dataset)} images\")\n",
    "    \n",
    "    # Run inference\n",
    "    test_preds = []\n",
    "    best_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            logits = best_model(images, masks)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            test_preds.append(preds)\n",
    "    \n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    print(f\"âœ“ Got {len(test_preds)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b3d5d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create submission CSV\n",
    "label_map = {'Triple negative': 0, 'Luminal A': 1, 'Luminal B': 2, 'HER2(+)': 3}\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_index': test_filenames,\n",
    "    'label': [reverse_label_map[pred] for pred in test_preds]\n",
    "})\n",
    "\n",
    "# Create descriptive filename\n",
    "submission_filename = f\"submission_{EXPERIMENT_NAME}_dual_path.csv\"\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print(f\"\\nâœ“ Submission file created: {submission_filename}\")\n",
    "print(f\"  Total predictions: {len(submission_df)}\")\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission_df.head(10))\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(submission_df['label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
