{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ffa1054",
   "metadata": {},
   "source": [
    "# Kaggle Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e4c0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:51:20.195981Z",
     "iopub.status.busy": "2025-12-10T13:51:20.195828Z",
     "iopub.status.idle": "2025-12-10T13:51:39.621497Z",
     "shell.execute_reply": "2025-12-10T13:51:39.620527Z",
     "shell.execute_reply.started": "2025-12-10T13:51:20.195965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# KAGGLE IMPORTS\n",
    "# Clone repo\n",
    "!git clone https://github.com/francinze/Ch_An2DL.git /kaggle/working/ch2\n",
    "\n",
    "# Install kaggle API\n",
    "!pip install -q kaggle\n",
    "\n",
    "# Configure kaggle.json\n",
    "!mkdir -p /root/.config/kaggle\n",
    "\n",
    "# Copy your kaggle.json there\n",
    "!cp /kaggle/working/ch2/kaggle.json /root/.config/kaggle/\n",
    "\n",
    "# Set correct permissions\n",
    "!chmod 600 /root/.config/kaggle/kaggle.json\n",
    "\n",
    "# Move into the working directory\n",
    "%cd /kaggle/working/ch2/\n",
    "\n",
    "!mkdir -p data\n",
    "!mkdir -p models\n",
    "\n",
    "# Download competition files WITH CORRECT PATH\n",
    "!kaggle competitions download -c an2dl2526c2v2 -p ./data/\n",
    "\n",
    "# Unzip dataset WITH CORRECT PATH\n",
    "!unzip -o ./data/an2dl2526c2v2.zip -d ./data/\n",
    "\n",
    "# Verify download\n",
    "!ls -la ./data/\n",
    "!echo \"Download complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cc23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import Swin_T_Weights\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687201fe",
   "metadata": {},
   "source": [
    "#  Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8eea66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:52:02.534005Z",
     "iopub.status.busy": "2025-12-10T13:52:02.533415Z",
     "iopub.status.idle": "2025-12-10T13:52:02.795231Z",
     "shell.execute_reply": "2025-12-10T13:52:02.794629Z",
     "shell.execute_reply.started": "2025-12-10T13:52:02.533965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PATH_PREFIX = ''\n",
    "if os.path.exists('./data/train_data'):\n",
    "    PATH_PREFIX = './'\n",
    "    print(\"Found ./data/train_data (Local or Colab)\")\n",
    "elif os.path.exists('/kaggle/input/an2dl-breast-cancer-classification/data/train_data'):\n",
    "    PATH_PREFIX = '/kaggle/input/an2dl-breast-cancer-classification/'\n",
    "    print(\"Found /data/train_data (Kaggle)\")\n",
    "elif os.path.exists('data/train_data'):\n",
    "    PATH_PREFIX = ''\n",
    "    print(\"Found data/train_data (Current directory)\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Data directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ccb0d",
   "metadata": {},
   "source": [
    "## Organize Data by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a05d7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:52:08.427564Z",
     "iopub.status.busy": "2025-12-10T13:52:08.426872Z",
     "iopub.status.idle": "2025-12-10T13:52:09.285945Z",
     "shell.execute_reply": "2025-12-10T13:52:09.285285Z",
     "shell.execute_reply.started": "2025-12-10T13:52:08.427537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Detect environment and set appropriate path prefix\n",
    "if os.path.exists('./data/train_data'):\n",
    "    PATH_PREFIX = './'\n",
    "    print(\"Found ./data/train_data (Local or Colab)\")\n",
    "elif os.path.exists('/data/train_data'):\n",
    "    PATH_PREFIX = '/'\n",
    "    print(\"Found /data/train_data (Kaggle)\")\n",
    "elif os.path.exists('data/train_data'):\n",
    "    PATH_PREFIX = ''\n",
    "    print(\"Found data/train_data (Current directory)\")\n",
    "else:\n",
    "    print(\"✗ Data not found in expected locations!\")\n",
    "    PATH_PREFIX = '/'\n",
    "\n",
    "print(f\"Using PATH_PREFIX: {PATH_PREFIX}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ORGANIZING DATA INTO SEPARATE DIRECTORIES BY TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define source directories\n",
    "train_data_dir = PATH_PREFIX + 'data/train_data/'\n",
    "test_data_dir = PATH_PREFIX + 'data/test_data/'\n",
    "\n",
    "# Define target directories for organized data\n",
    "train_img_dir = PATH_PREFIX + 'data/train_img/'\n",
    "train_mask_dir = PATH_PREFIX + 'data/train_mask/'\n",
    "test_img_dir = PATH_PREFIX + 'data/test_img/'\n",
    "test_mask_dir = PATH_PREFIX + 'data/test_mask/'\n",
    "\n",
    "train_labels = pd.read_csv(PATH_PREFIX + 'data/train_labels.csv')\n",
    "\n",
    "# Create target directories if they don't exist\n",
    "for directory in [train_img_dir, train_mask_dir, test_img_dir, test_mask_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Function to organize files by type\n",
    "def organize_data_by_type(source_dir, img_dir, mask_dir):\n",
    "    \"\"\"\n",
    "    Move image and mask files from source directory to separate directories.\n",
    "    Only moves files if they don't already exist in the target directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(source_dir):\n",
    "        print(f\"⚠ Warning: Source directory not found: {source_dir}\")\n",
    "        return 0, 0\n",
    "    \n",
    "    files = os.listdir(source_dir)\n",
    "    img_count = 0\n",
    "    mask_count = 0\n",
    "    \n",
    "    for filename in files:\n",
    "        source_path = os.path.join(source_dir, filename)\n",
    "        \n",
    "        # Skip if not a file\n",
    "        if not os.path.isfile(source_path):\n",
    "            continue\n",
    "        \n",
    "        # Determine target directory based on filename prefix\n",
    "        if filename.startswith('img_'):\n",
    "            target_path = os.path.join(img_dir, filename)\n",
    "            if not os.path.exists(target_path):\n",
    "                shutil.copy2(source_path, target_path)\n",
    "                img_count += 1\n",
    "        elif filename.startswith('mask_'):\n",
    "            target_path = os.path.join(mask_dir, filename)\n",
    "            if not os.path.exists(target_path):\n",
    "                shutil.copy2(source_path, target_path)\n",
    "                mask_count += 1\n",
    "    \n",
    "    return img_count, mask_count\n",
    "\n",
    "# Organize training data\n",
    "print(\"\\nOrganizing training data...\")\n",
    "train_img_moved, train_mask_moved = organize_data_by_type(\n",
    "    train_data_dir, train_img_dir, train_mask_dir\n",
    ")\n",
    "print(f\"  Images: {train_img_moved} files copied to {train_img_dir}\")\n",
    "print(f\"  Masks: {train_mask_moved} files copied to {train_mask_dir}\")\n",
    "\n",
    "# Organize test data\n",
    "print(\"\\nOrganizing test data...\")\n",
    "test_img_moved, test_mask_moved = organize_data_by_type(\n",
    "    test_data_dir, test_img_dir, test_mask_dir\n",
    ")\n",
    "print(f\"  Images: {test_img_moved} files copied to {test_img_dir}\")\n",
    "print(f\"  Masks: {test_mask_moved} files copied to {test_mask_dir}\")\n",
    "\n",
    "# Verify organization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA ORGANIZATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train images: {len(os.listdir(train_img_dir)) if os.path.exists(train_img_dir) else 0} files in {train_img_dir}\")\n",
    "print(f\"Train masks: {len(os.listdir(train_mask_dir)) if os.path.exists(train_mask_dir) else 0} files in {train_mask_dir}\")\n",
    "print(f\"Test images: {len(os.listdir(test_img_dir)) if os.path.exists(test_img_dir) else 0} files in {test_img_dir}\")\n",
    "print(f\"Test masks: {len(os.listdir(test_mask_dir)) if os.path.exists(test_mask_dir) else 0} files in {test_mask_dir}\")\n",
    "print(\"=\"*80)\n",
    "print(\"Data organization complete!\")\n",
    "print(\"  - Organized copies are in train_img/, train_mask/, test_img/, test_mask/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af21e5c",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af5838",
   "metadata": {},
   "source": [
    "## Remove Shrek & Slimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef36cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:52:17.308775Z",
     "iopub.status.busy": "2025-12-10T13:52:17.308367Z",
     "iopub.status.idle": "2025-12-10T13:52:17.347904Z",
     "shell.execute_reply": "2025-12-10T13:52:17.347144Z",
     "shell.execute_reply.started": "2025-12-10T13:52:17.308737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parse the contaminated indices from the text file\n",
    "contaminated_indices = []\n",
    "with open('shrek_and_slimes.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and line.isdigit():\n",
    "            contaminated_indices.append(int(line))\n",
    "\n",
    "print(f\"Found {len(contaminated_indices)} contaminated samples to remove\")\n",
    "\n",
    "# Define directories to clean (both img and mask directories)\n",
    "train_img_dir_clean = PATH_PREFIX + 'data/train_img/'\n",
    "train_mask_dir_clean = PATH_PREFIX + 'data/train_mask/'\n",
    "\n",
    "# Remove corresponding image and mask files from both directories\n",
    "removed_count = 0\n",
    "for idx in contaminated_indices:\n",
    "    img_name = f'img_{idx:04d}.png'\n",
    "    mask_name = f'mask_{idx:04d}.png'\n",
    "    \n",
    "    # Remove from train_img directory\n",
    "    img_path = os.path.join(train_img_dir_clean, img_name)\n",
    "    if os.path.exists(img_path):\n",
    "        os.remove(img_path)\n",
    "        removed_count += 1\n",
    "    \n",
    "    # Remove from train_mask directory\n",
    "    mask_path = os.path.join(train_mask_dir_clean, mask_name)\n",
    "    if os.path.exists(mask_path):\n",
    "        os.remove(mask_path)\n",
    "        removed_count += 1\n",
    "\n",
    "print(f\"Removed {removed_count} files from organized directories\")\n",
    "\n",
    "# Update train_labels by removing contaminated indices\n",
    "train_labels = train_labels[~train_labels['sample_index'].str.extract(r'(\\d+)')[0].astype(int).isin(contaminated_indices)]\n",
    "print(f\"Training labels updated: {len(train_labels)} samples remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3, figsize=(4, 4))\n",
    "fig.suptitle('Samples from Each Class', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "class_names = ['Luminal A', 'Luminal B', 'HER2(+)', 'Triple negative']\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12']\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    class_samples = train_labels[train_labels['label'] == class_name]['sample_index'].tolist()\n",
    "    selected_samples = random.sample(class_samples, min(3, len(class_samples)))\n",
    "    \n",
    "    for col_idx, sample_idx in enumerate(selected_samples):\n",
    "        ax = axes[class_idx, col_idx]\n",
    "        \n",
    "        img_path = os.path.join(train_img_dir, str(sample_idx))\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path)\n",
    "            ax.imshow(img)\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(colors[class_idx])\n",
    "            spine.set_linewidth(3)\n",
    "        \n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(class_name, fontsize=11, fontweight='bold', \n",
    "                         rotation=0, ha='right', va='center', labelpad=40)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_images_grid.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "original_counts = train_labels['label'].value_counts().sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12']\n",
    "class_names = ['Luminal A', 'Luminal B', 'HER2(+)', 'Triple Negative']\n",
    "\n",
    "bars = ax.bar(range(len(original_counts)), original_counts.values, \n",
    "              color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i, (bar, count) in enumerate(zip(bars, original_counts.values)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(count)}',\n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('# Samples', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Class Distribution After Data Cleaning\\n(692 → 581 samples)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(range(len(class_names)))\n",
    "ax.set_xticklabels(class_names, fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, max(original_counts.values) * 1.15)\n",
    "\n",
    "total = sum(original_counts.values)\n",
    "ax.text(0.98, 0.98, f'Total: {total} samples', \n",
    "        transform=ax.transAxes, ha='right', va='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "        fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86386251",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4e14e-b5c7-4134-a63a-108cccf383df",
   "metadata": {},
   "source": [
    "## Masks as Focus Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4165b80-ee19-4d1f-80e0-c61896dc68fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:52:22.165747Z",
     "iopub.status.busy": "2025-12-10T13:52:22.165409Z",
     "iopub.status.idle": "2025-12-10T13:52:23.889737Z",
     "shell.execute_reply": "2025-12-10T13:52:23.889007Z",
     "shell.execute_reply.started": "2025-12-10T13:52:22.165719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_mask(image, mask):\n",
    "    \"\"\"Apply binary mask to image. Masked pixels are set to black.\"\"\"\n",
    "    img_np = np.array(image).astype(np.uint8)\n",
    "    mask_np = np.array(mask).astype(np.uint8) / 255.0\n",
    "    \n",
    "    if img_np.ndim == 3:\n",
    "        mask_np = np.expand_dims(mask_np, axis=-1)\n",
    "    \n",
    "    masked_img_np = (img_np * mask_np).astype(np.uint8)\n",
    "    return Image.fromarray(masked_img_np)\n",
    "\n",
    "samples = sorted(os.listdir(train_img_dir))[:4]\n",
    "fig, axes = plt.subplots(len(samples), 2, figsize=(12, 2 * len(samples)))\n",
    "\n",
    "for i, img_name in enumerate(samples):\n",
    "    img_path = os.path.join(train_img_dir, img_name)\n",
    "    mask_path = os.path.join(train_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "    masked_image = apply_mask(image, mask)\n",
    "\n",
    "    axes[i, 0].imshow(image)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    axes[i, 1].imshow(masked_image)\n",
    "    axes[i, 1].set_title(\"Masked Image\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed302cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:52:29.934788Z",
     "iopub.status.busy": "2025-12-10T13:52:29.934226Z",
     "iopub.status.idle": "2025-12-10T13:54:36.240682Z",
     "shell.execute_reply": "2025-12-10T13:54:36.240017Z",
     "shell.execute_reply.started": "2025-12-10T13:52:29.934764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save masked images to a new directory\n",
    "masked_train_img_dir = PATH_PREFIX + 'data/train_img_masked/'\n",
    "os.makedirs(masked_train_img_dir, exist_ok=True)\n",
    "masked_test_img_dir = PATH_PREFIX + 'data/test_img_masked/'\n",
    "os.makedirs(masked_test_img_dir, exist_ok=True)\n",
    "\n",
    "# Apply masking to all training images and save\n",
    "train_img_files = sorted(os.listdir(train_img_dir))\n",
    "\n",
    "for img_name in tqdm(train_img_files, desc=\"Processing training images\"):\n",
    "    img_path = os.path.join(train_img_dir, img_name)\n",
    "    mask_path = os.path.join(train_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "    masked_image = apply_mask(image, mask)\n",
    "    masked_image.save(os.path.join(masked_train_img_dir, img_name))\n",
    "print(\"Masked training images saved.\")\n",
    "\n",
    "# Apply masking to all test images and save\n",
    "test_img_files = sorted(os.listdir(test_img_dir))\n",
    "for img_name in tqdm(test_img_files, desc=\"Processing test images\"):\n",
    "    img_path = os.path.join(test_img_dir, img_name)\n",
    "    mask_path = os.path.join(test_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "    masked_image = apply_mask(image, mask)\n",
    "    masked_image.save(os.path.join(masked_test_img_dir, img_name))\n",
    "\n",
    "print(\"Masked test images saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f213e",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b4216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:54:43.071884Z",
     "iopub.status.busy": "2025-12-10T13:54:43.071581Z",
     "iopub.status.idle": "2025-12-10T13:54:43.463588Z",
     "shell.execute_reply": "2025-12-10T13:54:43.463044Z",
     "shell.execute_reply.started": "2025-12-10T13:54:43.071861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze class distribution after removal\n",
    "class_distribution = train_labels['label'].value_counts().sort_index()\n",
    "\n",
    "# Class with the most samples (majority)\n",
    "max_class = class_distribution.max()\n",
    "max_class_name = class_distribution.idxmax()\n",
    "print(f\"\\nClass with the most samples (Majority): {max_class_name} ({max_class} samples)\")\n",
    "\n",
    "# Class with the fewest samples (minority)\n",
    "min_class = class_distribution.min()\n",
    "min_class_name = class_distribution.idxmin()\n",
    "print(f\"Class with the fewest samples (Minority): {min_class_name} ({min_class} samples)\")\n",
    "\n",
    "# Imbalance ratio\n",
    "imbalance_ratio = max_class / min_class\n",
    "print(f\"\\nImbalance ratio (Max/Min): {imbalance_ratio:.2f}x\")\n",
    "\n",
    "# Augmentation strategy\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Augmentation Strategy:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAugmentations to apply:\")\n",
    "print(\"  1. Horizontal Flip (p=0.5)\")\n",
    "print(\"  2. Vertical Flip (p=0.5)\")\n",
    "print(\"  3. Random Translation (0.2, 0.2)\")\n",
    "print(\"  4. Random Zoom/Scale (0.8, 1.2)\")\n",
    "\n",
    "# Target number of samples for each class\n",
    "target_samples = 250\n",
    "\n",
    "print(f\"\\nTarget: {target_samples} samples for EACH class\")\n",
    "\n",
    "augmentation_strategy_balanced = {}\n",
    "total_to_generate = 0\n",
    "\n",
    "for class_name in class_distribution.index:\n",
    "    n_samples = class_distribution[class_name]\n",
    "    n_needed = target_samples - n_samples\n",
    "    n_augmentations = max(0, n_needed)  # We cannot have negative augmentations\n",
    "    \n",
    "    augmentation_strategy_balanced[class_name] = {\n",
    "        'original': n_samples,\n",
    "        'target': target_samples,\n",
    "        'augment_count': n_augmentations,\n",
    "        'ratio_multiplier': n_augmentations / n_samples if n_samples > 0 else 0\n",
    "    }\n",
    "    \n",
    "    total_to_generate += n_augmentations\n",
    "\n",
    "# Projection of the dataset after augmentation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data after balanced augmentation:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Class':<20} {'Original':<15} {'New Augment':<15} {'Augmentations per image':<25} {'Total':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "total_original = 0\n",
    "total_augmented = 0\n",
    "for class_name in class_distribution.index:\n",
    "    n_original = class_distribution[class_name]\n",
    "    n_aug = augmentation_strategy_balanced[class_name]['augment_count']\n",
    "    n_total = n_original + n_aug\n",
    "    \n",
    "    total_original += n_original\n",
    "    total_augmented += n_total\n",
    "    \n",
    "    print(f\"{class_name:<20} {n_original:<15} {n_aug:<15} {augmentation_strategy_balanced[class_name]['ratio_multiplier']:<25.2f} {n_total:<15}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'TOTAL':<20} {total_original:<15} {total_to_generate:<15} {np.mean([augmentation_strategy_balanced[class_name]['ratio_multiplier'] for class_name in class_distribution.index]):<25.2f} {total_augmented:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722ec8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:54:54.084275Z",
     "iopub.status.busy": "2025-12-10T13:54:54.083581Z",
     "iopub.status.idle": "2025-12-10T13:55:41.586288Z",
     "shell.execute_reply": "2025-12-10T13:55:41.585469Z",
     "shell.execute_reply.started": "2025-12-10T13:54:54.084250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create folder for augmented data if it doesn't exist\n",
    "augmented_dir = PATH_PREFIX + f'data/train_data_augmented/'\n",
    "train_dir = PATH_PREFIX + 'data/train_img_cropped/'\n",
    "if not os.path.exists(augmented_dir):\n",
    "    os.makedirs(augmented_dir)\n",
    "    print(f\"Created directory: {augmented_dir}\")\n",
    "else:\n",
    "    existing_files = len(os.listdir(augmented_dir))\n",
    "    print(f\"Directory already exists: {augmented_dir}\")\n",
    "    print(f\"Found {existing_files} existing augmented files\")\n",
    "\n",
    "# Define augmentations for each class\n",
    "# Using expand=True in RandomRotation to preserve aspect ratio and prevent cropping\n",
    "augmentation_transforms = {\n",
    "    'flip': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(180, expand=True, fill=0)  # expand=True prevents cropping, fill=0 for black padding\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"\\nStarting augmentation\")\n",
    "\n",
    "# Loop through each class and generate augmentations\n",
    "total_augmented = 0\n",
    "augmented_rows = []\n",
    "\n",
    "for class_name in sorted(augmentation_strategy_balanced.keys()):\n",
    "    info = augmentation_strategy_balanced[class_name]\n",
    "    n_augment = info['augment_count']\n",
    "    \n",
    "    if n_augment == 0:\n",
    "        print(f\"\\n{class_name}: No augmentation needed (already at target)\")\n",
    "        continue\n",
    "    \n",
    "    # Get original images of this class\n",
    "    class_samples = train_labels[train_labels['label'] == class_name]['sample_index'].tolist()\n",
    "    n_original = len(class_samples)\n",
    "    \n",
    "    # Calculate how many augmentations per original image\n",
    "    aug_per_img = n_augment / n_original\n",
    "    \n",
    "    # For each original image\n",
    "    aug_count = 0\n",
    "    for img_idx, img_name in enumerate(tqdm(class_samples, desc=f\"Augmenting {class_name}\")):        \n",
    "        img_path = os.path.join(masked_train_img_dir, img_name)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"  File not found: {img_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Load the original image/mask\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_pil = img.copy()\n",
    "        \n",
    "        # Generate augmentations for this image\n",
    "        n_to_generate = int(np.ceil(aug_per_img)) if img_idx < n_augment % n_original else int(np.floor(aug_per_img))\n",
    "        \n",
    "        for aug_num in range(n_to_generate):\n",
    "            if aug_count <= n_augment:\n",
    "                base_name = img_name.replace('.png', '')\n",
    "\n",
    "                # Choose an augmentation type cyclically\n",
    "                aug_types = list(augmentation_transforms.keys())\n",
    "                aug_type = aug_types[aug_count % len(aug_types)]\n",
    "                transform = augmentation_transforms[aug_type]\n",
    "                img_augmented = transform(img_pil)\n",
    "                augmented_img_name = f\"{base_name}_aug_{aug_num}_{aug_type}.png\"\n",
    "                \n",
    "                # Save augmented image\n",
    "                augmented_img_path = os.path.join(augmented_dir, augmented_img_name)\n",
    "                img_augmented.save(augmented_img_path)\n",
    "                \n",
    "                # Add new row for augmented image\n",
    "                augmented_rows.append({'sample_index': augmented_img_name, 'label': class_name})\n",
    "                \n",
    "            aug_count += 1\n",
    "    \n",
    "    total_augmented += aug_count\n",
    "    print(f\"  {class_name}: Completed. {aug_count} augmentations generated\")\n",
    "\n",
    "# Update train_labels with augmented files\n",
    "if augmented_rows:\n",
    "    train_labels = pd.concat([train_labels, pd.DataFrame(augmented_rows)], ignore_index=True)\n",
    "    print(f\"train_labels updated: {len(train_labels)} rows (including augmentations)\")\n",
    "\n",
    "print(f\"Total augmented images generated: {total_augmented}\")\n",
    "print(f\"Save directory: {augmented_dir}\")\n",
    "\n",
    "# Verify file count\n",
    "augmented_files = os.listdir(augmented_dir)\n",
    "print(f\"\\nFiles in augmented folder: {len(augmented_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c11365",
   "metadata": {},
   "source": [
    "## Crop Masked Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3ab60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:56:22.753161Z",
     "iopub.status.busy": "2025-12-10T13:56:22.752048Z",
     "iopub.status.idle": "2025-12-10T13:56:24.539240Z",
     "shell.execute_reply": "2025-12-10T13:56:24.538579Z",
     "shell.execute_reply.started": "2025-12-10T13:56:22.753127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MIN_IMG_SIZE = (256, 256)\n",
    "\n",
    "def crop_to_mask(image, min_size=MIN_IMG_SIZE):\n",
    "    \"\"\"Crop image to bounding box of non-zero pixels and pad if needed.\"\"\"\n",
    "    img_np = np.array(image).astype(np.uint8)\n",
    "    non_zero_mask = np.sum(img_np, axis=2) > 0\n",
    "    coords = np.column_stack(np.where(non_zero_mask))\n",
    "    \n",
    "    if coords.size == 0:\n",
    "        return image\n",
    "    \n",
    "    y_min, x_min = coords.min(axis=0)\n",
    "    y_max, x_max = coords.max(axis=0) + 1\n",
    "    cropped = image.crop((x_min, y_min, x_max, y_max))\n",
    "    \n",
    "    width, height = cropped.size\n",
    "    min_width, min_height = min_size\n",
    "    \n",
    "    if width < min_width or height < min_height:\n",
    "        pad_width = max(0, min_width - width)\n",
    "        pad_height = max(0, min_height - height)\n",
    "        padded = Image.new('RGB', (max(width, min_width), max(height, min_height)), (0, 0, 0))\n",
    "        padded.paste(cropped, (pad_width // 2, pad_height // 2))\n",
    "        return padded\n",
    "    \n",
    "    return cropped\n",
    "\n",
    "samples = sorted(os.listdir(masked_train_img_dir))[:4]\n",
    "fig, axes = plt.subplots(len(samples), 2, figsize=(12, 3 * len(samples)))\n",
    "\n",
    "for i, img_name in enumerate(samples):\n",
    "    img_path = os.path.join(masked_train_img_dir, img_name)\n",
    "    mask_path = os.path.join(train_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "    masked_image = apply_mask(image, mask)\n",
    "    cropped_image = crop_to_mask(masked_image)\n",
    "\n",
    "    axes[i, 0].imshow(masked_image)\n",
    "    axes[i, 0].set_title(\"Masked Image\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    axes[i, 1].imshow(cropped_image)\n",
    "    axes[i, 1].set_title(f\"Cropped Image\\n{cropped_image.size[0]}x{cropped_image.size[1]}\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473d6cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:56:33.277052Z",
     "iopub.status.busy": "2025-12-10T13:56:33.276322Z",
     "iopub.status.idle": "2025-12-10T13:58:39.297028Z",
     "shell.execute_reply": "2025-12-10T13:58:39.296250Z",
     "shell.execute_reply.started": "2025-12-10T13:56:33.277026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Crop all images and save to new directories\n",
    "# Define new directories for cropped images\n",
    "cropped_train_img_dir = PATH_PREFIX + 'data/train_img_cropped/'\n",
    "cropped_test_img_dir = PATH_PREFIX + 'data/test_img_cropped/'\n",
    "os.makedirs(cropped_train_img_dir, exist_ok=True)\n",
    "os.makedirs(cropped_test_img_dir, exist_ok=True)\n",
    "\n",
    "# Process training images\n",
    "for img_name in tqdm(os.listdir(masked_train_img_dir), desc=\"Processing training images\"):\n",
    "    img_path = os.path.join(masked_train_img_dir, img_name)\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    cropped_image = crop_to_mask(image)\n",
    "\n",
    "    cropped_image.save(os.path.join(cropped_train_img_dir, img_name))\n",
    "print(\"\\nCropped training images saved.\")\n",
    "# Process test images\n",
    "for img_name in tqdm(os.listdir(masked_test_img_dir), desc=\"Processing test images\"):\n",
    "    img_path = os.path.join(masked_test_img_dir, img_name)\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    cropped_image = crop_to_mask(image)\n",
    "\n",
    "    cropped_image.save(os.path.join(cropped_test_img_dir, img_name))\n",
    "print(\"\\nCropped test images saved.\")\n",
    "\n",
    "# Crop also all augmented images\n",
    "for img_name in tqdm(os.listdir(augmented_dir), desc=\"Processing augmented images\"):\n",
    "    img_path = os.path.join(augmented_dir, img_name)\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    cropped_image = crop_to_mask(image)\n",
    "\n",
    "    cropped_image.save(os.path.join(cropped_train_img_dir, img_name))\n",
    "print(\"\\nCropped augmented images saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca46753",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d99f57",
   "metadata": {},
   "source": [
    "## Blank Patch Filtering\n",
    "\n",
    "Patches with zero or near-zero information content (black backgrounds, empty borders) are filtered using variance-based thresholding:\n",
    "- Variance threshold: 0.001\n",
    "- Only patches with variance > threshold are included\n",
    "- Applied during patch extraction for both training and test data\n",
    "\n",
    "This reduces dataset size and improves training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe745789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:59:06.051051Z",
     "iopub.status.busy": "2025-12-10T13:59:06.050497Z",
     "iopub.status.idle": "2025-12-10T13:59:41.633029Z",
     "shell.execute_reply": "2025-12-10T13:59:41.632323Z",
     "shell.execute_reply.started": "2025-12-10T13:59:06.051026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "imagenet_norm = transforms.Normalize(\n",
    "    mean=(0.485, 0.456, 0.406),\n",
    "    std=(0.229, 0.224, 0.225)\n",
    ")\n",
    "\n",
    "# ===== PATCH-BASED PROCESSING SETTINGS =====\n",
    "# Instead of resizing images to 224x224 (which loses detail),\n",
    "# we extract patches at full resolution and process them separately\n",
    "PATCH_SIZE = 256  # Size of each patch (256x256)\n",
    "PATCH_STRIDE = 100\n",
    "# ==========================================\n",
    "\n",
    "# For backward compatibility when not using patches\n",
    "IMG_SIZE = (PATCH_SIZE, PATCH_SIZE)\n",
    "\n",
    "# Create DataLoaders\n",
    "# Larger batch size for multi-GPU: DataParallel splits batch across GPUs\n",
    "# With 2 GPUs: effective batch per GPU = BATCH_SIZE / 2\n",
    "BATCH_SIZE = 128  # 64 per GPU with DataParallel\n",
    "\n",
    "# ===== GPU OPTIMIZATION SETTINGS =====\n",
    "# Kaggle T4 x2 optimization: maximize data loading throughput\n",
    "# Each T4 GPU can handle 4 workers efficiently\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    NUM_WORKERS = 2 * torch.cuda.device_count()  # 2 workers per GPU = 4 total for T4 x2\n",
    "    PIN_MEMORY = True  # CRITICAL: Enables async CPU-to-GPU transfer while GPU computes\n",
    "    PERSISTENT_WORKERS = True  # Keeps workers alive between epochs (faster)\n",
    "    print(f\"GPU detected: {torch.cuda.device_count()} GPU(s) - using {NUM_WORKERS} workers\")\n",
    "else:\n",
    "    # CPU-only environment\n",
    "    NUM_WORKERS = 0\n",
    "    PIN_MEMORY = False\n",
    "    PERSISTENT_WORKERS = False\n",
    "# ======================================\n",
    "\n",
    "# Load original + augmented images into tensors\n",
    "print(\"Loading cropped images from disk\")\n",
    "print(f\"Cropped directory: {cropped_train_img_dir}\")\n",
    "\n",
    "# Check if augmented directory exists and validate files\n",
    "if not os.path.exists(augmented_dir):\n",
    "    print(f\"\\nWARNING: Augmented directory does not exist!\")\n",
    "    print(f\"Expected: {augmented_dir}\")\n",
    "    print(f\"No augmented data will be loaded. Only original images will be used.\")\n",
    "    augmented_files = []\n",
    "else:\n",
    "    # Create list of augmented images\n",
    "    augmented_files = os.listdir(augmented_dir)\n",
    "    print(f\"Augmented images found: {len(augmented_files)}\")\n",
    "\n",
    "# Create new dataframe with all images (original + augmented)\n",
    "train_labels_augmented = train_labels.copy()\n",
    "\n",
    "# Add augmented images\n",
    "augmented_rows = []\n",
    "for aug_img_name in augmented_files:\n",
    "    # Extract original file name (works for both img_ and mask_ prefixes)\n",
    "    # Format: {prefix}_{number}_aug_{aug_num}_{aug_type}.png\n",
    "    base_name = aug_img_name.split('_aug_')[0] + '.png'\n",
    "    \n",
    "    original_row = train_labels[train_labels['sample_index'] == base_name]\n",
    "    if not original_row.empty:\n",
    "        class_label = original_row.iloc[0]['label']\n",
    "        augmented_rows.append({'sample_index': aug_img_name, 'label': class_label})\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "train_labels_augmented = pd.concat([train_labels_augmented, augmented_df], ignore_index=True)\n",
    "\n",
    "print(f\"\\nOriginal dataset: {len(train_labels)} samples\")\n",
    "print(f\"Augmented dataset: {len(train_labels_augmented)} samples\")\n",
    "print(f\"\\nDistribution in augmented dataset:\")\n",
    "print(train_labels_augmented['label'].value_counts().sort_index())\n",
    "\n",
    "print(\"Patch pre-extraction to disk\")\n",
    "\n",
    "# Directory to save pre-extracted patches\n",
    "patches_dir = os.path.join(PATH_PREFIX, \"data\", \"patches_cache\")\n",
    "os.makedirs(patches_dir, exist_ok=True)\n",
    "\n",
    "# Metadata file to track all batch files\n",
    "metadata_file = os.path.join(patches_dir, f\"metadata_ps{PATCH_SIZE}_stride{PATCH_STRIDE}.json\")\n",
    "\n",
    "if os.path.exists(metadata_file):\n",
    "    print(f\"Found pre-extracted patches. Loading metadata from {metadata_file}...\")\n",
    "    \n",
    "    import json\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"Found {metadata['num_batches']} batch files with {metadata['total_patches']} total patches\")\n",
    "    print(f\"Found {metadata['num_batches']} batch files with {metadata['total_patches']} total patches\")\n",
    "    \n",
    "else:\n",
    "    print(\"No pre-extracted patches found. Extracting now...\")\n",
    "    print(\"Strategy: Save each batch to separate file to minimize memory usage.\")\n",
    "    \n",
    "    # Helper function to extract patches from a single image\n",
    "    def extract_patches_from_image(img_path, patch_size, stride, min_variance_threshold=0.001):\n",
    "        \"\"\"\n",
    "        Extract all patches from a single image, filtering out blank patches.\n",
    "        \n",
    "        Args:\n",
    "            img_path: Path to the image file\n",
    "            patch_size: Size of each patch\n",
    "            stride: Stride for patch extraction\n",
    "            min_variance_threshold: Minimum variance to consider patch non-blank (default: 0.001)\n",
    "        \n",
    "        Returns:\n",
    "            List of patch tensors (excluding blank patches)\n",
    "        \"\"\"\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        img_array = np.array(img, dtype=np.float32)\n",
    "        h, w, c = img_array.shape\n",
    "        patches = []\n",
    "        \n",
    "        if h < patch_size or w < patch_size:\n",
    "            pad_h = max(0, patch_size - h)\n",
    "            pad_w = max(0, patch_size - w)\n",
    "            img_array = np.pad(img_array, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "            patch = img_array[:patch_size, :patch_size, :]\n",
    "            \n",
    "            # Check if patch is not blank\n",
    "            patch_normalized = patch / 255.0\n",
    "            if np.var(patch_normalized) > min_variance_threshold:\n",
    "                patch_tensor = torch.from_numpy(patch.transpose(2, 0, 1) / 255.0).float()\n",
    "                patches.append(patch_tensor)\n",
    "        else:\n",
    "            n_patches_h = max(1, (h - patch_size) // stride + 1)\n",
    "            n_patches_w = max(1, (w - patch_size) // stride + 1)\n",
    "            \n",
    "            for row_idx in range(n_patches_h):\n",
    "                for col_idx in range(n_patches_w):\n",
    "                    start_h = min(row_idx * stride, h - patch_size)\n",
    "                    start_w = min(col_idx * stride, w - patch_size)\n",
    "                    patch = img_array[start_h:start_h+patch_size, start_w:start_w+patch_size, :]\n",
    "                    \n",
    "                    # Check if patch is not blank (has sufficient variance)\n",
    "                    patch_normalized = patch / 255.0\n",
    "                    if np.var(patch_normalized) > min_variance_threshold:\n",
    "                        patch_tensor = torch.from_numpy(patch.transpose(2, 0, 1) / 255.0).float()\n",
    "                        patches.append(patch_tensor)\n",
    "        \n",
    "        return patches\n",
    "    \n",
    "    label_map = {'Triple negative': 0, 'Luminal A': 1, 'Luminal B': 2, 'HER2(+)': 3}\n",
    "    IMAGES_PER_BATCH = 50  # Process 50 images at a time (stride=256 = 4x fewer patches)\n",
    "    \n",
    "    print(f\"\\nExtracting and saving patches in batches of {IMAGES_PER_BATCH} images...\")\n",
    "    \n",
    "    batch_patches = []\n",
    "    batch_labels = []\n",
    "    batch_num = 0\n",
    "    total_patches = 0\n",
    "    batch_files = []\n",
    "    \n",
    "    for idx, row in train_labels_augmented.iterrows():\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"  Progress: {idx}/{len(train_labels_augmented)} images...\")\n",
    "        \n",
    "        img_name = row['sample_index']\n",
    "        label = label_map[row['label']]\n",
    "        \n",
    "        img_path = os.path.join(cropped_train_img_dir, img_name)\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            patches = extract_patches_from_image(img_path, PATCH_SIZE, PATCH_STRIDE)\n",
    "            batch_patches.extend(patches)\n",
    "            batch_labels.extend([label] * len(patches))\n",
    "        \n",
    "        # Save batch to disk when full (DON'T accumulate in memory!)\n",
    "        if (idx + 1) % IMAGES_PER_BATCH == 0 or idx == len(train_labels_augmented) - 1:\n",
    "            if len(batch_patches) > 0:\n",
    "                batch_tensor = torch.stack(batch_patches)\n",
    "                batch_labels_tensor = torch.tensor(batch_labels, dtype=torch.long)\n",
    "                \n",
    "                # Save this batch to its own file\n",
    "                batch_file = os.path.join(patches_dir, f\"batch_{batch_num:03d}.pt\")\n",
    "                torch.save({'patches': batch_tensor, 'labels': batch_labels_tensor}, batch_file)\n",
    "                \n",
    "                batch_files.append(batch_file)\n",
    "                total_patches += len(batch_patches)\n",
    "                print(f\"    ✓ Saved batch {batch_num}: {len(batch_patches)} patches → {batch_file}\")\n",
    "                \n",
    "                # CRITICAL: Clear memory immediately!\n",
    "                batch_patches = []\n",
    "                batch_labels = []\n",
    "                batch_num += 1\n",
    "    \n",
    "    # Save metadata\n",
    "    import json\n",
    "    metadata = {\n",
    "        'num_batches': batch_num,\n",
    "        'total_patches': total_patches,\n",
    "        'batch_files': batch_files,\n",
    "        'patch_size': PATCH_SIZE,\n",
    "        'stride': PATCH_STRIDE\n",
    "    }\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(\"\\nExtraction complete\")\n",
    "    print(f\"Saved {batch_num} batch files with {total_patches} total patches\")\n",
    "    print(f\"Metadata saved to: {metadata_file}\")\n",
    "\n",
    "# Load metadata for dataset creation\n",
    "import json\n",
    "with open(metadata_file, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Creating multi-file dataset...\")\n",
    "print(f\"Total patches: {metadata['total_patches']}\")\n",
    "\n",
    "# Custom Dataset that loads from multiple batch files\n",
    "class MultiFilePatchDataset(Dataset):\n",
    "    \"\"\"Dataset that loads patches from multiple batch files with caching\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata, use_pretrained: bool = False, transform=None):\n",
    "        self.batch_files = metadata['batch_files']\n",
    "        self.total_patches = int(metadata['total_patches'])\n",
    "\n",
    "        self.use_pretrained = bool(use_pretrained)\n",
    "        self.transform = transform  # opzionale: callable(x)->x\n",
    "\n",
    "        # Cache per batch (key: batch_idx, value: batch_data)\n",
    "        self._cache = {}\n",
    "\n",
    "        # Indici per mappare idx globale -> (batch_idx, local_idx)\n",
    "        self.batch_sizes = []\n",
    "        self.cumulative_sizes = [0]\n",
    "\n",
    "        print(f\"Loading batch data from {len(self.batch_files)} files...\")\n",
    "        for batch_idx, batch_file in enumerate(self.batch_files):\n",
    "            batch_data = torch.load(batch_file, map_location=\"cpu\")\n",
    "            # attesi: batch_data['patches'], batch_data['labels']\n",
    "            n = batch_data['labels'].shape[0]\n",
    "\n",
    "            self._cache[batch_idx] = batch_data\n",
    "            self.batch_sizes.append(n)\n",
    "            self.cumulative_sizes.append(self.cumulative_sizes[-1] + n)\n",
    "\n",
    "        # In caso il metadata non coincida, rendi coerente con quanto caricato\n",
    "        self.total_patches = self.cumulative_sizes[-1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_patches\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = np.searchsorted(self.cumulative_sizes[1:], idx, side='right')\n",
    "        local_idx = idx - self.cumulative_sizes[batch_idx]\n",
    "\n",
    "        batch_data = self._cache[batch_idx]\n",
    "\n",
    "        x = batch_data['patches'][local_idx]   # (3,H,W) in [0,1]\n",
    "        y = batch_data['labels'][local_idx]\n",
    "\n",
    "        if self.use_pretrained:\n",
    "            x = self.transform(x) if self.transform is not None else imagenet_norm(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Load metadata for dataset creation\n",
    "import json\n",
    "with open(metadata_file, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = MultiFilePatchDataset(metadata, transform=imagenet_norm) \n",
    "\n",
    "\n",
    "# ===== CREATE TRAIN/VAL SPLIT =====\n",
    "print(\"Creating train/val split...\")\n",
    "\n",
    "# Create indices for train/val split\n",
    "# Need to load all labels to stratify properly\n",
    "print(\"Loading all labels for stratified split...\")\n",
    "all_labels_list = []\n",
    "for batch_file in metadata['batch_files']:\n",
    "    batch_data = torch.load(batch_file)\n",
    "    all_labels_list.extend(batch_data['labels'].tolist())\n",
    "\n",
    "all_labels_array = np.array(all_labels_list)\n",
    "\n",
    "# Split indices (not data!)\n",
    "train_indices, val_indices = train_test_split(\n",
    "    np.arange(len(all_labels_array)), test_size=0.2, random_state=42, \n",
    "    stratify=all_labels_array\n",
    ")\n",
    "\n",
    "print(f\"Train patches: {len(train_indices)}\")\n",
    "print(f\"Val patches: {len(val_indices)}\")\n",
    "\n",
    "# Create subset datasets\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "label_map = {'Triple negative': 0, 'Luminal A': 1, 'Luminal B': 2, 'HER2(+)': 3}\n",
    "\n",
    "# Create DataLoaders with GPU optimizations\n",
    "train_loader_kwargs = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY\n",
    "}\n",
    "val_loader_kwargs = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY\n",
    "}\n",
    "if NUM_WORKERS > 0:\n",
    "    train_loader_kwargs['persistent_workers'] = PERSISTENT_WORKERS\n",
    "    val_loader_kwargs['persistent_workers'] = PERSISTENT_WORKERS\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **train_loader_kwargs)\n",
    "val_loader = DataLoader(val_dataset, **val_loader_kwargs)\n",
    "\n",
    "print(f\"\\nOptimization: {NUM_WORKERS} workers, pin_memory={PIN_MEMORY}, persistent_workers={PERSISTENT_WORKERS}\")\n",
    "print(f\"\\nCreated DataLoaders:\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4a0a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:00:34.639822Z",
     "iopub.status.busy": "2025-12-10T14:00:34.638928Z",
     "iopub.status.idle": "2025-12-10T14:00:40.415220Z",
     "shell.execute_reply": "2025-12-10T14:00:40.414517Z",
     "shell.execute_reply.started": "2025-12-10T14:00:34.639797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "imagenet_norm = transforms.Normalize(\n",
    "    mean=(0.485, 0.456, 0.406),\n",
    "    std=(0.229, 0.224, 0.225)\n",
    ")\n",
    "\n",
    "# Memory-efficient test dataset\n",
    "class TestPatchDataset(Dataset):\n",
    "    \"\"\"Memory-efficient test dataset that extracts patches on-the-fly\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, patch_size=PATCH_SIZE, stride=PATCH_STRIDE,\n",
    "                 min_variance_threshold=0.001, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.min_variance_threshold = min_variance_threshold\n",
    "\n",
    "        self.transform = transform  # optional: callable(x)->x\n",
    "\n",
    "        self.use_patches = bool(self.patch_size) and bool(self.stride)\n",
    "\n",
    "        # Get image files\n",
    "        self.image_files = sorted([f for f in os.listdir(data_dir) if f.startswith('img_')])\n",
    "\n",
    "        # Pre-compute patch counts and mapping (se uso patches)\n",
    "        if self.use_patches:\n",
    "            self.patch_counts = []\n",
    "            self.cumulative_patches = [0]\n",
    "            self.patch_to_image = []\n",
    "\n",
    "            print(f\"Computing patch counts for {len(self.image_files)} test images (filtering blank patches)...\")\n",
    "            for i, img_name in enumerate(self.image_files):\n",
    "                img_path = os.path.join(self.data_dir, img_name)\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_array = np.array(img, dtype=np.float32)\n",
    "                h, w, _ = img_array.shape\n",
    "\n",
    "                if h < self.patch_size or w < self.patch_size:\n",
    "                    n_patches = 1\n",
    "                else:\n",
    "                    n_patches_h = max(1, (h - self.patch_size) // self.stride + 1)\n",
    "                    n_patches_w = max(1, (w - self.patch_size) // self.stride + 1)\n",
    "\n",
    "                    valid_patches = 0\n",
    "                    for row_idx in range(n_patches_h):\n",
    "                        for col_idx in range(n_patches_w):\n",
    "                            start_h = min(row_idx * self.stride, h - self.patch_size)\n",
    "                            start_w = min(col_idx * self.stride, w - self.patch_size)\n",
    "                            patch = img_array[start_h:start_h+self.patch_size, start_w:start_w+self.patch_size, :]\n",
    "\n",
    "                            if np.var(patch / 255.0) > self.min_variance_threshold:\n",
    "                                valid_patches += 1\n",
    "\n",
    "                    n_patches = max(1, valid_patches)\n",
    "\n",
    "                self.patch_counts.append(n_patches)\n",
    "                self.cumulative_patches.append(self.cumulative_patches[-1] + n_patches)\n",
    "                for _ in range(n_patches):\n",
    "                    self.patch_to_image.append(i)\n",
    "\n",
    "            self.total_patches = self.cumulative_patches[-1]\n",
    "            print(f\"Total test patches (excluding blank): {self.total_patches}\")\n",
    "        else:\n",
    "            self.total_patches = len(self.image_files)\n",
    "            self.patch_to_image = list(range(len(self.image_files)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_patches\n",
    "    \n",
    "    def _get_valid_patches_for_image(self, img_array, img_h, img_w):\n",
    "        \"\"\"Extract all valid (non-blank) patches from an image and return their positions\"\"\"\n",
    "        valid_patches_info = []\n",
    "        \n",
    "        if img_h < self.patch_size or img_w < self.patch_size:\n",
    "            # For small images, use padded single patch\n",
    "            valid_patches_info.append((0, 0))  # placeholder position\n",
    "        else:\n",
    "            n_patches_h = max(1, (img_h - self.patch_size) // self.stride + 1)\n",
    "            n_patches_w = max(1, (img_w - self.patch_size) // self.stride + 1)\n",
    "            \n",
    "            for row_idx in range(n_patches_h):\n",
    "                for col_idx in range(n_patches_w):\n",
    "                    start_h = min(row_idx * self.stride, img_h - self.patch_size)\n",
    "                    start_w = min(col_idx * self.stride, img_w - self.patch_size)\n",
    "                    patch = img_array[start_h:start_h+self.patch_size, start_w:start_w+self.patch_size, :]\n",
    "                    \n",
    "                    # Check if patch has sufficient variance\n",
    "                    if np.var(patch / 255.0) > self.min_variance_threshold:\n",
    "                        valid_patches_info.append((start_h, start_w))\n",
    "            \n",
    "            # Ensure at least one patch per image\n",
    "            if len(valid_patches_info) == 0:\n",
    "                valid_patches_info.append((0, 0))\n",
    "        \n",
    "        return valid_patches_info\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.use_patches:\n",
    "            img_idx = np.searchsorted(self.cumulative_patches[1:], idx, side='right')\n",
    "            patch_idx = idx - self.cumulative_patches[img_idx]\n",
    "            img_name = self.image_files[img_idx]\n",
    "        else:\n",
    "            img_name = self.image_files[idx]\n",
    "\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_array = np.array(img, dtype=np.float32)\n",
    "\n",
    "        if self.use_patches:\n",
    "            h, w, _ = img_array.shape\n",
    "\n",
    "            if h < self.patch_size or w < self.patch_size:\n",
    "                pad_h = max(0, self.patch_size - h)\n",
    "                pad_w = max(0, self.patch_size - w)\n",
    "                img_array = np.pad(img_array, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "                final_img = img_array[:self.patch_size, :self.patch_size, :]\n",
    "            else:\n",
    "                valid_positions = self._get_valid_patches_for_image(img_array, h, w)\n",
    "                # sicurezza: se per qualche motivo tornasse vuota\n",
    "                if not valid_positions:\n",
    "                    valid_positions = [(0, 0)]\n",
    "\n",
    "                start_h, start_w = valid_positions[patch_idx % len(valid_positions)]\n",
    "                final_img = img_array[start_h:start_h+self.patch_size, start_w:start_w+self.patch_size, :]\n",
    "        else:\n",
    "            img_pil = Image.fromarray(img_array.astype('uint8'))\n",
    "            img_resized = img_pil.resize((self.patch_size, self.patch_size), Image.BILINEAR)\n",
    "            final_img = np.array(img_resized)\n",
    "\n",
    "        img_tensor = torch.from_numpy(final_img).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        # Use pre-trained ImageNet weights\n",
    "        img_tensor = self.transform(img_tensor) if self.transform is not None else imagenet_norm(img_tensor)\n",
    "\n",
    "        return img_tensor\n",
    "\n",
    "\n",
    "# Load test data\n",
    "print(f\"\\nLoading test data\")\n",
    "print(f\"Using PATCH-BASED processing for test data\")\n",
    "    \n",
    "test_dataset = TestPatchDataset(\n",
    "    cropped_test_img_dir,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    stride=PATCH_STRIDE,\n",
    "    transform=imagenet_norm\n",
    ")\n",
    "\n",
    "test_filenames = test_dataset.image_files\n",
    "test_patch_to_image = test_dataset.patch_to_image\n",
    "\n",
    "print(f\"Test patches: {len(test_dataset)}\")\n",
    "print(f\"Test image files: {len(test_filenames)}\")\n",
    "\n",
    "# Create DataLoader with GPU optimizations\n",
    "test_loader_kwargs = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY\n",
    "}\n",
    "if NUM_WORKERS > 0:\n",
    "    test_loader_kwargs['persistent_workers'] = PERSISTENT_WORKERS\n",
    "\n",
    "test_loader = DataLoader(test_dataset, **test_loader_kwargs)\n",
    "\n",
    "print(f\"\\nDataLoader created:\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "print(f\"Optimization: {NUM_WORKERS} workers, pin_memory={PIN_MEMORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6b729",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:01:23.626113Z",
     "iopub.status.busy": "2025-12-10T14:01:23.625747Z",
     "iopub.status.idle": "2025-12-10T14:01:23.636955Z",
     "shell.execute_reply": "2025-12-10T14:01:23.636087Z",
     "shell.execute_reply.started": "2025-12-10T14:01:23.626085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_map = {class_names[i]: i for i in range(4)}\n",
    "input_shape = (3, PATCH_SIZE, PATCH_SIZE) if PATCH_SIZE else (3, IMG_SIZE[0], IMG_SIZE[1])\n",
    "num_classes = len(label_map)\n",
    "\n",
    "# ===== MULTI-GPU SETUP =====\n",
    "# Check for multiple GPUs and set up DataParallel\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    device = torch.device('cuda:0')\n",
    "    print(f\"Found {num_gpus} GPU(s) available:\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\" GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    if num_gpus > 1:\n",
    "        print(f\"Multi-GPU training enabled: Will use {num_gpus} GPUs with DataParallel\")\n",
    "    else:\n",
    "        print(f\"Single GPU training\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    num_gpus = 0\n",
    "    print(\"No GPU available, using CPU\")\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85cd3aa",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00880a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:01:46.770623Z",
     "iopub.status.busy": "2025-12-10T14:01:46.770007Z",
     "iopub.status.idle": "2025-12-10T14:01:46.775577Z",
     "shell.execute_reply": "2025-12-10T14:01:46.775037Z",
     "shell.execute_reply.started": "2025-12-10T14:01:46.770594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 30\n",
    "PATIENCE = 50\n",
    "\n",
    "# Regularisation\n",
    "DROPOUT_RATE = 0.2       # Dropout probability\n",
    "L1_LAMBDA = 0            # L1 penalty\n",
    "L2_LAMBDA = 0.01         # L2 penalty\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", EPOCHS)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Learning Rate:\", LEARNING_RATE)\n",
    "print(\"Dropout Rate:\", DROPOUT_RATE)\n",
    "print(\"L1 Penalty:\", L1_LAMBDA)\n",
    "print(\"L2 Penalty:\", L2_LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb42d0",
   "metadata": {},
   "source": [
    "# Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67a91a-dd4b-452c-930b-8fa213a01a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:41:32.464150Z",
     "iopub.status.busy": "2025-12-10T14:41:32.463874Z",
     "iopub.status.idle": "2025-12-10T14:41:32.482154Z",
     "shell.execute_reply": "2025-12-10T14:41:32.481221Z",
     "shell.execute_reply.started": "2025-12-10T14:41:32.464131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Swin Transformer ---\n",
    "MODEL_NAME = \"swin_t\"\n",
    "model = models.swin_t(weights=(Swin_T_Weights.IMAGENET1K_V1))\n",
    "# Modify the model for our classification task\n",
    "# Assuming model is already defined and loaded, e.g., a Swin Transformer or Vision Transformer\n",
    "# Steps:\n",
    "\n",
    "# 1) Freeze all layers\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 2) Replace head based on model type\n",
    "if MODEL_NAME.startswith(\"swin\") and hasattr(model, \"head\"):\n",
    "    in_f = model.head.in_features\n",
    "    model.head = nn.Sequential(\n",
    "        nn.Dropout(DROPOUT_RATE),\n",
    "        nn.Linear(in_f, num_classes)\n",
    "    )\n",
    "\n",
    "elif MODEL_NAME.startswith(\"vit\") and hasattr(model, \"heads\"):\n",
    "    # Vision Transformer\n",
    "    in_f = model.heads.head.in_features\n",
    "    model.heads = nn.Sequential(\n",
    "        nn.Dropout(DROPOUT_RATE),\n",
    "        nn.Linear(in_f, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(DROPOUT_RATE),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    print(f\"DataParallel active on {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea301fbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:02:13.623319Z",
     "iopub.status.busy": "2025-12-10T14:02:13.623085Z",
     "iopub.status.idle": "2025-12-10T14:02:15.154532Z",
     "shell.execute_reply": "2025-12-10T14:02:15.153902Z",
     "shell.execute_reply.started": "2025-12-10T14:02:13.623304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display model architecture summary\n",
    "print(\"Model summary:\")\n",
    "summary(model, input_size=input_shape, device=str(device).split(':')[0])\n",
    "\n",
    "# Count trainable vs frozen parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(\"Parameter stats:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters (classifier only): {trainable_params:,}\")\n",
    "print(f\"Frozen parameters (feature extractor): {frozen_params:,}\")\n",
    "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d3174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:02:43.995362Z",
     "iopub.status.busy": "2025-12-10T14:02:43.995087Z",
     "iopub.status.idle": "2025-12-10T14:02:44.001049Z",
     "shell.execute_reply": "2025-12-10T14:02:44.000441Z",
     "shell.execute_reply.started": "2025-12-10T14:02:43.995342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define optimizer - ONLY train classifier parameters (feature extractor is frozen)\n",
    "# Filter to get only parameters that require gradients (classifier layers)\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "\n",
    "# Enable mixed precision training for GPU acceleration\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "print(\"Optimizer configured to train only classifier layers\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay (L2): {L2_LAMBDA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f5f56",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a4724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:03:00.740165Z",
     "iopub.status.busy": "2025-12-10T14:03:00.739893Z",
     "iopub.status.idle": "2025-12-10T14:03:00.743625Z",
     "shell.execute_reply": "2025-12-10T14:03:00.743012Z",
     "shell.execute_reply.started": "2025-12-10T14:03:00.740144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize best model tracking variables\n",
    "best_model = None\n",
    "best_performance = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e4dd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:03:07.937634Z",
     "iopub.status.busy": "2025-12-10T14:03:07.937088Z",
     "iopub.status.idle": "2025-12-10T14:03:07.944998Z",
     "shell.execute_reply": "2025-12-10T14:03:07.944295Z",
     "shell.execute_reply.started": "2025-12-10T14:03:07.937610Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    \"\"\"\n",
    "    Perform one complete training epoch through the entire training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): Lambda for L1 regularization\n",
    "        l2_lambda (float): Lambda for L2 regularization\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Iterate through training batches\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Move data to device (GPU/CPU)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Clear gradients from previous step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with mixed precision (if CUDA available)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Add L1 and L2 regularization\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b99110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:03:12.996954Z",
     "iopub.status.busy": "2025-12-10T14:03:12.996688Z",
     "iopub.status.idle": "2025-12-10T14:03:13.003109Z",
     "shell.execute_reply": "2025-12-10T14:03:13.002347Z",
     "shell.execute_reply.started": "2025-12-10T14:03:12.996933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Perform one complete validation epoch through the entire validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        criterion (nn.Module): Loss function used to calculate validation loss\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
    "\n",
    "    Note:\n",
    "        This function automatically sets the model to evaluation mode and disables\n",
    "        gradient computation for efficiency during validation.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            # Move data to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision (if CUDA available)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05834891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:03:21.518374Z",
     "iopub.status.busy": "2025-12-10T14:03:21.517659Z",
     "iopub.status.idle": "2025-12-10T14:03:21.527532Z",
     "shell.execute_reply": "2025-12-10T14:03:21.526890Z",
     "shell.execute_reply.started": "2025-12-10T14:03:21.518353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Train the neural network model on the training data and validate on the validation data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        epochs (int): Number of training epochs\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
    "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
    "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
    "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
    "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
    "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
    "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
    "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
    "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, training_history) - Trained model and metrics history\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize metrics tracking\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "\n",
    "    # Configure early stopping if patience is set\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    # Main training loop: iterate through epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        # Forward pass through training data, compute gradients, update weights\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        # Evaluate model on validation data without updating weights\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        # Store metrics for plotting and analysis\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        # Print progress every N epochs or on first epoch\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
    "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping logic: monitor metric and save best model\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(),\"models/\"+experiment_name+'_model.pt')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    # Restore best model weights if early stopping was used\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
    "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "\n",
    "    # Save final model if no early stopping\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history, best_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8b6f7",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0c487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:03:28.275559Z",
     "iopub.status.busy": "2025-12-10T14:03:28.275084Z",
     "iopub.status.idle": "2025-12-10T14:30:39.748457Z",
     "shell.execute_reply": "2025-12-10T14:30:39.747396Z",
     "shell.execute_reply.started": "2025-12-10T14:03:28.275538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = f\"pretrained_{MODEL_NAME}_transformer\"\n",
    "\n",
    "print(f\"Training with {MODEL_NAME.upper()} pretrained - transfer learning\")\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")\n",
    "print(f\"Frozen feature extractor + Trainable classifier\")\n",
    "\n",
    "model, history, best_f1 = fit(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    verbose=10,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    patience=PATIENCE\n",
    ")\n",
    "\n",
    "if best_f1 > best_performance:\n",
    "    best_model = model\n",
    "    best_performance = best_f1\n",
    "    print(f\"\\nNew best model saved with F1 Score: {best_performance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829b683",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe5e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation predictions\n",
    "val_preds = []\n",
    "val_targets = []\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        logits = best_model(inputs)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        val_preds.append(preds)\n",
    "        val_targets.append(targets.numpy())\n",
    "\n",
    "val_preds = np.concatenate(val_preds)\n",
    "val_targets = np.concatenate(val_targets)\n",
    "\n",
    "# Calculate overall validation set metrics\n",
    "val_acc = accuracy_score(val_targets, val_preds)\n",
    "val_prec = precision_score(val_targets, val_preds, average='weighted')\n",
    "val_rec = recall_score(val_targets, val_preds, average='weighted')\n",
    "val_f1 = f1_score(val_targets, val_preds, average='weighted')\n",
    "\n",
    "print(f\"Accuracy over the validation set: {val_acc:.4f}\")\n",
    "print(f\"Precision over the validation set: {val_prec:.4f}\")\n",
    "print(f\"Recall over the validation set: {val_rec:.4f}\")\n",
    "print(f\"F1 score over the validation set: {val_f1:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(val_targets, val_preds)\n",
    "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix — Validation Set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two side-by-side subplots (two columns)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot of training and validation loss on the first axis\n",
    "ax1.plot(history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax1.plot(history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot of training and validation accuracy on the second axis\n",
    "ax2.plot(history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax2.plot(history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\n",
    "ax2.set_title('F1 Score')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)\n",
    "plt.savefig('training_curves.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = []\n",
    "val_targets = []\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        logits = best_model(inputs)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        val_preds.append(preds)\n",
    "        val_targets.append(targets.numpy())\n",
    "\n",
    "val_preds = np.concatenate(val_preds)\n",
    "val_targets = np.concatenate(val_targets)\n",
    "\n",
    "cm = confusion_matrix(val_targets, val_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "class_names = ['Luminal A', 'Luminal B', 'HER2(+)', 'Triple Neg.']\n",
    "im = ax.imshow(cm_normalized, interpolation='nearest', cmap='Blues', vmin=0, vmax=100)\n",
    "\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel('Percentage (%)', rotation=90, va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xticks(np.arange(len(class_names)))\n",
    "ax.set_yticks(np.arange(len(class_names)))\n",
    "ax.set_xticklabels(class_names, fontsize=11)\n",
    "ax.set_yticklabels(class_names, fontsize=11)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        count = cm[i, j]\n",
    "        percentage = cm_normalized[i, j]\n",
    "        text_color = \"white\" if percentage > 50 else \"black\"\n",
    "        text = ax.text(j, i, f'{count}\\n({percentage:.1f}%)',\n",
    "                      ha=\"center\", va=\"center\", color=text_color,\n",
    "                      fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Confusion Matrix - {MODEL_NAME.upper()} (Validation Set)\\nF1 Score: {best_f1:.4f}', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: confusion_matrix.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(val_targets, val_preds, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea326ad9",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229c1f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:31:04.114051Z",
     "iopub.status.busy": "2025-12-10T14:31:04.113751Z",
     "iopub.status.idle": "2025-12-10T14:31:51.770016Z",
     "shell.execute_reply": "2025-12-10T14:31:51.768998Z",
     "shell.execute_reply.started": "2025-12-10T14:31:04.114025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "patch_preds = []\n",
    "patch_probs = []\n",
    "best_model.eval()\n",
    "\n",
    "print(f\"Running inference on {len(test_dataset)} test patches...\")\n",
    "print(f\"Will aggregate to {len(test_filenames)} images\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        xb = batch.to(device)\n",
    "        \n",
    "        logits = best_model(xb)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        patch_preds.append(preds)\n",
    "        patch_probs.append(probs)\n",
    "\n",
    "patch_preds = np.concatenate(patch_preds)\n",
    "patch_probs = np.concatenate(patch_probs)\n",
    "\n",
    "print(f\"Got {len(patch_preds)} patch predictions\")\n",
    "\n",
    "image_preds = []\n",
    "for img_idx in range(len(test_filenames)):\n",
    "    patch_indices = [i for i, img_id in enumerate(test_patch_to_image) if img_id == img_idx]\n",
    "    image_patch_probs = patch_probs[patch_indices]\n",
    "    avg_probs = image_patch_probs.mean(axis=0)\n",
    "    final_pred = avg_probs.argmax()\n",
    "    image_preds.append(final_pred)\n",
    "\n",
    "test_preds = np.array(image_preds)\n",
    "print(f\"Aggregated to {len(test_preds)} image predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b3d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:33:21.666170Z",
     "iopub.status.busy": "2025-12-10T14:33:21.665924Z",
     "iopub.status.idle": "2025-12-10T14:33:21.692230Z",
     "shell.execute_reply": "2025-12-10T14:33:21.691317Z",
     "shell.execute_reply.started": "2025-12-10T14:33:21.666154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create reverse label mapping\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "test_filenames = [fn.replace('mask', 'img') for fn in test_filenames]\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_index': test_filenames,\n",
    "    'label': [reverse_label_map[pred] for pred in test_preds]\n",
    "})\n",
    "\n",
    "# Create descriptive filename with all hyperparameters\n",
    "filename_parts = [\n",
    "    f\"submission_{EXPERIMENT_NAME}\",\n",
    "    f\"focus_filter\",\n",
    "    f\"bs_{BATCH_SIZE}\",\n",
    "    f\"lr_{LEARNING_RATE}\",\n",
    "    f\"drop_{DROPOUT_RATE}\",\n",
    "    f\"l1_{L1_LAMBDA}\",\n",
    "    f\"l2_{L2_LAMBDA}\",\n",
    "    f\"epochs_{EPOCHS}\",\n",
    "    f\"patience_{PATIENCE}\",\n",
    "    f\"imgsize_{IMG_SIZE[0]}x{IMG_SIZE[1]}\"\n",
    "]\n",
    "submission_filename = \"_\".join(filename_parts) + \".csv\"\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print(f\"Submission file created: {submission_filename}\")\n",
    "print(f\"Total predictions: {len(submission_df)}\")\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
