{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f7c75a",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa28a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235021a",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f43b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path('data')\n",
    "TRAIN_IMG_DIR = DATA_DIR / 'train_img'\n",
    "TRAIN_LABELS_PATH = DATA_DIR / 'train_labels.csv'\n",
    "MODEL_SAVE_DIR = Path('models')\n",
    "MODEL_SAVE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Contrastive Learning Hyperparameters\n",
    "PRETRAIN_EPOCHS = 200\n",
    "PRETRAIN_BATCH_SIZE = 64  # Larger batch size helps contrastive learning\n",
    "PRETRAIN_LR = 3e-4\n",
    "TEMPERATURE = 0.5  # Temperature for NT-Xent loss\n",
    "PROJECTION_DIM = 128  # Dimension of projection head output\n",
    "\n",
    "# Fine-tuning Hyperparameters\n",
    "FINETUNE_EPOCHS = 100\n",
    "FINETUNE_BATCH_SIZE = 32\n",
    "FINETUNE_LR = 1e-4\n",
    "\n",
    "# Model\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 8  # Adjust based on your dataset\n",
    "\n",
    "print(f\"Pretraining: {PRETRAIN_EPOCHS} epochs, batch size {PRETRAIN_BATCH_SIZE}\")\n",
    "print(f\"Fine-tuning: {FINETUNE_EPOCHS} epochs, batch size {FINETUNE_BATCH_SIZE}\")\n",
    "print(f\"Temperature: {TEMPERATURE}, Projection dim: {PROJECTION_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a4cb5",
   "metadata": {},
   "source": [
    "## 3. Strong Augmentation Pipeline for Contrastive Learning\n",
    "\n",
    "The key to contrastive learning is creating diverse augmented views of the same image.\n",
    "We use strong augmentations to force the model to learn invariant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f7324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTransform:\n",
    "    \"\"\"Creates two augmented views of the same image for contrastive learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224):\n",
    "        # Strong augmentation pipeline (SimCLR style)\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((img_size, img_size)),\n",
    "            T.RandomResizedCrop(img_size, scale=(0.2, 1.0)),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "            T.RandomRotation(degrees=30),\n",
    "            T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Return two different augmented views\n",
    "        return self.transform(x), self.transform(x)\n",
    "\n",
    "\n",
    "class SimpleTransform:\n",
    "    \"\"\"Simple transform for fine-tuning/evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, augment=False):\n",
    "        if augment:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((img_size, img_size)),\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.RandomRotation(degrees=15),\n",
    "                T.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((img_size, img_size)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96746c5c",
   "metadata": {},
   "source": [
    "## 4. Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveDataset(Dataset):\n",
    "    \"\"\"Dataset for contrastive pretraining (no labels needed).\"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.image_files = sorted(list(self.img_dir.glob('*.png')) + \n",
    "                                   list(self.img_dir.glob('*.jpg')))\n",
    "        self.transform = transform or ContrastiveTransform()\n",
    "        print(f\"Loaded {len(self.image_files)} images for contrastive learning\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Get two augmented views\n",
    "        view1, view2 = self.transform(image)\n",
    "        \n",
    "        return view1, view2\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, labels_df, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform or SimpleTransform()\n",
    "        print(f\"Loaded {len(self.labels_df)} labeled images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        img_path = self.img_dir / f\"{row['id']}.png\"\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706ce9b",
   "metadata": {},
   "source": [
    "## 5. SimCLR Model Architecture\n",
    "\n",
    "SimCLR consists of:\n",
    "1. **Encoder (f)**: Backbone network (ResNet) that extracts features\n",
    "2. **Projection Head (g)**: MLP that projects features to contrastive space\n",
    "\n",
    "During pretraining, we train both f and g.\n",
    "During fine-tuning, we discard g and add a classification head to f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e904f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    \"\"\"SimCLR model for contrastive learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_encoder='resnet50', projection_dim=128):\n",
    "        super(SimCLR, self).__init__()\n",
    "        \n",
    "        # Encoder: Use ResNet backbone\n",
    "        if base_encoder == 'resnet18':\n",
    "            self.encoder = models.resnet18(weights=None)  # Train from scratch\n",
    "            feature_dim = 512\n",
    "        elif base_encoder == 'resnet50':\n",
    "            self.encoder = models.resnet50(weights=None)\n",
    "            feature_dim = 2048\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported encoder: {base_encoder}\")\n",
    "        \n",
    "        # Remove the final classification layer\n",
    "        self.encoder = nn.Sequential(*list(self.encoder.children())[:-1])\n",
    "        \n",
    "        # Projection head: MLP with one hidden layer\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, projection_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        h = self.encoder(x)  # [batch, feature_dim, 1, 1]\n",
    "        h = torch.flatten(h, 1)  # [batch, feature_dim]\n",
    "        \n",
    "        # Project to contrastive space\n",
    "        z = self.projection_head(h)  # [batch, projection_dim]\n",
    "        \n",
    "        return h, z\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"Classifier for fine-tuning (uses pretrained encoder).\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, feature_dim, num_classes, dropout=0.3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = torch.flatten(h, 1)\n",
    "        logits = self.classifier(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b71c62",
   "metadata": {},
   "source": [
    "## 6. NT-Xent Loss (Normalized Temperature-scaled Cross Entropy)\n",
    "\n",
    "This is the core of contrastive learning:\n",
    "- For each image, we create two views (positive pair)\n",
    "- All other images in the batch are negative examples\n",
    "- Goal: Maximize similarity between positive pairs, minimize similarity with negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8748fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTXentLoss(nn.Module):\n",
    "    \"\"\"Normalized Temperature-scaled Cross Entropy Loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z_i: Projections of first augmented views [batch_size, projection_dim]\n",
    "            z_j: Projections of second augmented views [batch_size, projection_dim]\n",
    "        \"\"\"\n",
    "        batch_size = z_i.shape[0]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "        \n",
    "        # Concatenate both views\n",
    "        z = torch.cat([z_i, z_j], dim=0)  # [2*batch_size, projection_dim]\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.mm(z, z.T)  # [2*batch_size, 2*batch_size]\n",
    "        \n",
    "        # Create masks for positive and negative pairs\n",
    "        # Positive pairs: (i, i+batch_size) and (i+batch_size, i)\n",
    "        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)\n",
    "        \n",
    "        # Remove self-similarities\n",
    "        similarity_matrix = similarity_matrix[~mask].view(2 * batch_size, -1)\n",
    "        \n",
    "        # Positive pairs are at positions [batch_size-1] and [2*batch_size-2]\n",
    "        positives = torch.cat([\n",
    "            torch.diag(similarity_matrix, batch_size - 1),\n",
    "            torch.diag(similarity_matrix, -(batch_size - 1))\n",
    "        ], dim=0).view(2 * batch_size, 1)\n",
    "        \n",
    "        # Negatives are all other entries\n",
    "        negatives = similarity_matrix\n",
    "        \n",
    "        # Compute logits\n",
    "        logits = torch.cat([positives, negatives], dim=1) / self.temperature\n",
    "        \n",
    "        # Labels: positive pair is always at index 0\n",
    "        labels = torch.zeros(2 * batch_size, dtype=torch.long, device=z.device)\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f6b7b",
   "metadata": {},
   "source": [
    "## 7. Contrastive Pretraining Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_contrastive(model, dataloader, optimizer, criterion, epochs, device):\n",
    "    \"\"\"Pretrain the encoder using contrastive learning.\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    history = {'loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for view1, view2 in pbar:\n",
    "            view1 = view1.to(device)\n",
    "            view2 = view2.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            _, z_i = model(view1)\n",
    "            _, z_j = model(view2)\n",
    "            \n",
    "            # Compute contrastive loss\n",
    "            loss = criterion(z_i, z_j)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        history['loss'].append(avg_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eec733",
   "metadata": {},
   "source": [
    "## 8. Fine-tuning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29047247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
    "    \"\"\"Fine-tune the classifier with labeled data.\"\"\"\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{epochs}\")\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{100.*train_correct/train_total:.2f}%\"\n",
    "            })\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_DIR / 'best_contrastive_model.pt')\n",
    "            print(f\"✓ Best model saved! Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42ae57",
   "metadata": {},
   "source": [
    "## 9. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels for supervised fine-tuning\n",
    "labels_df = pd.read_csv(TRAIN_LABELS_PATH)\n",
    "print(f\"Total labeled samples: {len(labels_df)}\")\n",
    "print(f\"Label distribution:\\n{labels_df['label'].value_counts().sort_index()}\")\n",
    "\n",
    "# Split into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    labels_df, \n",
    "    test_size=0.2, \n",
    "    random_state=SEED,\n",
    "    stratify=labels_df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a99006",
   "metadata": {},
   "source": [
    "## 10. Phase 1: Contrastive Pretraining\n",
    "\n",
    "**\"Before the name is given, the difference is felt.\"**\n",
    "\n",
    "We pretrain the encoder without using labels, learning representations through contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contrastive dataset (uses all training images, ignores labels)\n",
    "contrastive_dataset = ContrastiveDataset(\n",
    "    img_dir=TRAIN_IMG_DIR,\n",
    "    transform=ContrastiveTransform(IMG_SIZE)\n",
    ")\n",
    "\n",
    "contrastive_loader = DataLoader(\n",
    "    contrastive_dataset,\n",
    "    batch_size=PRETRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Contrastive batches: {len(contrastive_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e53223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SimCLR model\n",
    "simclr_model = SimCLR(\n",
    "    base_encoder='resnet18',  # Can also use 'resnet50' for more capacity\n",
    "    projection_dim=PROJECTION_DIM\n",
    ").to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "pretrain_optimizer = torch.optim.Adam(simclr_model.parameters(), lr=PRETRAIN_LR)\n",
    "contrastive_criterion = NTXentLoss(temperature=TEMPERATURE)\n",
    "\n",
    "print(f\"SimCLR model initialized with ResNet18 backbone\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in simclr_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain the model\n",
    "print(\"Starting contrastive pretraining...\\n\")\n",
    "\n",
    "pretrain_history = pretrain_contrastive(\n",
    "    model=simclr_model,\n",
    "    dataloader=contrastive_loader,\n",
    "    optimizer=pretrain_optimizer,\n",
    "    criterion=contrastive_criterion,\n",
    "    epochs=PRETRAIN_EPOCHS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save the pretrained encoder\n",
    "torch.save(simclr_model.encoder.state_dict(), MODEL_SAVE_DIR / 'contrastive_encoder.pt')\n",
    "print(f\"\\n✓ Pretrained encoder saved to {MODEL_SAVE_DIR / 'contrastive_encoder.pt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pretraining loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pretrain_history['loss'], label='Contrastive Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Contrastive Pretraining Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122cea84",
   "metadata": {},
   "source": [
    "## 11. Phase 2: Supervised Fine-tuning\n",
    "\n",
    "**\"Upon this geometry of understanding, the house of classification, sturdy it stands.\"**\n",
    "\n",
    "Now we use the pretrained encoder for classification with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c7643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create supervised datasets\n",
    "train_dataset = SupervisedDataset(\n",
    "    img_dir=TRAIN_IMG_DIR,\n",
    "    labels_df=train_df,\n",
    "    transform=SimpleTransform(IMG_SIZE, augment=True)\n",
    ")\n",
    "\n",
    "val_dataset = SupervisedDataset(\n",
    "    img_dir=TRAIN_IMG_DIR,\n",
    "    labels_df=val_df,\n",
    "    transform=SimpleTransform(IMG_SIZE, augment=False)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=FINETUNE_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=FINETUNE_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77931f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier with pretrained encoder\n",
    "classifier = Classifier(\n",
    "    encoder=simclr_model.encoder,  # Use the pretrained encoder\n",
    "    feature_dim=512,  # ResNet18 feature dimension\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Optimizer and loss for fine-tuning\n",
    "finetune_optimizer = torch.optim.Adam(classifier.parameters(), lr=FINETUNE_LR)\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Classifier initialized with pretrained encoder\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in classifier.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9154a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the classifier\n",
    "print(\"Starting supervised fine-tuning...\\n\")\n",
    "\n",
    "finetune_history = train_classifier(\n",
    "    model=classifier,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=finetune_optimizer,\n",
    "    criterion=classification_criterion,\n",
    "    epochs=FINETUNE_EPOCHS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fine-tuning results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(finetune_history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(finetune_history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Fine-tuning Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(finetune_history['train_acc'], label='Train Accuracy')\n",
    "axes[1].plot(finetune_history['val_acc'], label='Val Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Fine-tuning Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation accuracy: {max(finetune_history['val_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b3cd2",
   "metadata": {},
   "source": [
    "## 12. Comparison: Pretrained vs From-Scratch\n",
    "\n",
    "To verify the benefit of contrastive pretraining, let's train a model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh encoder (not pretrained)\n",
    "scratch_encoder = models.resnet18(weights=None)\n",
    "scratch_encoder = nn.Sequential(*list(scratch_encoder.children())[:-1])\n",
    "\n",
    "scratch_classifier = Classifier(\n",
    "    encoder=scratch_encoder,\n",
    "    feature_dim=512,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "scratch_optimizer = torch.optim.Adam(scratch_classifier.parameters(), lr=FINETUNE_LR)\n",
    "\n",
    "print(\"Training classifier from scratch (no pretraining)...\\n\")\n",
    "\n",
    "scratch_history = train_classifier(\n",
    "    model=scratch_classifier,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=scratch_optimizer,\n",
    "    criterion=classification_criterion,\n",
    "    epochs=FINETUNE_EPOCHS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n✓ From-scratch training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Validation loss comparison\n",
    "axes[0].plot(finetune_history['val_loss'], label='Contrastive Pretrained', linewidth=2)\n",
    "axes[0].plot(scratch_history['val_loss'], label='From Scratch', linewidth=2, linestyle='--')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Validation Loss')\n",
    "axes[0].set_title('Validation Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy comparison\n",
    "axes[1].plot(finetune_history['val_acc'], label='Contrastive Pretrained', linewidth=2)\n",
    "axes[1].plot(scratch_history['val_acc'], label='From Scratch', linewidth=2, linestyle='--')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Accuracy (%)')\n",
    "axes[1].set_title('Validation Accuracy Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(f\"Contrastive Pretrained - Best Val Acc: {max(finetune_history['val_acc']):.2f}%\")\n",
    "print(f\"From Scratch - Best Val Acc: {max(scratch_history['val_acc']):.2f}%\")\n",
    "print(f\"Improvement: {max(finetune_history['val_acc']) - max(scratch_history['val_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16fbec3",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "**Contrastive Learning Benefits:**\n",
    "1. Learns robust, domain-specific features without labels\n",
    "2. Better generalization compared to training from scratch\n",
    "3. More sample-efficient when labeled data is limited\n",
    "4. Creates meaningful geometric structure in embedding space\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different augmentation strategies\n",
    "- Try different backbone architectures (ResNet50, EfficientNet)\n",
    "- Adjust temperature and projection dimension\n",
    "- Use larger batch sizes if GPU memory allows (helps contrastive learning)\n",
    "- Consider adding mask information as additional augmentation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
