{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from preprocessing import run_preprocessing\n",
    "from windows import build_windows\n",
    "\n",
    "# Preprocess Data\n",
    "df_train, df_val, train_targets, val_targets = run_preprocessing()\n",
    "\n",
    "# Hyperparameters\n",
    "WINDOW_SIZE = 300\n",
    "STRIDE = 150\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Build sequences - returns 3D arrays (samples, timesteps, features)\n",
    "X_train, y_train, _ = build_windows(df_train, train_targets, WINDOW_SIZE, STRIDE, feature=\"3d\")\n",
    "X_val, y_val, _ = build_windows(df_val, val_targets, WINDOW_SIZE, STRIDE, feature=\"3d\")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
    "\n",
    "# Compute class distribution\n",
    "class_counts = np.bincount(y_train.astype(int))\n",
    "print(\"\\nðŸ“Š Class distribution in training set:\")\n",
    "for cls, count in enumerate(class_counts):\n",
    "    print(f\"  Class {cls}: {count} samples ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# WEIGHTED SAMPLING (to handle class imbalance)\n",
    "# ============================================================================\n",
    "# Calculate inverse-frequency weights per class\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / np.sum(class_weights)  # Normalize weights\n",
    "print(\"\\nâš–ï¸  Sample weights (inverse frequency):\")\n",
    "for cls, weight in enumerate(class_weights):\n",
    "    print(f\"  Class {cls}: {weight:.4f}\")\n",
    "\n",
    "# Assign a weight to each sample based on its class\n",
    "sample_weights = class_weights[y_train.astype(int)]\n",
    "sample_weights = torch.from_numpy(sample_weights).double()\n",
    "\n",
    "# Create WeightedRandomSampler (oversamples minority classes)\n",
    "'''\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "'''\n",
    "# ============================================================================\n",
    "# CREATE DATALOADERS\n",
    "# ============================================================================\n",
    "# Convert to PyTorch datasets\n",
    "'''\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "val_ds = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, sampler=sampler)\n",
    "val_loader = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "'''\n",
    "# CREATE DATALOADERS\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "val_ds = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "\n",
    "# Create data loaders - SOLO class weights, niente sampler\n",
    "train_loader = make_loader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,      # â† ORA TRUE\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = make_loader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "\n",
    "# Store metadata for model creation\n",
    "input_shape = X_train.shape\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"\\nâœ… DataLoaders created\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Input shape: {input_shape}\")\n",
    "print(f\"   Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59de246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CNN1DClassifier_bidirectional(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        hidden_size,\n",
    "        dropout=0.4,\n",
    "        bidirectional=False,\n",
    "        rnn_type='GRU',     # 'RNN', 'LSTM', 'GRU'\n",
    "        num_layers=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ====== CNN BLOCK ======\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=5, padding=2)\n",
    "        self.bn1   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn2   = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.pool    = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # ====== RNN BLOCK ======\n",
    "        self.rnn_type     = rnn_type\n",
    "        self.num_layers   = num_layers\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        rnn_map = {\n",
    "            'RNN': nn.RNN,\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        # Dropout solo tra i layer dell'RNN (se num_layers > 1)\n",
    "        rnn_dropout = dropout if num_layers > 1 else 0.0\n",
    "\n",
    "        # L'RNN prende in input i canali finali della CNN: 256\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=256,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=rnn_dropout\n",
    "        )\n",
    "\n",
    "        # ====== CLASSIFIER ======\n",
    "        if self.bidirectional:\n",
    "            classifier_input_size = hidden_size * 2  # concat fwd + bwd\n",
    "        else:\n",
    "            classifier_input_size = hidden_size\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(classifier_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features)\n",
    "        # CNN vuole (batch, channels, seq_len)\n",
    "        x = x.transpose(1, 2)   # -> (batch, features, seq_len)\n",
    "\n",
    "        # ----- CNN -----\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        # x: (batch, 256, seq_len_cnn)\n",
    "\n",
    "        # ----- RNN -----\n",
    "        # RNN vuole (batch, seq_len, features)\n",
    "        x_rnn_in = x.transpose(1, 2)  # -> (batch, seq_len_cnn, 256)\n",
    "\n",
    "        rnn_out, hidden = self.rnn(x_rnn_in)\n",
    "        # rnn_out: (batch, seq_len_cnn, hidden_size * num_directions)\n",
    "        # hidden:  (num_layers * num_directions, batch, hidden_size)\n",
    "        # per LSTM: hidden = (h_n, c_n), ci serve solo h_n\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # (num_layers, 2, batch, hidden_size)\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            # ultimi stati fwd e bwd dellâ€™ultima layer\n",
    "            hidden_to_classify = torch.cat(\n",
    "                [hidden[-1, 0, :, :], hidden[-1, 1, :, :]],\n",
    "                dim=1\n",
    "            )  # -> (batch, hidden_size * 2)\n",
    "        else:\n",
    "            hidden_to_classify = hidden[-1]  # (batch, hidden_size)\n",
    "\n",
    "        logits = self.fc(hidden_to_classify)\n",
    "        return logits\n",
    "\n",
    "model = CNN1DClassifier_bidirectional(\n",
    "    input_size=X_train.shape[-1],\n",
    "    num_classes=3,\n",
    "    hidden_size=256,\n",
    "    dropout=0.3,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "print(\"CNN_biderectional Model created with stronger regularization\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nðŸ”§ Model: {model.__class__.__name__}\")\n",
    "print(f\" Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. LOSS FUNCTION (with class weights for imbalance)\n",
    "# ----------------------------------------------------------------------------\n",
    "# Calculate class weights: inverse frequency with normalization\n",
    "'''\n",
    "train_class_counts = np.bincount(y_train.astype(int))\n",
    "class_weights_loss = len(y_train) / (len(train_class_counts) * train_class_counts)\n",
    "class_weights_loss = torch.tensor(class_weights_loss, dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"\\nâš–ï¸  Loss weights (amplifies gradients for minority classes):\")\n",
    "for cls, weight in enumerate(class_weights_loss):\n",
    "    print(f\"  Class {cls}: {weight:.4f}x\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_loss)\n",
    "'''\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. LOSS FUNCTION (with class weights for imbalance) - SOLO CLASS WEIGHTS\n",
    "# ----------------------------------------------------------------------------\n",
    "class_counts = np.bincount(y_train.astype(int))\n",
    "\n",
    "# Pesi inversamente proporzionali alla frequenza\n",
    "class_weights_loss = 1.0 / class_counts\n",
    "class_weights_loss = class_weights_loss / class_weights_loss.sum()  # opzionale, normalizza\n",
    "\n",
    "class_weights_loss = torch.tensor(class_weights_loss, dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"\\nâš–ï¸  Loss weights (amplifies gradients for minority classes):\")\n",
    "for cls, weight in enumerate(class_weights_loss):\n",
    "    print(f\"  Class {cls}: {weight:.4f}x\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_loss)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. OPTIMIZER\n",
    "# ----------------------------------------------------------------------------\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=5e-4,           # Learning rate\n",
    "    weight_decay=1e-4  # L2 regularization\n",
    ")\n",
    "\n",
    "# Alternative optimizers:\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. GRADIENT SCALER (for mixed precision training)\n",
    "# ----------------------------------------------------------------------------\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "from model_logic import fit\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training {model.__class__.__name__}...\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "_, history = fit(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=1500,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    writer=None,\n",
    "    verbose=3,\n",
    "    experiment_name=\"model_training\",\n",
    "    patience=100,       # Set > 0 for early stopping\n",
    "    l1_lambda=0,      # L1 regularization\n",
    "    l2_lambda=0       # L2 regularization (or use weight_decay in optimizer)\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“Š TRAINING RESULTS:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Initial val F1: {history['val_f1'][0]:.4f}\")\n",
    "print(f\"  Final val F1:   {history['val_f1'][-1]:.4f}\")\n",
    "print(f\"  Best val F1:    {max(history['val_f1']):.4f}\")\n",
    "print(f\"  Improvement:    {max(history['val_f1']) - history['val_f1'][0]:+.4f}\")\n",
    "\n",
    "# Per-class predictions\n",
    "from sklearn.metrics import classification_report\n",
    "model.eval()\n",
    "val_preds = []\n",
    "val_true = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "        val_true.extend(targets.cpu().numpy())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“ˆ CLASSIFICATION REPORT:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(\n",
    "    val_true, val_preds,\n",
    "    target_names=['no_pain', 'low_pain', 'high_pain'],\n",
    "    digits=4\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
