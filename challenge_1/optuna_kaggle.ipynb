{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2ca7226be0fb4aba8da572c1de6a4581":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_08ab6fe501124265a5b70344a65f355f","IPY_MODEL_13543bfea0c6448d845efd76dbc21740","IPY_MODEL_ee916d648ba9459d915b849a7faceb23"],"layout":"IPY_MODEL_8291d4a284804857b357d8172f8ddb2a"}},"08ab6fe501124265a5b70344a65f355f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b47fedd624449edbda54b48dbb11741","placeholder":"‚Äã","style":"IPY_MODEL_22c4af484e6347e29b635bbc868ff753","value":"Best‚Äátrial:‚Äá0.‚ÄáBest‚Äávalue:‚Äá0.0563104:‚Äá‚Äá‚Äá2%"}},"13543bfea0c6448d845efd76dbc21740":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_70caad3413314d7f9671f15ab612cb58","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1c230b755da4518a962177f2ac50975","value":1}},"ee916d648ba9459d915b849a7faceb23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d34ce560f5c848c9b222b15007b6282e","placeholder":"‚Äã","style":"IPY_MODEL_fea054748496413599098f17d3bda058","value":"‚Äá1/50‚Äá[02:26&lt;1:59:56,‚Äá146.88s/it,‚Äá146.84/21600‚Äáseconds]"}},"8291d4a284804857b357d8172f8ddb2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b47fedd624449edbda54b48dbb11741":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22c4af484e6347e29b635bbc868ff753":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70caad3413314d7f9671f15ab612cb58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1c230b755da4518a962177f2ac50975":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d34ce560f5c848c9b222b15007b6282e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fea054748496413599098f17d3bda058":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":647688,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":488518,"modelId":503944}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# CELL 1: SETUP KAGGLE\n# ============================================================================\n\n# Clone repository\n#!git clone https://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@github.com/francinze/Ch1_An2DL.git\n\n# Install dependencies\n!pip install -q kaggle optuna\n\n# Setup Kaggle credentials\n#!mkdir -p ~/.kaggle\n#!cp Ch1_An2DL/kaggle.json ~/.kaggle/\n#!chmod 600 ~/.kaggle/kaggle.json\n\n# Download competition data\n#!kaggle competitions download -c an2dl2526c1\n#!unzip -q an2dl2526c1.zip -d Ch1_An2DL/\n\n# Change to working directory\n%cd /kaggle/input/ch-an2dl/pytorch/default/1/Ch1_An2DL\n\n# Verify files are present\nimport os\nprint(\"\\n‚úÖ Setup complete! Files in directory:\")\nfor file in ['pirate_pain_train.csv', 'pirate_pain_test.csv', 'pirate_pain_train_labels.csv']:\n    if os.path.exists(file):\n        size_mb = os.path.getsize(file) / (1024 * 1024)\n        print(f\"   ‚úì {file} ({size_mb:.2f} MB)\")\n    else:\n        print(f\"   ‚úó {file} NOT FOUND!\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ert72eTQ9LyU","outputId":"14c97a47-70eb-4d63-e735-d3aef9d8e95d","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T00:50:28.712037Z","iopub.execute_input":"2025-11-17T00:50:28.712808Z","iopub.status.idle":"2025-11-17T00:50:31.917002Z","shell.execute_reply.started":"2025-11-17T00:50:28.712774Z","shell.execute_reply":"2025-11-17T00:50:31.916061Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ch-an2dl/pytorch/default/1/Ch1_An2DL\n\n‚úÖ Setup complete! Files in directory:\n   ‚úì pirate_pain_train.csv (60.78 MB)\n   ‚úì pirate_pain_test.csv (125.24 MB)\n   ‚úì pirate_pain_train_labels.csv (0.01 MB)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 2: IMPORTS & SEED SETUP\n# ============================================================================\nimport os\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\nimport optuna\nfrom optuna.pruners import MedianPruner\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom collections import Counter\nfrom tqdm import tqdm\nimport pickle\n\n# Set seed for reproducibility\nSEED = 42\nos.environ['PYTHONHASHSEED'] = str(SEED)\nos.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\n\nnp.random.seed(SEED)\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"üêç Python packages:\")\nprint(f\"   PyTorch: {torch.__version__}\")\nprint(f\"   Optuna: {optuna.__version__}\")\nprint(f\"   Pandas: {pd.__version__}\")\nprint(f\"   NumPy: {np.__version__}\")\nprint(f\"\\nüñ•  Device: {device}\")\nprint(f\"   Random seed: {SEED}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6S_k5YMv9SpJ","outputId":"222fd2a0-0e07-4a62-ef4e-a350d0e1f5e7","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T00:50:31.918778Z","iopub.execute_input":"2025-11-17T00:50:31.919018Z","iopub.status.idle":"2025-11-17T00:50:31.930553Z","shell.execute_reply.started":"2025-11-17T00:50:31.918994Z","shell.execute_reply":"2025-11-17T00:50:31.929929Z"}},"outputs":[{"name":"stdout","text":"üêç Python packages:\n   PyTorch: 2.6.0+cu124\n   Optuna: 4.5.0\n   Pandas: 2.2.3\n   NumPy: 1.26.4\n\nüñ•  Device: cuda\n   Random seed: 42\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 3: PREPROCESSING FUNCTIONS (INLINE)\n# ============================================================================\n\ndef add_time_features(df_train, df_test):\n    \"\"\"\n    Add time-based features implementing November 12 clue.\n    Creates 4 new features from 'time' column:\n    - time_normalized: position in sequence [0.0, 1.0]\n    - time_sin, time_cos: cyclical encoding\n    - time_position: categorical [0=early, 1=mid, 2=late]\n    \"\"\"\n    print(\"\\n‚è∞ Adding time-based features...\")\n    print(\"=\" * 60)\n\n    for df, name in [(df_train, 'train'), (df_test, 'test')]:\n        # Convert time to numeric if needed\n        if df['time'].dtype == 'object':\n            df['time'] = pd.to_datetime(df['time'])\n            df['time'] = (df['time'] - df['time'].min()).dt.total_seconds()\n\n        # Feature 1: Normalized time (position in sequence: 0.0 to 1.0)\n        df['time_normalized'] = df.groupby('sample_index')['time'].transform(\n            lambda x: x / x.max() if x.max() > 0 else 0\n        )\n\n        # Analyze sequence lengths for cyclical period\n        lengths = df.groupby('sample_index')['time'].max()\n        avg_length = lengths.mean()\n\n        # Feature 2 & 3: Cyclical encoding (captures periodic patterns)\n        period = max(50, avg_length / 3)  # ~3 cycles per sequence\n        df['time_sin'] = np.sin(2 * np.pi * df['time'] / period)\n        df['time_cos'] = np.cos(2 * np.pi * df['time'] / period)\n\n        # Feature 4: Time position categories (early/mid/late)\n        def categorize_time_position(group):\n            normalized = group / group.max() if group.max() > 0 else 0\n            return pd.cut(normalized, bins=[0, 0.33, 0.66, 1.0],\n                         labels=[0, 1, 2], include_lowest=True).astype(int)\n\n        df['time_position'] = df.groupby('sample_index')['time'].transform(categorize_time_position)\n\n        print(f\"‚úÖ {name.capitalize()} set: Added 4 time features\")\n        print(f\"   - Avg sequence length: {avg_length:.1f} timesteps\")\n        print(f\"   - Cyclical period: {period:.1f} timesteps\")\n\n    # Show distribution\n    print(f\"\\nüìä Time position distribution:\")\n    for label, value in [('Early', 0), ('Mid', 1), ('Late', 2)]:\n        count = (df_train['time_position'] == value).sum()\n        pct = (count / len(df_train)) * 100\n        print(f\"   {label}: {count:,} ({pct:.1f}%)\")\n\n    return df_train, df_test\n\n\ndef add_prosthetics_feature(df, df_test):\n    \"\"\"Create binary prosthetics feature\"\"\"\n    print(\"\\nü¶æ Creating 'has_prosthetics' feature...\")\n    print(\"=\" * 60)\n\n    df['has_prosthetics'] = (df['n_legs'] != 'two').astype(int)\n    df_test['has_prosthetics'] = (df_test['n_legs'] != 'two').astype(int)\n\n    # Show distribution\n    print(f\"\\nTraining set:\")\n    train_dist = df['has_prosthetics'].value_counts().sort_index()\n    for value, count in train_dist.items():\n        label = \"Natural\" if value == 0 else \"Prosthetics\"\n        pct = (count / len(df)) * 100\n        print(f\"  {value} ({label:12s}): {count:6,} samples ({pct:.2f}%)\")\n\n    print(f\"\\nTest set:\")\n    test_dist = df_test['has_prosthetics'].value_counts().sort_index()\n    for value, count in test_dist.items():\n        label = \"Natural\" if value == 0 else \"Prosthetics\"\n        pct = (count / len(df_test)) * 100\n        print(f\"  {value} ({label:12s}): {count:6,} samples ({pct:.2f}%)\")\n\n    # Drop original columns\n    cols_to_drop = ['n_legs', 'n_hands', 'n_eyes',\n                    'n_legs_encoded', 'n_hands_encoded', 'n_eyes_encoded']\n    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n    df_test = df_test.drop(columns=[col for col in cols_to_drop if col in df_test.columns])\n\n    print(\"\\n‚úÖ Feature created successfully!\")\n    return df, df_test\n\n\ndef scale_joint_columns(df, scaler=None):\n    \"\"\"Apply Min-Max normalization to joint columns\"\"\"\n    print(\"\\nüìè Applying Min-Max normalization...\")\n    print(\"=\" * 60)\n\n    # All possible joint columns (0-29)\n    all_joint_cols = [f\"joint_{str(i).zfill(2)}\" for i in range(30)]\n\n    # Filter only existing columns (handles joint_11, joint_30 already dropped)\n    joint_cols = [c for c in all_joint_cols if c in df.columns]\n\n    print(f\"   Found {len(joint_cols)} joint columns to scale\")\n    print(f\"   (Expected 28 after dropping joint_11 and joint_30)\")\n\n    # Ensure float32 type\n    for col in joint_cols:\n        df[col] = df[col].astype(np.float32)\n\n    if scaler is None:\n        scaler = MinMaxScaler()\n        df[joint_cols] = scaler.fit_transform(df[joint_cols])\n        print(f\"‚úÖ Scaler fitted on training data\")\n        print(f\"   Min (first 5): {scaler.data_min_[:5]}\")\n        print(f\"   Max (first 5): {scaler.data_max_[:5]}\")\n    else:\n        df[joint_cols] = scaler.transform(df[joint_cols])\n        print(f\"‚úÖ Scaler applied to test/validation data\")\n\n    return df, scaler\n\n\ndef apply_target_weighting(target):\n    \"\"\"Map labels to integers and show distribution\"\"\"\n    print(\"\\n‚öñ  Processing target labels...\")\n    print(\"=\" * 60)\n\n    label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n\n    # Show original distribution\n    print(\"Original label distribution:\")\n    for label in target['label'].unique():\n        count = len(target[target['label'] == label])\n        pct = (count / len(target)) * 100\n        print(f\"  {label}: {count} ({pct:.2f}%)\")\n\n    target['label'] = target['label'].map(label_mapping)\n\n    print(f\"\\n‚úÖ Labels mapped: {label_mapping}\")\n    return target\n\n\ndef train_val_split(df, target, val_ratio=0.2):\n    \"\"\"Split data by unique users\"\"\"\n    print(f\"\\n‚úÇ  Train/validation split ({int((1-val_ratio)*100)}/{int(val_ratio*100)})...\")\n    print(\"=\" * 60)\n\n    unique_users = df['sample_index'].unique()\n    random.seed(SEED)\n    random.shuffle(unique_users)\n\n    num_val_users = int(len(unique_users) * val_ratio)\n    val_users = unique_users[:num_val_users]\n    train_users = unique_users[num_val_users:]\n\n    train_df = df[df['sample_index'].isin(train_users)].reset_index(drop=True)\n    val_df = df[df['sample_index'].isin(val_users)].reset_index(drop=True)\n    train_target = target[target['sample_index'].isin(train_users)].reset_index(drop=True)\n    val_target = target[target['sample_index'].isin(val_users)].reset_index(drop=True)\n\n    print(f\"‚úÖ Training users: {len(train_users)}, Validation users: {len(val_users)}\")\n    print(f\"   Training samples: {len(train_df):,}, Validation samples: {len(val_df):,}\")\n\n    # Show class distribution\n    print(f\"\\nTraining label distribution:\")\n    for label in sorted(train_target['label'].unique()):\n        count = (train_target['label'] == label).sum()\n        pct = (count / len(train_target)) * 100\n        print(f\"   Class {label}: {count} ({pct:.1f}%)\")\n\n    print(f\"\\nValidation label distribution:\")\n    for label in sorted(val_target['label'].unique()):\n        count = (val_target['label'] == label).sum()\n        pct = (count / len(val_target)) * 100\n        print(f\"   Class {label}: {count} ({pct:.1f}%)\")\n\n    return train_df, val_df, train_target, val_target\n\n\nprint(\"‚úÖ Preprocessing functions defined\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"moajbOkO9ceb","outputId":"17d34f06-22b1-4c91-e4ce-8f9c85991a32","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T00:50:31.931310Z","iopub.execute_input":"2025-11-17T00:50:31.931572Z","iopub.status.idle":"2025-11-17T00:50:31.953930Z","shell.execute_reply.started":"2025-11-17T00:50:31.931545Z","shell.execute_reply":"2025-11-17T00:50:31.953306Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Preprocessing functions defined\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================================================\n# CELL 4: WINDOW BUILDING FUNCTION (INLINE)\n# ============================================================================\n\nLABEL_MAP = {\"no_pain\": 0, \"low_pain\": 1, \"high_pain\": 2}\n\ndef build_windows(df, targets=None, window_size=110, stride=22, feature=\"3d\"):\n    \"\"\"\n    Build sliding windows from time-series data.\n\n    Args:\n        df: DataFrame with sample_index and feature columns\n        targets: DataFrame with labels (None for test set)\n        window_size: Size of each window\n        stride: Step size between windows\n        feature: \"3d\" for (samples, timesteps, features) shape\n\n    Returns:\n        X: numpy array of windows\n        y: numpy array of labels (or None)\n        sample_mapping: list of (sample_index, window_idx) tuples\n    \"\"\"\n    print(f\"\\nü™ü Building windows (size={window_size}, stride={stride})...\")\n    print(\"=\" * 60)\n\n    # Detect feature columns (exclude metadata)\n    metadata_cols = ['sample_index', 'time', 'Unnamed: 0', 'index']\n    data_cols = [c for c in df.columns if c not in metadata_cols]\n\n    print(f\"   Feature columns: {len(data_cols)}\")\n    print(f\"   First 5: {data_cols[:5]}\")\n    print(f\"   Last 5: {data_cols[-5:]}\")\n\n    X_list = []\n    y_list = []\n    sample_mapping = []\n\n    unique_samples = sorted(df['sample_index'].unique())\n\n    for sample_idx in unique_samples:\n        sample_data = df[df['sample_index'] == sample_idx][data_cols].values\n\n        if len(sample_data) == 0:\n            print(f\"‚ö†  Warning: Sample {sample_idx} has no data, skipping...\")\n            continue\n\n        # Get label if available\n        if targets is not None:\n            label_row = targets[targets['sample_index'] == sample_idx]\n            if len(label_row) == 0:\n                print(f\"‚ö†  Warning: Sample {sample_idx} has no label, skipping...\")\n                continue\n\n            label_value = label_row['label'].values[0]\n            label = LABEL_MAP.get(label_value, label_value) if isinstance(label_value, str) else int(label_value)\n        else:\n            label = None\n\n        # Sliding window\n        for start in range(0, len(sample_data) - window_size + 1, stride):\n            window = sample_data[start:start + window_size]\n\n            if window.shape[0] != window_size:\n                continue\n\n            X_list.append(window)\n            if label is not None:\n                y_list.append(label)\n            sample_mapping.append((sample_idx, len(X_list) - 1))\n\n    X = np.array(X_list, dtype=np.float32)\n    y = np.array(y_list, dtype=np.int64) if y_list else None\n\n    print(f\"\\n‚úÖ Window creation complete:\")\n    print(f\"   Total windows: {len(X):,}\")\n    print(f\"   From {len(unique_samples)} samples\")\n    print(f\"   Window shape: {X.shape}\")\n\n    if y is not None:\n        print(f\"   Label shape: {y.shape}\")\n        print(f\"   Label distribution: {np.bincount(y)}\")\n\n    return X, y, sample_mapping\n\n\nprint(\"‚úÖ Window building function defined\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KnT104eG9irm","outputId":"801ae402-8357-4709-9adc-68f44f470a9e","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T00:50:31.955369Z","iopub.execute_input":"2025-11-17T00:50:31.955589Z","iopub.status.idle":"2025-11-17T00:50:31.969143Z","shell.execute_reply.started":"2025-11-17T00:50:31.955574Z","shell.execute_reply":"2025-11-17T00:50:31.968518Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Window building function defined\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: BILSTM MODEL (INLINE)\n# ============================================================================\n\nclass BiLSTM(nn.Module):\n    def __init__(self, input_size, num_classes, hidden_size=128, num_layers=2, dropout_rate=0.3):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout_rate if num_layers > 1 else 0.0\n        )\n        self.attention_weights = nn.Linear(hidden_size * 2, 1)\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_size * 2, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_size, num_classes)\n        )\n\n    def attention(self, lstm_output):\n        attn_scores = self.attention_weights(lstm_output)\n        attn_weights = torch.softmax(attn_scores, dim=1)\n        context = torch.sum(attn_weights * lstm_output, dim=1)\n        return context\n\n    def forward(self, x):\n        lstm_output, _ = self.lstm(x)\n        context = self.attention(lstm_output)\n        return self.classifier(context)\n\n\nprint(\"‚úÖ BiLSTM model class defined\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLWiAMis9oH5","outputId":"b3e7d142-0502-42aa-b321-e82c51b88f5b","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T00:50:31.969891Z","iopub.execute_input":"2025-11-17T00:50:31.970105Z","iopub.status.idle":"2025-11-17T00:50:31.984866Z","shell.execute_reply.started":"2025-11-17T00:50:31.970088Z","shell.execute_reply":"2025-11-17T00:50:31.984243Z"}},"outputs":[{"name":"stdout","text":"‚úÖ BiLSTM model class defined\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"\n\n\n# ============================================================================\n# CELL 6: TRAINING FUNCTIONS (INLINE)\n# ============================================================================\n\ndef train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_targets = []\n\n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        optimizer.zero_grad()\n\n        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            if l1_lambda > 0:\n                l1_norm = sum(p.abs().sum() for p in model.parameters())\n                loss = loss + l1_lambda * l1_norm\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())\n\n    avg_loss = total_loss / len(train_loader)\n    f1 = f1_score(all_targets, all_preds, average='macro')\n\n    return avg_loss, f1\n\n\ndef validate_one_epoch(model, val_loader, criterion, device):\n    \"\"\"Validate for one epoch\"\"\"\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_targets = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n\n            total_loss += loss.item()\n            preds = outputs.argmax(dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    avg_loss = total_loss / len(val_loader)\n    f1 = f1_score(all_targets, all_preds, average='macro')\n\n    return avg_loss, f1\n\n\ndef fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n        patience=30, l1_lambda=0, verbose=10):\n    \"\"\"Training loop with early stopping\"\"\"\n\n    history = {\n        'train_loss': [], 'train_f1': [],\n        'val_loss': [], 'val_f1': []\n    }\n\n    best_val_f1 = 0\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        train_loss, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda)\n        val_loss, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n\n        history['train_loss'].append(train_loss)\n        history['train_f1'].append(train_f1)\n        history['val_loss'].append(val_loss)\n        history['val_f1'].append(val_f1)\n\n        if verbose > 0 and (epoch + 1) % verbose == 0:\n            print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n                  f\"Train Loss: {train_loss:.4f} F1: {train_f1:.4f} | \"\n                  f\"Val Loss: {val_loss:.4f} F1: {val_f1:.4f}\")\n\n        # Early stopping\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience > 0 and patience_counter >= patience:\n                print(f\"‚èπ  Early stopping at epoch {epoch+1} (patience={patience})\")\n                break\n\n    return model, history\n\n\ndef make_loader(ds, batch_size, shuffle, drop_last, sampler=None):\n    \"\"\"Create DataLoader with optimal settings\"\"\"\n    cpu_cores = os.cpu_count() or 2\n    num_workers = max(2, min(4, cpu_cores))\n\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=shuffle if sampler is None else False,\n        sampler=sampler,\n        drop_last=drop_last,\n        num_workers=num_workers,\n        pin_memory=True,\n        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n        prefetch_factor=4,\n    )\n\n\nprint(\"‚úÖ Training functions defined\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40wTbRto9t0s","outputId":"c7f19a22-e280-449c-ab06-531761dd232b","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T00:50:31.985507Z","iopub.execute_input":"2025-11-17T00:50:31.985765Z","iopub.status.idle":"2025-11-17T00:50:32.005599Z","shell.execute_reply.started":"2025-11-17T00:50:31.985746Z","shell.execute_reply":"2025-11-17T00:50:32.004866Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Training functions defined\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 7: DATA LOADING & PREPROCESSING\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üì¶ LOADING AND PREPROCESSING DATA\")\nprint(\"=\"*80)\n\n# Load data\nprint(\"\\n1Ô∏è‚É£  Loading CSV files...\")\ndf = pd.read_csv(\"pirate_pain_train.csv\")\ndf_test = pd.read_csv(\"pirate_pain_test.csv\")\ntarget = pd.read_csv(\"pirate_pain_train_labels.csv\")\n\nprint(f\"   Training data: {df.shape}\")\nprint(f\"   Test data: {df_test.shape}\")\nprint(f\"   Target data: {target.shape}\")\n\n# Drop problematic joints FIRST\nprint(\"\\n2Ô∏è‚É£  Dropping problematic joints (joint_11, joint_30)...\")\ndf = df.drop(columns=['joint_30', 'joint_11'], errors='ignore')\ndf_test = df_test.drop(columns=['joint_30', 'joint_11'], errors='ignore')\nprint(f\"   Training data after drop: {df.shape}\")\nprint(f\"   Test data after drop: {df_test.shape}\")\n\n# Add time features BEFORE dropping time column\nprint(\"\\n3Ô∏è‚É£  Adding time-based features...\")\ndf, df_test = add_time_features(df, df_test)\n\n# Add prosthetics feature\nprint(\"\\n4Ô∏è‚É£  Adding prosthetics feature...\")\ndf, df_test = add_prosthetics_feature(df, df_test)\n\n# Drop time column AFTER extracting features\nprint(\"\\n5Ô∏è‚É£  Dropping 'time' column...\")\ndf = df.drop(columns='time', errors='ignore')\ndf_test = df_test.drop(columns='time', errors='ignore')\nprint(f\"   Training data: {df.shape}\")\nprint(f\"   Test data: {df_test.shape}\")\n\n# Scale joint columns\nprint(\"\\n6Ô∏è‚É£  Scaling joint columns...\")\ndf, scaler = scale_joint_columns(df, scaler=None)\ndf_test, _ = scale_joint_columns(df_test, scaler=scaler)\n\n# Process targets\nprint(\"\\n7Ô∏è‚É£  Processing target labels...\")\ntarget = apply_target_weighting(target)\n\n# Train/Val split\nprint(\"\\n8Ô∏è‚É£  Splitting train/validation...\")\ntrain_df, val_df, train_target, val_target = train_val_split(df, target, val_ratio=0.2)\n\n# FINAL CHECK: Verify no 'time' column exists\nprint(\"\\n9Ô∏è‚É£  Final column check...\")\nprint(f\"   Training columns ({len(train_df.columns)} total): {list(train_df.columns)[:10]}...\")\nprint(f\"   Validation columns ({len(val_df.columns)} total): {list(val_df.columns)[:10]}...\")\nprint(f\"   Test columns ({len(df_test.columns)} total): {list(df_test.columns)[:10]}...\")\n\n# Verify 'time' is not in columns\nassert 'time' not in train_df.columns, \"‚ùå ERROR: 'time' still in train_df!\"\nassert 'time' not in val_df.columns, \"‚ùå ERROR: 'time' still in val_df!\"\nassert 'time' not in df_test.columns, \"‚ùå ERROR: 'time' still in df_test!\"\n\nprint(\"\\n‚úÖ Data preprocessing complete!\")\nprint(f\"   Training set: {len(train_df):,} samples ({len(train_df['sample_index'].unique())} users)\")\nprint(f\"   Validation set: {len(val_df):,} samples ({len(val_df['sample_index'].unique())} users)\")\nprint(f\"   Test set: {len(df_test):,} samples ({len(df_test['sample_index'].unique())} users)\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"opyQvKtS9xdR","outputId":"3d90700a-7429-4eac-a696-f34dba58e725","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T00:50:32.006527Z","iopub.execute_input":"2025-11-17T00:50:32.006800Z","iopub.status.idle":"2025-11-17T00:50:35.972437Z","shell.execute_reply.started":"2025-11-17T00:50:32.006777Z","shell.execute_reply":"2025-11-17T00:50:35.971754Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüì¶ LOADING AND PREPROCESSING DATA\n================================================================================\n\n1Ô∏è‚É£  Loading CSV files...\n   Training data: (105760, 40)\n   Test data: (211840, 40)\n   Target data: (661, 2)\n\n2Ô∏è‚É£  Dropping problematic joints (joint_11, joint_30)...\n   Training data after drop: (105760, 38)\n   Test data after drop: (211840, 38)\n\n3Ô∏è‚É£  Adding time-based features...\n\n‚è∞ Adding time-based features...\n============================================================\n‚úÖ Train set: Added 4 time features\n   - Avg sequence length: 159.0 timesteps\n   - Cyclical period: 53.0 timesteps\n‚úÖ Test set: Added 4 time features\n   - Avg sequence length: 159.0 timesteps\n   - Cyclical period: 53.0 timesteps\n\nüìä Time position distribution:\n   Early: 35,033 (33.1%)\n   Mid: 34,372 (32.5%)\n   Late: 36,355 (34.4%)\n\n4Ô∏è‚É£  Adding prosthetics feature...\n\nü¶æ Creating 'has_prosthetics' feature...\n============================================================\n\nTraining set:\n  0 (Natural     ): 104,800 samples (99.09%)\n  1 (Prosthetics ):    960 samples (0.91%)\n\nTest set:\n  0 (Natural     ): 209,760 samples (99.02%)\n  1 (Prosthetics ):  2,080 samples (0.98%)\n\n‚úÖ Feature created successfully!\n\n5Ô∏è‚É£  Dropping 'time' column...\n   Training data: (105760, 39)\n   Test data: (211840, 39)\n\n6Ô∏è‚É£  Scaling joint columns...\n\nüìè Applying Min-Max normalization...\n============================================================\n   Found 29 joint columns to scale\n   (Expected 28 after dropping joint_11 and joint_30)\n‚úÖ Scaler fitted on training data\n   Min (first 5): [0.         0.         0.00101504 0.00540321 0.        ]\n   Max (first 5): [1.407968  1.3346131 1.3060458 1.2547286 1.3592042]\n\nüìè Applying Min-Max normalization...\n============================================================\n   Found 29 joint columns to scale\n   (Expected 28 after dropping joint_11 and joint_30)\n‚úÖ Scaler applied to test/validation data\n\n7Ô∏è‚É£  Processing target labels...\n\n‚öñ  Processing target labels...\n============================================================\nOriginal label distribution:\n  no_pain: 511 (77.31%)\n  low_pain: 94 (14.22%)\n  high_pain: 56 (8.47%)\n\n‚úÖ Labels mapped: {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n\n8Ô∏è‚É£  Splitting train/validation...\n\n‚úÇ  Train/validation split (80/20)...\n============================================================\n‚úÖ Training users: 529, Validation users: 132\n   Training samples: 84,640, Validation samples: 21,120\n\nTraining label distribution:\n   Class 0: 417 (78.8%)\n   Class 1: 68 (12.9%)\n   Class 2: 44 (8.3%)\n\nValidation label distribution:\n   Class 0: 94 (71.2%)\n   Class 1: 26 (19.7%)\n   Class 2: 12 (9.1%)\n\n9Ô∏è‚É£  Final column check...\n   Training columns (39 total): ['sample_index', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04']...\n   Validation columns (39 total): ['sample_index', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04']...\n   Test columns (39 total): ['sample_index', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'joint_00', 'joint_01', 'joint_02', 'joint_03', 'joint_04']...\n\n‚úÖ Data preprocessing complete!\n   Training set: 84,640 samples (529 users)\n   Validation set: 21,120 samples (132 users)\n   Test set: 211,840 samples (1324 users)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 8: K-FOLD SETUP\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä SETTING UP K-FOLD CROSS-VALIDATION\")\nprint(\"=\"*80)\n\nK_FOLDS = 5\nWINDOW_SIZE = 110\nSTRIDE = 22\n\n# Build windows from training data\nX_train, y_train, _ = build_windows(train_df, train_target, WINDOW_SIZE, STRIDE, feature=\"3d\")\nX_val, y_val, _ = build_windows(val_df, val_target, WINDOW_SIZE, STRIDE, feature=\"3d\")\n\nprint(f\"\\nüìê Data shapes:\")\nprint(f\"   Training: X={X_train.shape}, y={y_train.shape}\")\nprint(f\"   Validation: X={X_val.shape}, y={y_val.shape}\")\n\n# Class distribution\nclass_counts = np.bincount(y_train.astype(int))\nprint(f\"\\nüìä Training class distribution:\")\nfor cls, count in enumerate(class_counts):\n    print(f\"   Class {cls}: {count:,} samples ({count/len(y_train)*100:.1f}%)\")\n\n# Store metadata\ninput_shape = X_train.shape\nnum_classes = len(np.unique(y_train))\n\nprint(f\"\\n‚úÖ K-Fold setup complete\")\nprint(f\"   K={K_FOLDS} folds\")\nprint(f\"   Input features: {input_shape[-1]}\")\nprint(f\"   Number of classes: {num_classes}\")\n\n# Create K-Fold splits (fixed, will be reused for all Optuna trials)\nkfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\nfold_indices = list(kfold.split(X_train))\n\nprint(f\"   Fold splits created: {len(fold_indices)} folds ready\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6POK4wcm91yf","outputId":"5bc5adfd-b229-4270-88f9-b09bed47b0bd","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T00:50:35.973212Z","iopub.execute_input":"2025-11-17T00:50:35.973508Z","iopub.status.idle":"2025-11-17T00:50:36.851433Z","shell.execute_reply.started":"2025-11-17T00:50:35.973482Z","shell.execute_reply":"2025-11-17T00:50:36.850726Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüìä SETTING UP K-FOLD CROSS-VALIDATION\n================================================================================\n\nü™ü Building windows (size=110, stride=22)...\n============================================================\n   Feature columns: 38\n   First 5: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'joint_00']\n   Last 5: ['time_normalized', 'time_sin', 'time_cos', 'time_position', 'has_prosthetics']\n\n‚úÖ Window creation complete:\n   Total windows: 1,587\n   From 529 samples\n   Window shape: (1587, 110, 38)\n   Label shape: (1587,)\n   Label distribution: [1251  204  132]\n\nü™ü Building windows (size=110, stride=22)...\n============================================================\n   Feature columns: 38\n   First 5: ['pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4', 'joint_00']\n   Last 5: ['time_normalized', 'time_sin', 'time_cos', 'time_position', 'has_prosthetics']\n\n‚úÖ Window creation complete:\n   Total windows: 396\n   From 132 samples\n   Window shape: (396, 110, 38)\n   Label shape: (396,)\n   Label distribution: [282  78  36]\n\nüìê Data shapes:\n   Training: X=(1587, 110, 38), y=(1587,)\n   Validation: X=(396, 110, 38), y=(396,)\n\nüìä Training class distribution:\n   Class 0: 1,251 samples (78.8%)\n   Class 1: 204 samples (12.9%)\n   Class 2: 132 samples (8.3%)\n\n‚úÖ K-Fold setup complete\n   K=5 folds\n   Input features: 38\n   Number of classes: 3\n   Fold splits created: 5 folds ready\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 9: OPTUNA OBJECTIVE FUNCTION\n# ============================================================================\n\ndef optuna_objective(trial):\n    \"\"\"\n    Optuna objective function.\n    Trains model on K-Fold and returns average validation F1.\n    \"\"\"\n\n    # ========================================================================\n    # SUGGEST HYPERPARAMETERS\n    # ========================================================================\n    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256, 512])\n    num_layers = trial.suggest_int('num_layers', 1, 3)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5, step=0.1)\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n    l1_lambda = trial.suggest_categorical('l1_lambda', [0, 0.001, 0.01])\n    l2_lambda = trial.suggest_categorical('l2_lambda', [0, 1e-5, 1e-4, 1e-3])\n\n    # ========================================================================\n    # K-FOLD TRAINING\n    # ========================================================================\n    fold_scores = []\n\n    for fold_idx, (train_idx, val_idx) in enumerate(fold_indices):\n        # Split data for this fold\n        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n\n        # Create datasets\n        train_ds = TensorDataset(torch.from_numpy(X_fold_train).float(), torch.from_numpy(y_fold_train).long())\n        val_ds = TensorDataset(torch.from_numpy(X_fold_val).float(), torch.from_numpy(y_fold_val).long())\n\n        # Weighted sampling for class imbalance\n        fold_class_counts = np.bincount(y_fold_train.astype(int))\n        class_weights_sampling = 1.0 / fold_class_counts\n        class_weights_sampling = class_weights_sampling / np.sum(class_weights_sampling)\n        sample_weights = class_weights_sampling[y_fold_train.astype(int)]\n        sample_weights = torch.from_numpy(sample_weights).float()\n\n        sampler = WeightedRandomSampler(\n            weights=sample_weights,\n            num_samples=len(sample_weights),\n            replacement=True\n        )\n\n        # Create data loaders\n        train_loader = make_loader(train_ds, batch_size=batch_size, shuffle=False, drop_last=True, sampler=sampler)\n        val_loader = make_loader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n\n        # Create model\n        model = BiLSTM(\n            input_size=input_shape[-1],\n            num_classes=num_classes,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            dropout_rate=dropout_rate\n        ).to(device)\n\n        # Loss & Optimizer\n        fold_class_weights_loss = len(y_fold_train) / (len(fold_class_counts) * fold_class_counts)\n        fold_class_weights_loss = torch.tensor(fold_class_weights_loss, dtype=torch.float32).to(device)\n        criterion = nn.CrossEntropyLoss(weight=fold_class_weights_loss)\n\n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=learning_rate,\n            weight_decay=l2_lambda\n        )\n\n        scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n\n        # Train\n        _, history = fit(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            epochs=30,\n            criterion=criterion,\n            optimizer=optimizer,\n            scaler=scaler,\n            device=device,\n            patience=50,\n            l1_lambda=l1_lambda,\n            verbose=0  # Silent during Optuna\n        )\n\n        # Get best F1 for this fold\n        best_f1 = max(history['val_f1'])\n        fold_scores.append(best_f1)\n\n        # Report intermediate value for pruning\n        trial.report(best_f1, fold_idx)\n\n        # Prune if performing poorly\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n\n    # Return average F1 across all folds\n    avg_f1 = np.mean(fold_scores)\n    return avg_f1\n\n\nprint(\"‚úÖ Optuna objective function defined\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2qcCILG096l8","outputId":"4b1fa771-6f7b-4d59-f1b4-c68840f1d5f4","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T00:50:36.852198Z","iopub.execute_input":"2025-11-17T00:50:36.852447Z","iopub.status.idle":"2025-11-17T00:50:36.864095Z","shell.execute_reply.started":"2025-11-17T00:50:36.852430Z","shell.execute_reply":"2025-11-17T00:50:36.863408Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Optuna objective function defined\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 10: RUN OPTUNA OPTIMIZATION\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üîç STARTING OPTUNA HYPERPARAMETER OPTIMIZATION\")\nprint(\"=\"*80)\n\n# Optuna configuration\nN_TRIALS = 50\nTIMEOUT = 6 * 3600  # 6 hours\n\n# Create pruner\npruner = MedianPruner(\n    n_startup_trials=5,\n    n_warmup_steps=30,\n    interval_steps=10\n)\n\n# Create study\nstudy = optuna.create_study(\n    direction='maximize',\n    pruner=pruner,\n    study_name='bilstm_kfold_optimization'\n)\n\nprint(f\"\\n‚öô  Configuration:\")\nprint(f\"   Trials: {N_TRIALS}\")\nprint(f\"   Timeout: {TIMEOUT/3600:.1f} hours\")\nprint(f\"   K-Folds: {K_FOLDS}\")\nprint(f\"   Epochs per trial: 200 (with patience=30)\")\nprint(f\"   Pruning: Enabled (MedianPruner)\")\n\nprint(f\"\\nüöÄ Starting optimization...\")\nprint(f\"   This will take approximately 5-8 hours\")\nprint(\"=\"*80)\n\n# Run optimization\nstudy.optimize(\n    optuna_objective,\n    n_trials=N_TRIALS,\n    timeout=TIMEOUT,\n    show_progress_bar=True\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ OPTUNA OPTIMIZATION COMPLETE!\")\nprint(\"=\"*80)\n\n# Best trial\nbest_trial = study.best_trial\nprint(f\"\\nüèÜ Best Trial:\")\nprint(f\"   Trial number: {best_trial.number}\")\nprint(f\"   Best F1 score: {best_trial.value:.4f}\")\nprint(f\"\\nüéØ Best Hyperparameters:\")\nfor key, value in best_trial.params.items():\n    print(f\"   {key}: {value}\")\n\n# Save study\nwith open('optuna_study_bilstm.pkl', 'wb') as f:\n    pickle.dump(study, f)\nprint(f\"\\nüíæ Study saved to 'optuna_study_bilstm.pkl'\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":451,"referenced_widgets":["2ca7226be0fb4aba8da572c1de6a4581","08ab6fe501124265a5b70344a65f355f","13543bfea0c6448d845efd76dbc21740","ee916d648ba9459d915b849a7faceb23","8291d4a284804857b357d8172f8ddb2a","5b47fedd624449edbda54b48dbb11741","22c4af484e6347e29b635bbc868ff753","70caad3413314d7f9671f15ab612cb58","b1c230b755da4518a962177f2ac50975","d34ce560f5c848c9b222b15007b6282e","fea054748496413599098f17d3bda058"]},"id":"uk_fZplF999x","outputId":"6f50b09d-ae9a-4994-d1dd-61def76e81a3","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T00:50:36.866225Z","iopub.execute_input":"2025-11-17T00:50:36.866444Z","iopub.status.idle":"2025-11-17T02:59:14.676863Z","shell.execute_reply.started":"2025-11-17T00:50:36.866429Z","shell.execute_reply":"2025-11-17T02:59:14.675824Z"}},"outputs":[{"name":"stderr","text":"[I 2025-11-17 00:50:36,877] A new study created in memory with name: bilstm_kfold_optimization\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nüîç STARTING OPTUNA HYPERPARAMETER OPTIMIZATION\n================================================================================\n\n‚öô  Configuration:\n   Trials: 50\n   Timeout: 6.0 hours\n   K-Folds: 5\n   Epochs per trial: 200 (with patience=30)\n   Pruning: Enabled (MedianPruner)\n\nüöÄ Starting optimization...\n   This will take approximately 5-8 hours\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1c2dcf81d7e4ddaab54c4aa9cd5ca8b"}},"metadata":{}},{"name":"stdout","text":"[I 2025-11-17 00:51:31,005] Trial 0 finished with value: 0.07209058292124977 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.00021175049189129955, 'batch_size': 128, 'l1_lambda': 0.01, 'l2_lambda': 0}. Best is trial 0 with value: 0.07209058292124977.\n[I 2025-11-17 00:55:03,768] Trial 1 finished with value: 0.05114730463002202 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0002882158121158541, 'batch_size': 32, 'l1_lambda': 0.001, 'l2_lambda': 0}. Best is trial 0 with value: 0.07209058292124977.\n[I 2025-11-17 00:56:05,870] Trial 2 finished with value: 0.05114730463002202 and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.006025500418073251, 'batch_size': 64, 'l1_lambda': 0.001, 'l2_lambda': 0.001}. Best is trial 0 with value: 0.07209058292124977.\n[I 2025-11-17 01:00:21,811] Trial 3 finished with value: 0.5055405845835693 and parameters: {'hidden_size': 512, 'num_layers': 3, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0004100342783734655, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:01:59,422] Trial 4 finished with value: 0.060981252505549935 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.0005042796445090898, 'batch_size': 32, 'l1_lambda': 0.01, 'l2_lambda': 1e-05}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:02:55,949] Trial 5 finished with value: 0.05114730463002202 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.009572655484380497, 'batch_size': 128, 'l1_lambda': 0.001, 'l2_lambda': 1e-05}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:04:50,853] Trial 6 finished with value: 0.05114730463002202 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.004132553658960039, 'batch_size': 32, 'l1_lambda': 0.01, 'l2_lambda': 0}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:07:22,585] Trial 7 finished with value: 0.4955780426774402 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0016593744426762857, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:11:23,955] Trial 8 finished with value: 0.05114730463002202 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.0033591935607502807, 'batch_size': 32, 'l1_lambda': 0.001, 'l2_lambda': 0.001}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:12:57,688] Trial 9 finished with value: 0.2983242789416905 and parameters: {'hidden_size': 64, 'num_layers': 3, 'dropout_rate': 0.5, 'learning_rate': 0.006072928190904094, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:17:11,830] Trial 10 finished with value: 0.3739478572141703 and parameters: {'hidden_size': 512, 'num_layers': 3, 'dropout_rate': 0.2, 'learning_rate': 0.0007629534333853568, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:18:48,017] Trial 11 finished with value: 0.4507452217514515 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.0016873662498878533, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:22:56,102] Trial 12 finished with value: 0.19906843673617755 and parameters: {'hidden_size': 512, 'num_layers': 3, 'dropout_rate': 0.2, 'learning_rate': 0.0016129391603543386, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:24:32,141] Trial 13 finished with value: 0.39180153736231255 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.00014674927401074053, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:28:06,735] Trial 14 finished with value: 0.4870722322763671 and parameters: {'hidden_size': 512, 'num_layers': 3, 'dropout_rate': 0.5, 'learning_rate': 0.00040791469862831514, 'batch_size': 128, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 3 with value: 0.5055405845835693.\n[I 2025-11-17 01:30:40,513] Trial 15 finished with value: 0.6298106123456901 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.001200098352646134, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:33:28,718] Trial 16 finished with value: 0.5053792558145344 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.0007389258905367393, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:35:09,388] Trial 17 finished with value: 0.4055278918937798 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0001152687767728283, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:36:44,590] Trial 18 finished with value: 0.40011687299170456 and parameters: {'hidden_size': 512, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.0011095040020473053, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:38:02,698] Trial 19 finished with value: 0.15687260206982723 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.00046743404943981387, 'batch_size': 128, 'l1_lambda': 0.01, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:43:43,834] Trial 20 finished with value: 0.1605789539169166 and parameters: {'hidden_size': 512, 'num_layers': 3, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0024410222190099024, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 1e-05}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:46:33,160] Trial 21 finished with value: 0.4528640661396559 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.0007863307068962896, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:49:21,133] Trial 22 finished with value: 0.4023727347057637 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.0010188046381617718, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:52:08,790] Trial 23 finished with value: 0.4804853713053527 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0006442150533753624, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:54:56,783] Trial 24 finished with value: 0.5602590029270987 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.00031463790501020484, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:56:09,868] Trial 25 finished with value: 0.29484464632563834 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.00027555659405635987, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:57:24,409] Trial 26 finished with value: 0.4926022309137603 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout_rate': 0.2, 'learning_rate': 0.0003305230019287367, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 01:58:28,124] Trial 27 finished with value: 0.24387048986293816 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.00017492930121016588, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:00:56,349] Trial 28 finished with value: 0.13361137142021573 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.00010154031073291395, 'batch_size': 128, 'l1_lambda': 0.01, 'l2_lambda': 0.001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:02:20,384] Trial 29 finished with value: 0.05114730463002202 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.4, 'learning_rate': 0.00022789602168021473, 'batch_size': 32, 'l1_lambda': 0.001, 'l2_lambda': 0.001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:03:57,600] Trial 30 finished with value: 0.055010598369564456 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout_rate': 0.2, 'learning_rate': 0.0012713506308368288, 'batch_size': 128, 'l1_lambda': 0.01, 'l2_lambda': 1e-05}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:06:45,154] Trial 31 finished with value: 0.45182465821516277 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.0006702883749822627, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:09:33,125] Trial 32 finished with value: 0.5624347501521059 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.00033128537226033243, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:12:20,825] Trial 33 finished with value: 0.4997814815367521 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0003720907984914978, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:15:08,769] Trial 34 finished with value: 0.05114730463002202 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.0002213225325911459, 'batch_size': 64, 'l1_lambda': 0.001, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:16:12,343] Trial 35 finished with value: 0.3879919100123834 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0005112601355327287, 'batch_size': 64, 'l1_lambda': 0, 'l2_lambda': 0.001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:18:18,432] Trial 36 finished with value: 0.4935483595865408 and parameters: {'hidden_size': 512, 'num_layers': 1, 'dropout_rate': 0.2, 'learning_rate': 0.00029484199205649696, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 1e-05}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:20:12,737] Trial 37 finished with value: 0.05114730463002202 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0005360031977498855, 'batch_size': 32, 'l1_lambda': 0.001, 'l2_lambda': 0.0001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:21:23,482] Trial 38 finished with value: 0.12557066061528854 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout_rate': 0.4, 'learning_rate': 0.00016263078690768355, 'batch_size': 64, 'l1_lambda': 0.01, 'l2_lambda': 0.001}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:25:16,928] Trial 39 finished with value: 0.5910634852657755 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.00025356268121969256, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:26:41,601] Trial 40 finished with value: 0.4957226731442058 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.0002542658729123741, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:30:34,258] Trial 41 finished with value: 0.5854769513191936 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.00018581688486987377, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:34:28,255] Trial 42 finished with value: 0.5004750657843695 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.00018415929249435974, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:38:24,688] Trial 43 finished with value: 0.6073960401961751 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.00011982892099901615, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:42:21,773] Trial 44 finished with value: 0.556814537044856 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.00012684480395512565, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:44:58,164] Trial 45 finished with value: 0.5189901850014824 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.00014253511657569775, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:48:55,030] Trial 46 finished with value: 0.05114730463002202 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.00020127806060463312, 'batch_size': 32, 'l1_lambda': 0.001, 'l2_lambda': 0}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:52:46,512] Trial 47 finished with value: 0.37092858955551533 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.002055394962513407, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:55:21,884] Trial 48 finished with value: 0.35145556317484616 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.5, 'learning_rate': 0.0001236641768697526, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 15 with value: 0.6298106123456901.\n[I 2025-11-17 02:59:14,646] Trial 49 finished with value: 0.5551551110066537 and parameters: {'hidden_size': 512, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.00010388878609480335, 'batch_size': 32, 'l1_lambda': 0, 'l2_lambda': 0}. Best is trial 15 with value: 0.6298106123456901.\n\n================================================================================\n‚úÖ OPTUNA OPTIMIZATION COMPLETE!\n================================================================================\n\nüèÜ Best Trial:\n   Trial number: 15\n   Best F1 score: 0.6298\n\nüéØ Best Hyperparameters:\n   hidden_size: 256\n   num_layers: 2\n   dropout_rate: 0.30000000000000004\n   learning_rate: 0.001200098352646134\n   batch_size: 32\n   l1_lambda: 0\n   l2_lambda: 0.0001\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/4245612651.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Save study\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'optuna_study_bilstm.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüíæ Study saved to 'optuna_study_bilstm.pkl'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'optuna_study_bilstm.pkl'"],"ename":"OSError","evalue":"[Errno 30] Read-only file system: 'optuna_study_bilstm.pkl'","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 11: OPTUNA RESULTS & VISUALIZATION\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä OPTUNA RESULTS ANALYSIS\")\nprint(\"=\"*80)\n\n# Trials DataFrame\ntrials_df = study.trials_dataframe()\ntrials_df = trials_df.sort_values('value', ascending=False)\n\nprint(f\"\\nüèÜ Top 10 Trials:\")\nprint(trials_df[['number', 'value', 'params_hidden_size', 'params_num_layers',\n                 'params_dropout_rate', 'params_learning_rate', 'params_batch_size']].head(10))\n\n# Save results\ntrials_df.to_csv('optuna_results_bilstm.csv', index=False)\nprint(f\"\\nüíæ Results saved to 'optuna_results_bilstm.csv'\")\n\n# Visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# 1. Optimization history\nax = axes[0, 0]\nax.plot([t.value for t in study.trials if t.value is not None], marker='o', alpha=0.6)\nax.set_xlabel('Trial')\nax.set_ylabel('F1 Score')\nax.set_title('Optimization History')\nax.grid(alpha=0.3)\n\n# 2. Hyperparameter importance\ntry:\n    importance = optuna.importance.get_param_importances(study)\n    ax = axes[0, 1]\n    params = list(importance.keys())\n    values = list(importance.values())\n    ax.barh(params, values)\n    ax.set_xlabel('Importance')\n    ax.set_title('Hyperparameter Importance')\n    ax.grid(alpha=0.3)\nexcept:\n    axes[0, 1].text(0.5, 0.5, 'Not enough trials\\nfor importance analysis',\n                    ha='center', va='center')\n\n# 3. Learning rate vs F1\nax = axes[1, 0]\nlrs = [t.params['learning_rate'] for t in study.trials if t.value is not None]\nf1s = [t.value for t in study.trials if t.value is not None]\nax.scatter(lrs, f1s, alpha=0.6)\nax.set_xscale('log')\nax.set_xlabel('Learning Rate')\nax.set_ylabel('F1 Score')\nax.set_title('Learning Rate vs F1')\nax.grid(alpha=0.3)\n\n# 4. Hidden size vs F1\nax = axes[1, 1]\nhidden_sizes = [t.params['hidden_size'] for t in study.trials if t.value is not None]\nax.scatter(hidden_sizes, f1s, alpha=0.6)\nax.set_xlabel('Hidden Size')\nax.set_ylabel('F1 Score')\nax.set_title('Hidden Size vs F1')\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('optuna_analysis_bilstm.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n‚úÖ Visualizations saved to 'optuna_analysis_bilstm.png'\")\n","metadata":{"id":"ridmK9_0-CAw","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:45:09.181628Z","iopub.execute_input":"2025-11-17T05:45:09.182405Z","iopub.status.idle":"2025-11-17T05:45:09.256763Z","shell.execute_reply.started":"2025-11-17T05:45:09.182379Z","shell.execute_reply":"2025-11-17T05:45:09.255914Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüìä OPTUNA RESULTS ANALYSIS\n================================================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/1395658108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Trials DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrials_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtrials_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrials_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"],"ename":"NameError","evalue":"name 'study' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 12: FINAL TRAINING WITH BEST HYPERPARAMETERS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéØ FINAL TRAINING WITH BEST HYPERPARAMETERS\")\nprint(\"=\"*80)\n\n# Extract best hyperparameters\nbest_params = best_trial.params\nprint(f\"\\n‚öô  Using hyperparameters:\")\nfor key, value in best_params.items():\n    print(f\"   {key}: {value}\")\n\n# Create datasets\ntrain_ds = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\nval_ds = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n\n# Weighted sampling\ntrain_class_counts = np.bincount(y_train.astype(int))\nclass_weights_sampling = 1.0 / train_class_counts\nclass_weights_sampling = class_weights_sampling / np.sum(class_weights_sampling)\nsample_weights = class_weights_sampling[y_train.astype(int)]\nsample_weights = torch.from_numpy(sample_weights).float()\n\nsampler = WeightedRandomSampler(\n    weights=sample_weights,\n    num_samples=len(sample_weights),\n    replacement=True\n)\n\n# Create data loaders\ntrain_loader = make_loader(train_ds, batch_size=best_params['batch_size'], shuffle=False, drop_last=True, sampler=sampler)\nval_loader = make_loader(val_ds, batch_size=best_params['batch_size'], shuffle=False, drop_last=False)\n\n# Create best model\nbest_model = BiLSTM(\n    input_size=input_shape[-1],\n    num_classes=num_classes,\n    hidden_size=best_params['hidden_size'],\n    num_layers=best_params['num_layers'],\n    dropout_rate=best_params['dropout_rate']\n).to(device)\n\n# Loss & Optimizer\nclass_weights_loss = len(y_train) / (len(train_class_counts) * train_class_counts)\nclass_weights_loss = torch.tensor(class_weights_loss, dtype=torch.float32).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights_loss)\n\noptimizer = torch.optim.AdamW(\n    best_model.parameters(),\n    lr=best_params['learning_rate'],\n    weight_decay=best_params['l2_lambda']\n)\n\nscaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n\nprint(f\"\\nüöÄ Starting final training (500 epochs, patience=50)...\")\nprint(\"=\"*80)\n\n# Train\n_, history = fit(\n    model=best_model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    epochs=500,\n    criterion=criterion,\n    optimizer=optimizer,\n    scaler=scaler,\n    device=device,\n    patience=50,\n    l1_lambda=best_params['l1_lambda'],\n    verbose=10\n)\n\n# Save model\ntorch.save(best_model.state_dict(), 'best_bilstm_model.pt')\nprint(f\"\\nüíæ Model saved to 'best_bilstm_model.pt'\")\n\n# Plot history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nax1.plot(history['train_loss'], label='Training', alpha=0.3, linestyle='--')\nax1.plot(history['val_loss'], label='Validation', alpha=0.9)\nax1.set_title('Loss')\nax1.legend()\nax1.grid(alpha=0.3)\n\nax2.plot(history['train_f1'], label='Training', alpha=0.3, linestyle='--')\nax2.plot(history['val_f1'], label='Validation', alpha=0.9)\nax2.set_title('F1 Score')\nax2.legend()\nax2.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('final_training_history_bilstm.png', dpi=150)\nplt.show()\n\nprint(f\"\\n‚úÖ Final training complete!\")\nprint(f\"   Best Val F1: {max(history['val_f1']):.4f}\")\nprint(f\"   Final Val F1: {history['val_f1'][-1]:.4f}\")\n","metadata":{"id":"pDrdCqmZ-IrN","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T02:59:14.678989Z","iopub.status.idle":"2025-11-17T02:59:14.679201Z","shell.execute_reply.started":"2025-11-17T02:59:14.679102Z","shell.execute_reply":"2025-11-17T02:59:14.679112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 13: CONFUSION MATRIX & CLASSIFICATION REPORT\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìà VALIDATION SET EVALUATION\")\nprint(\"=\"*80)\n\n# Get predictions\nbest_model.eval()\nval_preds = []\nval_true = []\n\nwith torch.no_grad():\n    for inputs, targets in val_loader:\n        inputs = inputs.to(device)\n        outputs = best_model(inputs)\n        preds = outputs.argmax(dim=1)\n        val_preds.extend(preds.cpu().numpy())\n        val_true.extend(targets.cpu().numpy())\n\n# Classification report\nprint(\"\\nüìä Classification Report:\")\nprint(\"=\"*80)\nprint(classification_report(\n    val_true, val_preds,\n    target_names=['no_pain', 'low_pain', 'high_pain'],\n    digits=4\n))\n\n# Confusion matrix\ncm = confusion_matrix(val_true, val_preds)\nclass_labels = ['no_pain', 'low_pain', 'high_pain']\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix - Best BiLSTM Model')\nplt.savefig('confusion_matrix_bilstm.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"‚úÖ Confusion matrix saved to 'confusion_matrix_bilstm.png'\")\n","metadata":{"id":"DuT9AUr3-LRF","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T02:59:14.680370Z","iopub.status.idle":"2025-11-17T02:59:14.680644Z","shell.execute_reply.started":"2025-11-17T02:59:14.680528Z","shell.execute_reply":"2025-11-17T02:59:14.680541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 14: INFERENCE & SUBMISSION\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üîÆ INFERENCE ON TEST SET\")\nprint(\"=\"*80)\n\n# Build test windows\nX_test, _, _ = build_windows(df_test, None, WINDOW_SIZE, STRIDE, feature=\"3d\")\ntest_loader = make_loader(\n    TensorDataset(torch.from_numpy(X_test).float()),\n    batch_size=32,\n    shuffle=False,\n    drop_last=False\n)\n\n# Generate predictions for all windows\nall_window_preds = []\nbest_model.eval()\n\nwith torch.no_grad():\n    for xb in test_loader:\n        xb = xb[0].to(device)\n        outputs = best_model(xb)\n        preds = outputs.argmax(dim=1)\n        all_window_preds.extend(preds.cpu().numpy())\n\nprint(f\"\\nüìä Generated {len(all_window_preds)} window predictions\")\n\n# Aggregate predictions per pirate (majority voting)\nnum_test_samples = len(df_test['sample_index'].unique())\nwindows_per_sample = len(all_window_preds) // num_test_samples\n\nprint(f\"   Test samples: {num_test_samples}\")\nprint(f\"   Windows per sample: {windows_per_sample}\")\n\nlabel_mapping = {0: 'no_pain', 1: 'low_pain', 2: 'high_pain'}\nfinal_predictions = []\n\nfor sample_idx in range(num_test_samples):\n    start_idx = sample_idx * windows_per_sample\n    end_idx = start_idx + windows_per_sample\n    window_preds = all_window_preds[start_idx:end_idx]\n\n    # Majority voting\n    most_common = Counter(window_preds).most_common(1)[0][0]\n    final_predictions.append(label_mapping[most_common])\n\nprint(f\"\\n‚úÖ Aggregated to {len(final_predictions)} final predictions (one per pirate)\")\n\n# Create submission CSV\npredictions_df = pd.DataFrame({\n    'sample_index': np.arange(num_test_samples),\n    'label': final_predictions\n})\n\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\nfilename = f'predictions_bilstm_optuna_{timestamp}.csv'\npredictions_df.to_csv(filename, index=False)\n\nprint(f\"\\nüíæ Predictions saved to: {filename}\")\nprint(f\"   Total predictions: {len(final_predictions)}\")\nprint(f\"\\nüìä Distribution:\")\nfor label in ['no_pain', 'low_pain', 'high_pain']:\n    count = final_predictions.count(label)\n    pct = (count / len(final_predictions)) * 100\n    print(f\"   {label:10s}: {count:5d} ({pct:5.2f}%)\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ ALL DONE! üéâ\")\nprint(\"=\"*80)\nprint(\"\\nGenerated files:\")\nprint(\"  üìÑ optuna_study_bilstm.pkl\")\nprint(\"  üìÑ optuna_results_bilstm.csv\")\nprint(\"  üìÑ best_bilstm_model.pt\")\nprint(\"  üìä optuna_analysis_bilstm.png\")\nprint(\"  üìä final_training_history_bilstm.png\")\nprint(\"  üìä confusion_matrix_bilstm.png\")\nprint(f\"  üìÑ {filename}\")","metadata":{"id":"5AUuoLuR89wx","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T02:59:14.681649Z","iopub.status.idle":"2025-11-17T02:59:14.681947Z","shell.execute_reply.started":"2025-11-17T02:59:14.681825Z","shell.execute_reply":"2025-11-17T02:59:14.681838Z"}},"outputs":[],"execution_count":null}]}