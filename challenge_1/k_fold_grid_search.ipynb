{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c93e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1\n",
      "  Training model for current fold...\n",
      "Training 500 epochs...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Generate fold indices\n",
    "kf_splits = list(kf.split(unique_users))\n",
    "\n",
    "fold_results = [] # List to store results for each fold\n",
    "\n",
    "fold = 0\n",
    "for train_users_fold, val_users_fold in kf_splits:\n",
    "    print(f\"Processing Fold {fold + 1}\")\n",
    "\n",
    "    # Get the actual user IDs for this fold\n",
    "    train_user_ids = unique_users[train_users_fold]\n",
    "    val_user_ids = unique_users[val_users_fold]\n",
    "\n",
    "    # Create training and validation dataframes for the current fold\n",
    "    df_fold_train = df[df['sample_index'].isin(train_user_ids)]\n",
    "    df_fold_val = df[df['sample_index'].isin(val_users_fold)]\n",
    "\n",
    "    # Build sequences for each fold\n",
    "    X_train_fold, y_train_fold = build_sequences(df_fold_train, WINDOW_SIZE, STRIDE)\n",
    "    X_val_fold, y_val_fold = build_sequences(df_fold_val, WINDOW_SIZE, STRIDE)\n",
    "\n",
    "    # Flatten the sequences for SMOTE\n",
    "    X_train_fold_flat = X_train_fold.reshape(X_train_fold.shape[0], -1)\n",
    "\n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=SEED)\n",
    "    X_train_fold_resampled, y_train_fold_resampled = smote.fit_resample(X_train_fold_flat, y_train_fold)\n",
    "\n",
    "    # Reshape the resampled data back to sequences\n",
    "    X_train_fold_resampled = X_train_fold_resampled.reshape(X_train_fold_resampled.shape[0], WINDOW_SIZE, X_train_fold.shape[2])\n",
    "\n",
    "    # Convert numpy arrays to PyTorch datasets\n",
    "    train_ds_fold = TensorDataset(torch.from_numpy(X_train_fold_resampled), torch.from_numpy(y_train_fold_resampled))\n",
    "    val_ds_fold   = TensorDataset(torch.from_numpy(X_val_fold), torch.from_numpy(y_val_fold))\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader_fold = make_loader(train_ds_fold, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "    val_loader_fold   = make_loader(val_ds_fold, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "    input_shape = X_train_fold_resampled.shape\n",
    "    num_classes = len(np.unique(y_train_fold_resampled))\n",
    "\n",
    "    # Initialize a new model for each fold\n",
    "    fold_model = RecurrentClassifier(\n",
    "        input_size=input_shape[-1],\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=0.1,\n",
    "        bidirectional=False,\n",
    "        rnn_type='GRU'\n",
    "    ).to(device)\n",
    "\n",
    "    # Set up optimizer and scaler for the current fold\n",
    "    fold_optimizer = torch.optim.AdamW(fold_model.parameters(), lr=1e-3, weight_decay=0)\n",
    "    fold_scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "    fold_criterion = nn.CrossEntropyLoss(weight=WEIGHTS) # Use the defined criterion\n",
    "\n",
    "    # Train model for the current fold\n",
    "    print(\"  Training model for current fold...\")\n",
    "    trained_fold_model, fold_training_history = fit(\n",
    "        model=fold_model,\n",
    "        train_loader=train_loader_fold,\n",
    "        val_loader=val_loader_fold,\n",
    "        epochs=EPOCHS,\n",
    "        criterion=fold_criterion,\n",
    "        optimizer=fold_optimizer,\n",
    "        scaler=fold_scaler,\n",
    "        device=device,\n",
    "        writer=None, # Disable TensorBoard logging for individual folds to avoid clutter\n",
    "        verbose=50, # Print less frequently during fold training\n",
    "        experiment_name=f\"fold_{fold+1}_rnn\", # Save models with fold number\n",
    "        patience=PATIENCE\n",
    "    )\n",
    "\n",
    "    # Store the training history for this fold\n",
    "    fold_results.append(fold_training_history)\n",
    "\n",
    "    print(f\"  Finished training for Fold {fold + 1}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# After the loop, you can aggregate the results from fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a03292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss across folds: 0.7079\n",
      "Average Validation F1 Score across folds: 0.5624\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Aggregate results from all folds\n",
    "all_val_loss = [history['val_loss'][-1] for history in fold_results]\n",
    "all_val_f1 = [history['val_f1'][-1] for history in fold_results]\n",
    "\n",
    "average_val_loss = np.mean(all_val_loss)\n",
    "average_val_f1 = np.mean(all_val_f1)\n",
    "\n",
    "print(f\"Average Validation Loss across folds: {average_val_loss:.4f}\")\n",
    "print(f\"Average Validation F1 Score across folds: {average_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv(train_loader, val_loader, input_size, num_classes, device):\n",
    "    learning_rates = [1e-3, 1e-4]\n",
    "    hidden_sizes = [128, 256]\n",
    "    hidden_layers = [1, 2]\n",
    "    dropout_rates = [0.1, 0.2]\n",
    "    l1_lambdas = [0, 1e-4]\n",
    "    l2_lambdas = [0, 1e-4]\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_params = {}\n",
    "\n",
    "    # Set up loss function\n",
    "    criterion = nn.CrossEntropyLoss(weight=WEIGHTS)\n",
    "\n",
    "    total_combinations = (\n",
    "      len(learning_rates) * len(hidden_sizes) * len(hidden_layers) *\n",
    "      len(dropout_rates) * len(l2_lambdas) * len(l1_lambdas)\n",
    "    )\n",
    "    print(f\"Starting Grid Search with {total_combinations} combinations...\")\n",
    "    i = 0\n",
    "    for lr in learning_rates:\n",
    "        for hs in hidden_sizes:\n",
    "            for hl in hidden_layers:\n",
    "                for dr in dropout_rates:\n",
    "                  for l1 in l1_lambdas:\n",
    "                    for l2 in l2_lambdas:\n",
    "                      print(f\"\\nCombination no.{i+1}/{total_combinations}\")\n",
    "                      print(f\"\\nTraining with params: LR={lr}, HS={hs}, HL={hl}, DR={dr}, L1={l1}, L2={l2}\")\n",
    "\n",
    "                      # Initialize a new model for each combination\n",
    "                      model = RecurrentClassifier(\n",
    "                          input_size=input_size,\n",
    "                          hidden_size=hs,\n",
    "                          num_layers=hl,\n",
    "                          num_classes=num_classes,\n",
    "                          dropout_rate=dr,\n",
    "                          bidirectional=False, # Keep consistent for now\n",
    "                          rnn_type='GRU'       # Keep consistent for now\n",
    "                      ).to(device)\n",
    "\n",
    "                      # Set up optimizer and scaler for the current fold\n",
    "                      optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=l2)\n",
    "                      scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "                      # Train model\n",
    "                      # Using a reduced number of epochs and patience for grid search speed\n",
    "                      trained_model, training_history = fit(\n",
    "                          model=model,\n",
    "                          train_loader=train_loader,\n",
    "                          val_loader=val_loader,\n",
    "                          epochs=150,\n",
    "                          criterion=criterion,\n",
    "                          optimizer=optimizer,\n",
    "                          scaler=scaler,\n",
    "                          device=device,\n",
    "                          writer=None,\n",
    "                          verbose=10,\n",
    "                          experiment_name=f\"grid_search_lr{lr}_hs{hs}_hl_{hl}_dr{dr}_l1{l1}_l2{l2}\",\n",
    "                          patience=40,\n",
    "                          l1_lambda=l1,\n",
    "                          l2_lambda=l2\n",
    "                      )\n",
    "\n",
    "                      # Evaluate on validation set\n",
    "                      val_loss, val_f1 = validate_one_epoch(\n",
    "                          trained_model, val_loader, criterion, device\n",
    "                      )\n",
    "\n",
    "                      print(f\"  Validation F1 Score: {val_f1:.4f}\")\n",
    "                      i += 1\n",
    "                      # Check if current model is the best\n",
    "                      if val_f1 > best_f1:\n",
    "                          best_f1 = val_f1\n",
    "                          best_params = {\n",
    "                            'learning_rate': lr,\n",
    "                            'hidden_size': hs,\n",
    "                            'num_layers': hl,\n",
    "                            'dropout_rate': dr,\n",
    "                            'l1_lambda': l1,\n",
    "                            'l2_lambda': l2\n",
    "                          }\n",
    "                          print(f\"  New best F1 found: {best_f1:.4f} with params {best_params}\")\n",
    "\n",
    "    print(\"\\nGrid Search Finished.\")\n",
    "    print(f\"Best Validation F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# Assuming train_loader, val_loader, input_shape, num_classes, and device are defined\n",
    "# Call the grid search function\n",
    "best_hyperparameters = grid_search_cv(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    input_shape[-1], # Input size is the last dimension of input_shape\n",
    "    num_classes,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"\\nBest hyperparameters found by grid search:\")\n",
    "print(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca3c4af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'two'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[60], line 12\u001b[0m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Apply the SAME categorical encoding learned from training data\u001b[39;00m\n",
      "\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m number_cols:\n",
      "\u001b[0;32m---> 12\u001b[0m     test_df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Ensure float32\u001b[39;00m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Categorical features encoded using training data mappings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# List of joint columns to normalize\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/tempo_env/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:129\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n",
      "\u001b[1;32m    127\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;32m    128\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y)\n",
      "\u001b[0;32m--> 129\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# transform of empty array is empty array\u001b[39;00m\n",
      "\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\n",
      "File \u001b[0;32m~/tempo_env/lib/python3.12/site-packages/sklearn/utils/validation.py:1456\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn, device)\u001b[0m\n",
      "\u001b[1;32m   1418\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ravel column or 1d numpy array, else raises an error.\u001b[39;00m\n",
      "\u001b[1;32m   1419\u001b[0m \n",
      "\u001b[1;32m   1420\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1453\u001b[0m \u001b[38;5;124;03marray([1, 1])\u001b[39;00m\n",
      "\u001b[1;32m   1454\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m   1455\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y)\n",
      "\u001b[0;32m-> 1456\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1465\u001b[0m shape \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\n",
      "File \u001b[0;32m~/tempo_env/lib/python3.12/site-packages/sklearn/utils/validation.py:1053\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n",
      "\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(array\u001b[38;5;241m.\u001b[39mdtype, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex floating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "\u001b[1;32m   1046\u001b[0m         _assert_all_finite(\n",
      "\u001b[1;32m   1047\u001b[0m             array,\n",
      "\u001b[1;32m   1048\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1051\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n",
      "\u001b[1;32m   1052\u001b[0m         )\n",
      "\u001b[0;32m-> 1053\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   1055\u001b[0m     array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "\n",
      "File \u001b[0;32m~/tempo_env/lib/python3.12/site-packages/sklearn/utils/_array_api.py:399\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.astype\u001b[0;34m(self, x, dtype, copy, casting)\u001b[0m\n",
      "\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mastype\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, dtype, \u001b[38;5;241m*\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# astype is not defined in the top level NumPy namespace\u001b[39;00m\n",
      "\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'two'"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "test_df = pd.read_csv(\"pirate_pain_test.csv\")\n",
    "\n",
    "# üîß FIX: Load the label encoders fitted on TRAINING data\n",
    "import pickle\n",
    "\n",
    "with open('label_encoders.pkl', 'rb') as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "\n",
    "# Apply the SAME categorical encoding learned from training data\n",
    "for col in number_cols:\n",
    "    test_df[col] = label_encoders[col].transform(test_df[col]).astype(np.float32)  # Ensure float32\n",
    "\n",
    "print(\"‚úÖ Categorical features encoded using training data mappings\")\n",
    "\n",
    "# List of joint columns to normalize\n",
    "joint_cols = [\"joint_\" + str(i).zfill(2) for i in range(31)]\n",
    "\n",
    "for col in joint_cols:\n",
    "  test_df[col] = test_df[col].astype(np.float32)\n",
    "\n",
    "# üîß FIX: Load the scaler fitted on TRAINING data\n",
    "# DO NOT call fit_transform() on test data - this causes data leakage!\n",
    "import pickle\n",
    "\n",
    "with open('minmax_scaler.pkl', 'rb') as f:\n",
    "    minmax_scaler = pickle.load(f)\n",
    "\n",
    "# Apply the SAME normalization learned from training data\n",
    "test_df[joint_cols] = minmax_scaler.transform(test_df[joint_cols])  # ‚úÖ transform only!\n",
    "\n",
    "print(\"‚úÖ Test data normalized using training data statistics\")\n",
    "print(f\"Using scaler with Min: {minmax_scaler.data_min_[:5]}\")\n",
    "print(f\"Using scaler with Max: {minmax_scaler.data_max_[:5]}\")\n",
    "\n",
    "X_test = build_test_sequences(test_df)\n",
    "\n",
    "# üîß FIX: Ensure the data type is float32 (not float64)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "test_ds = TensorDataset(torch.from_numpy(X_test))\n",
    "test_loader = make_loader(test_ds, BATCH_SIZE, True, False)\n",
    "\n",
    "print(f\"‚úÖ Test sequences created with shape: {X_test.shape}, dtype: {X_test.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Verify data types are correct\n",
    "print(\"Data type verification:\")\n",
    "print(f\"test_df categorical columns dtype: {test_df[number_cols].dtypes.unique()}\")\n",
    "print(f\"test_df joint columns dtype: {test_df[joint_cols].dtypes.unique()}\")\n",
    "print(f\"X_test dtype: {X_test.dtype}\")\n",
    "\n",
    "# Check a sample tensor from the test loader\n",
    "sample_batch = next(iter(test_loader))\n",
    "print(f\"Test loader batch dtype: {sample_batch[0].dtype}\")\n",
    "print(\"\\n‚úÖ All dtypes should be float32 for model compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for xb in test_loader:\n",
    "        xb = xb[0].to(device)\n",
    "        outputs = best_model(xb)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#Re-map prediction indexes to the labels\n",
    "predicted_pains = []\n",
    "for pred in predictions:\n",
    "  if pred == 0:\n",
    "    predicted_pains.append(\"no_pain\")\n",
    "  elif pred == 1:\n",
    "    predicted_pains.append(\"low_pain\")\n",
    "  else:\n",
    "    predicted_pains.append(\"high_pain\")\n",
    "\n",
    "predictions_csv = pd.DataFrame({'sample_index': np.arange(len(test_ds)), 'label': predicted_pains})\n",
    "today_date = 'predictions_' + datetime.now().strftime(\"%Y%m%d_%H%M\") + '.csv'\n",
    "predictions_csv.to_csv(today_date, index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
