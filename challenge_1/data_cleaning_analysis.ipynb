{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ea845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90246fe",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data (before preprocessing)\n",
    "df_train = pd.read_csv('pirate_pain_train.csv')\n",
    "df_labels = pd.read_csv('pirate_pain_train_labels.csv')\n",
    "df_test = pd.read_csv('pirate_pain_test.csv')\n",
    "\n",
    "# Merge labels for analysis\n",
    "df_train = pd.merge(df_train, df_labels, on='sample_index', how='left')\n",
    "\n",
    "# Define joint columns (excluding joint_30 and joint_02 already removed)\n",
    "all_joints = [f'joint_{str(i).zfill(2)}' for i in range(31)]\n",
    "remaining_joints = [j for j in all_joints if j not in ['joint_30', 'joint_02']]\n",
    "\n",
    "print(f\"Total samples (train): {len(df_train)}\")\n",
    "print(f\"Unique pirates (train): {df_train['sample_index'].nunique()}\")\n",
    "print(f\"Unique pirates (test): {df_test['sample_index'].nunique()}\")\n",
    "print(f\"\\nJoint columns analyzed: {len(remaining_joints)}\")\n",
    "print(f\"Excluded: joint_30 (zero variance), joint_02 (high correlation)\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b0d83",
   "metadata": {},
   "source": [
    "## 2. Outlier Detection\n",
    "\n",
    "**Yoda's question:** \"That one row, so different from all others... an error, or a rare jewel is it?\"\n",
    "\n",
    "We'll use multiple methods to detect outliers:\n",
    "1. **Z-score method**: Statistical outliers (>3 standard deviations)\n",
    "2. **IQR method**: Values outside Q1-1.5√óIQR to Q3+1.5√óIQR\n",
    "3. **Isolation Forest**: ML-based anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de6e96d",
   "metadata": {},
   "source": [
    "### 2.1 Z-Score Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Z-scores for all joint columns\n",
    "z_scores = np.abs(stats.zscore(df_train[remaining_joints], nan_policy='omit'))\n",
    "\n",
    "# Find rows with any Z-score > 3 (extreme outliers)\n",
    "outlier_threshold = 3\n",
    "outlier_mask = (z_scores > outlier_threshold).any(axis=1)\n",
    "outlier_rows = df_train[outlier_mask]\n",
    "\n",
    "print(f\"Z-Score Outlier Detection (threshold={outlier_threshold}):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total outlier rows: {len(outlier_rows)} / {len(df_train)} ({len(outlier_rows)/len(df_train)*100:.2f}%)\")\n",
    "print(f\"Outlier pirates: {outlier_rows['sample_index'].nunique()}\")\n",
    "\n",
    "# Check distribution of outliers by pain label\n",
    "print(\"\\nOutlier distribution by pain label:\")\n",
    "outlier_label_dist = outlier_rows['label'].value_counts()\n",
    "for label, count in outlier_label_dist.items():\n",
    "    total_label = len(df_train[df_train['label'] == label])\n",
    "    pct = (count / total_label) * 100\n",
    "    print(f\"  {label:10s}: {count:5d} outliers / {total_label:6d} total ({pct:.2f}%)\")\n",
    "\n",
    "# Find which joints have most outliers\n",
    "outliers_per_joint = (z_scores > outlier_threshold).sum(axis=0)\n",
    "outliers_per_joint = pd.Series(outliers_per_joint, index=remaining_joints).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 joints with most outliers:\")\n",
    "for joint, count in outliers_per_joint.head(10).items():\n",
    "    print(f\"  {joint}: {count} outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers per joint\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Bar plot of outliers per joint\n",
    "axes[0].bar(range(len(outliers_per_joint)), outliers_per_joint.values, color='coral', edgecolor='black')\n",
    "axes[0].set_xticks(range(0, len(outliers_per_joint), 3))\n",
    "axes[0].set_xticklabels([remaining_joints[i] for i in range(0, len(remaining_joints), 3)], rotation=45)\n",
    "axes[0].set_xlabel('Joint Column')\n",
    "axes[0].set_ylabel('Number of Outliers (Z-score > 3)')\n",
    "axes[0].set_title('Outlier Distribution Across Joints')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Distribution of max Z-score per row\n",
    "max_z_scores = z_scores.max(axis=1)\n",
    "axes[1].hist(max_z_scores, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[1].axvline(x=outlier_threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold = {outlier_threshold}')\n",
    "axes[1].set_xlabel('Maximum Z-Score per Row')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Maximum Z-Scores')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Interpretation:\")\n",
    "print(f\"   - If outliers are EVENLY distributed ‚Üí likely measurement noise\")\n",
    "print(f\"   - If outliers are CONCENTRATED in specific joints ‚Üí those joints may be problematic\")\n",
    "print(f\"   - If outliers are CONCENTRATED in specific pain labels ‚Üí they might be informative!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd1cd6",
   "metadata": {},
   "source": [
    "### 2.2 IQR (Interquartile Range) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR-based outliers for each joint\n",
    "Q1 = df_train[remaining_joints].quantile(0.25)\n",
    "Q3 = df_train[remaining_joints].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Find outliers\n",
    "iqr_outliers = ((df_train[remaining_joints] < lower_bound) | (df_train[remaining_joints] > upper_bound)).any(axis=1)\n",
    "iqr_outlier_rows = df_train[iqr_outliers]\n",
    "\n",
    "print(f\"IQR Outlier Detection:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total outlier rows: {len(iqr_outlier_rows)} / {len(df_train)} ({len(iqr_outlier_rows)/len(df_train)*100:.2f}%)\")\n",
    "print(f\"Outlier pirates: {iqr_outlier_rows['sample_index'].nunique()}\")\n",
    "\n",
    "# Comparison with Z-score method\n",
    "both_methods = outlier_mask & iqr_outliers\n",
    "print(f\"\\nOverlap between methods:\")\n",
    "print(f\"  Detected by both Z-score AND IQR: {both_methods.sum()} rows\")\n",
    "print(f\"  Only Z-score: {(outlier_mask & ~iqr_outliers).sum()} rows\")\n",
    "print(f\"  Only IQR: {(~outlier_mask & iqr_outliers).sum()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3306d",
   "metadata": {},
   "source": [
    "### 2.3 Are Outliers \"Errors\" or \"Rare Jewels\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c173e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze if outliers are associated with pain labels\n",
    "print(\"üîç Analyzing if outliers are informative or just noise...\\n\")\n",
    "\n",
    "# Compare outlier rates across pain labels\n",
    "pain_labels = df_train['label'].unique()\n",
    "outlier_rates = {}\n",
    "\n",
    "for label in pain_labels:\n",
    "    label_mask = df_train['label'] == label\n",
    "    outlier_rate = (outlier_mask & label_mask).sum() / label_mask.sum()\n",
    "    outlier_rates[label] = outlier_rate\n",
    "\n",
    "print(\"Outlier rate by pain label (Z-score method):\")\n",
    "print(\"=\" * 70)\n",
    "for label, rate in sorted(outlier_rates.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {label:10s}: {rate*100:.2f}% of samples are outliers\")\n",
    "\n",
    "# Statistical test: Are outlier rates significantly different across labels?\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create contingency table: [label] x [is_outlier]\n",
    "contingency_table = pd.crosstab(df_train['label'], outlier_mask)\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"\\nüìä Chi-square test:\")\n",
    "print(f\"   Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"   P-value: {p_value:.4e}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"   ‚úÖ SIGNIFICANT! Outliers are associated with pain labels ‚Üí They are 'RARE JEWELS' (keep them!)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è NOT SIGNIFICANT. Outliers are randomly distributed ‚Üí They might be 'ERRORS' (consider removing)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üí° Recommendation:\")\n",
    "if p_value < 0.05:\n",
    "    print(\"   Keep outliers - they contain information about pain patterns!\")\n",
    "else:\n",
    "    print(\"   Consider removing extreme outliers - they are likely measurement noise\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b9c18",
   "metadata": {},
   "source": [
    "## 3. Feature Usefulness Analysis\n",
    "\n",
    "**Yoda's wisdom:** \"Not all that you see, is signal.\"\n",
    "\n",
    "Analyze which joints are actually useful for predicting pain:\n",
    "1. **Variance analysis**: Low variance = not useful\n",
    "2. **Correlation with target**: No correlation = not useful\n",
    "3. **ANOVA F-test**: Statistical significance\n",
    "4. **Train/Test distribution mismatch**: Different distributions = problematic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a0ab8a",
   "metadata": {},
   "source": [
    "### 3.1 Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dac662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variance for each joint\n",
    "variances = df_train[remaining_joints].var()\n",
    "variances_sorted = variances.sort_values()\n",
    "\n",
    "print(\"Joint Variance Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nLowest 10 variances (potentially useless features):\")\n",
    "for joint, var in variances_sorted.head(10).items():\n",
    "    print(f\"  {joint}: {var:.6e}\")\n",
    "\n",
    "print(f\"\\nHighest 10 variances (most variable features):\")\n",
    "for joint, var in variances_sorted.tail(10).items():\n",
    "    print(f\"  {joint}: {var:.6e}\")\n",
    "\n",
    "# Define threshold for low variance (you noted joints with e-7 values)\n",
    "low_variance_threshold = 1e-6\n",
    "low_variance_joints = variances[variances < low_variance_threshold].index.tolist()\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Joints with variance < {low_variance_threshold:.0e}:\")\n",
    "if low_variance_joints:\n",
    "    for joint in low_variance_joints:\n",
    "        print(f\"  {joint}: {variances[joint]:.6e}\")\n",
    "else:\n",
    "    print(\"  None found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a005d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize variance distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Bar plot of variances\n",
    "axes[0].bar(range(len(variances_sorted)), variances_sorted.values, color='steelblue', edgecolor='black')\n",
    "axes[0].axhline(y=low_variance_threshold, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Low variance threshold: {low_variance_threshold:.0e}')\n",
    "axes[0].set_xticks(range(0, len(variances_sorted), 3))\n",
    "axes[0].set_xticklabels([variances_sorted.index[i] for i in range(0, len(variances_sorted), 3)], rotation=45)\n",
    "axes[0].set_xlabel('Joint Column (sorted by variance)')\n",
    "axes[0].set_ylabel('Variance')\n",
    "axes[0].set_title('Variance Distribution Across Joints')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_yscale('log')  # Log scale to see small variances\n",
    "\n",
    "# Histogram of log-variances\n",
    "log_variances = np.log10(variances + 1e-12)  # Add small constant to avoid log(0)\n",
    "axes[1].hist(log_variances, bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].axvline(x=np.log10(low_variance_threshold), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Threshold: {low_variance_threshold:.0e}')\n",
    "axes[1].set_xlabel('Log10(Variance)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Log-Variances')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be3b6e",
   "metadata": {},
   "source": [
    "### 3.2 ANOVA F-Test: Which Joints Differ Across Pain Labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1555f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Map labels to integers for analysis\n",
    "label_mapping = {'no_pain': 0, 'low_pain': 1, 'high_pain': 2}\n",
    "df_train['label_int'] = df_train['label'].map(label_mapping)\n",
    "\n",
    "# Perform ANOVA for each joint\n",
    "anova_results = {}\n",
    "\n",
    "for joint in remaining_joints:\n",
    "    # Group data by pain level\n",
    "    groups = [df_train[df_train['label_int'] == label][joint].values \n",
    "              for label in [0, 1, 2]]\n",
    "    \n",
    "    # Perform ANOVA\n",
    "    f_stat, p_value = f_oneway(*groups)\n",
    "    anova_results[joint] = {'f_statistic': f_stat, 'p_value': p_value}\n",
    "\n",
    "# Convert to DataFrame\n",
    "anova_df = pd.DataFrame(anova_results).T\n",
    "anova_df = anova_df.sort_values('p_value')\n",
    "\n",
    "print(\"ANOVA F-Test Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Tests if joint values differ significantly across pain labels\\n\")\n",
    "\n",
    "print(\"Top 10 MOST discriminative joints (lowest p-values):\")\n",
    "for joint, row in anova_df.head(10).iterrows():\n",
    "    sig = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"\"\n",
    "    print(f\"  {joint}: F={row['f_statistic']:.2f}, p={row['p_value']:.4e} {sig}\")\n",
    "\n",
    "print(f\"\\nBottom 10 LEAST discriminative joints (highest p-values):\")\n",
    "for joint, row in anova_df.tail(10).iterrows():\n",
    "    print(f\"  {joint}: F={row['f_statistic']:.2f}, p={row['p_value']:.4e}\")\n",
    "\n",
    "# Count significant joints\n",
    "significant_joints = anova_df[anova_df['p_value'] < 0.05].index.tolist()\n",
    "non_significant_joints = anova_df[anova_df['p_value'] >= 0.05].index.tolist()\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Significant joints (p < 0.05): {len(significant_joints)} / {len(remaining_joints)}\")\n",
    "print(f\"   Non-significant joints: {len(non_significant_joints)}\")\n",
    "\n",
    "if non_significant_joints:\n",
    "    print(f\"\\n‚ö†Ô∏è Non-significant joints (candidates for removal):\")\n",
    "    for joint in non_significant_joints:\n",
    "        print(f\"  {joint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abdc36",
   "metadata": {},
   "source": [
    "### 3.3 Train/Test Distribution Mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Perform Kolmogorov-Smirnov test for each joint\n",
    "# Tests if train and test distributions are different\n",
    "ks_results = {}\n",
    "\n",
    "for joint in remaining_joints:\n",
    "    # Check if joint exists in test set\n",
    "    if joint not in df_test.columns:\n",
    "        continue\n",
    "    \n",
    "    # Perform KS test\n",
    "    ks_stat, p_value = ks_2samp(df_train[joint], df_test[joint])\n",
    "    ks_results[joint] = {'ks_statistic': ks_stat, 'p_value': p_value}\n",
    "\n",
    "# Convert to DataFrame\n",
    "ks_df = pd.DataFrame(ks_results).T\n",
    "ks_df = ks_df.sort_values('p_value')\n",
    "\n",
    "print(\"Kolmogorov-Smirnov Test (Train vs Test Distribution):\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Tests if train and test distributions are significantly different\\n\")\n",
    "\n",
    "print(\"Top 10 joints with MOST DIFFERENT distributions (lowest p-values):\")\n",
    "for joint, row in ks_df.head(10).iterrows():\n",
    "    sig = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"\"\n",
    "    print(f\"  {joint}: KS={row['ks_statistic']:.4f}, p={row['p_value']:.4e} {sig}\")\n",
    "\n",
    "# Identify problematic joints (you mentioned joint_13-17, 19-25)\n",
    "problematic_range_1 = [f'joint_{str(i).zfill(2)}' for i in range(13, 18)]\n",
    "problematic_range_2 = [f'joint_{str(i).zfill(2)}' for i in range(19, 26)]\n",
    "problematic_joints = [j for j in problematic_range_1 + problematic_range_2 if j in remaining_joints]\n",
    "\n",
    "print(f\"\\nüîç Your identified problematic joints (joint_13-17, 19-25):\")\n",
    "for joint in problematic_joints:\n",
    "    if joint in ks_df.index:\n",
    "        row = ks_df.loc[joint]\n",
    "        sig = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"\"\n",
    "        print(f\"  {joint}: KS={row['ks_statistic']:.4f}, p={row['p_value']:.4e} {sig}\")\n",
    "\n",
    "# Count significantly different distributions\n",
    "different_dist_joints = ks_df[ks_df['p_value'] < 0.05].index.tolist()\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Joints with different train/test distributions (p < 0.05): {len(different_dist_joints)}\")\n",
    "print(f\"   These joints may cause poor generalization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579a866",
   "metadata": {},
   "source": [
    "## 4. Combined Analysis: Which Joints to Remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e62b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive analysis table\n",
    "analysis_df = pd.DataFrame(index=remaining_joints)\n",
    "\n",
    "# Add variance\n",
    "analysis_df['variance'] = variances\n",
    "analysis_df['low_variance'] = analysis_df['variance'] < low_variance_threshold\n",
    "\n",
    "# Add ANOVA results\n",
    "analysis_df['anova_f_stat'] = anova_df['f_statistic']\n",
    "analysis_df['anova_p_value'] = anova_df['p_value']\n",
    "analysis_df['not_discriminative'] = analysis_df['anova_p_value'] >= 0.05\n",
    "\n",
    "# Add KS test results\n",
    "analysis_df['ks_statistic'] = ks_df['ks_statistic']\n",
    "analysis_df['ks_p_value'] = ks_df['p_value']\n",
    "analysis_df['different_distribution'] = analysis_df['ks_p_value'] < 0.05\n",
    "\n",
    "# Add outlier count\n",
    "analysis_df['outlier_count'] = outliers_per_joint\n",
    "\n",
    "# Flag joints with multiple issues\n",
    "analysis_df['num_issues'] = (\n",
    "    analysis_df['low_variance'].astype(int) +\n",
    "    analysis_df['not_discriminative'].astype(int) +\n",
    "    analysis_df['different_distribution'].astype(int)\n",
    ")\n",
    "\n",
    "# Sort by number of issues\n",
    "analysis_df = analysis_df.sort_values('num_issues', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPREHENSIVE JOINT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüö® Joints with MULTIPLE issues (strong candidates for removal):\")\n",
    "problematic = analysis_df[analysis_df['num_issues'] >= 2]\n",
    "if len(problematic) > 0:\n",
    "    for joint, row in problematic.iterrows():\n",
    "        issues = []\n",
    "        if row['low_variance']: issues.append('LOW_VAR')\n",
    "        if row['not_discriminative']: issues.append('NOT_DISCR')\n",
    "        if row['different_distribution']: issues.append('DIFF_DIST')\n",
    "        print(f\"  {joint}: {row['num_issues']} issues ‚Üí {', '.join(issues)}\")\n",
    "else:\n",
    "    print(\"  None found!\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Joints with ONE issue (consider for removal):\")\n",
    "moderate = analysis_df[analysis_df['num_issues'] == 1]\n",
    "if len(moderate) > 0:\n",
    "    for joint, row in moderate.iterrows():\n",
    "        issues = []\n",
    "        if row['low_variance']: issues.append('LOW_VAR')\n",
    "        if row['not_discriminative']: issues.append('NOT_DISCR')\n",
    "        if row['different_distribution']: issues.append('DIFF_DIST')\n",
    "        print(f\"  {joint}: {', '.join(issues)}\")\n",
    "else:\n",
    "    print(\"  None found!\")\n",
    "\n",
    "print(\"\\n‚úÖ Clean joints (no issues):\")\n",
    "clean = analysis_df[analysis_df['num_issues'] == 0]\n",
    "print(f\"  {len(clean)} joints: {', '.join(clean.index.tolist()[:10])}{'...' if len(clean) > 10 else ''}\")\n",
    "\n",
    "# Save analysis to CSV\n",
    "analysis_df.to_csv('joint_analysis_results.csv')\n",
    "print(\"\\nüíæ Detailed analysis saved to 'joint_analysis_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d68f2",
   "metadata": {},
   "source": [
    "## 5. Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RECOMMENDATIONS - November 15 Clue\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Joints to definitely remove (2+ issues)\n",
    "joints_to_remove = analysis_df[analysis_df['num_issues'] >= 2].index.tolist()\n",
    "\n",
    "# Joints to consider removing (1 issue)\n",
    "joints_to_consider = analysis_df[analysis_df['num_issues'] == 1].index.tolist()\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ OUTLIERS:\")\n",
    "if p_value < 0.05:\n",
    "    print(\"   ‚úÖ KEEP outliers - they are 'rare jewels' (associated with pain labels)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Consider REMOVING extreme outliers - they are likely 'errors'\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ JOINTS TO REMOVE (high confidence):\")\n",
    "if joints_to_remove:\n",
    "    print(f\"   Total: {len(joints_to_remove)} joints\")\n",
    "    print(f\"   Joints: {joints_to_remove}\")\n",
    "    print(\"\\n   Add to preprocessing.py:\")\n",
    "    print(f\"   df = df.drop(columns={joints_to_remove})\")\n",
    "else:\n",
    "    print(\"   None! All joints have at most 1 issue.\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ JOINTS TO CONSIDER REMOVING (medium confidence):\")\n",
    "if joints_to_consider:\n",
    "    print(f\"   Total: {len(joints_to_consider)} joints\")\n",
    "    print(f\"   Joints: {joints_to_consider}\")\n",
    "    print(\"   ‚Üí Test model performance with/without these\")\n",
    "else:\n",
    "    print(\"   None\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ CLEAN JOINTS (keep these):\")\n",
    "print(f\"   Total: {len(clean)} joints\")\n",
    "print(f\"   These are the 'signal' Yoda talks about!\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ NEXT STEPS:\")\n",
    "print(\"   1. Test model WITHOUT problematic joints\")\n",
    "print(\"   2. Compare validation F1 score: before vs after removal\")\n",
    "print(\"   3. If F1 improves or stays same ‚Üí keep the cleaned version\")\n",
    "print(\"   4. If F1 drops significantly ‚Üí some 'fog' joints might contain useful info\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° 'Not all that you see, is signal' - Yoda\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6fd23e",
   "metadata": {},
   "source": [
    "## 6. Export Cleaned Feature List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711aeed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of joints to keep (no issues or minor issues)\n",
    "joints_to_keep = analysis_df[analysis_df['num_issues'] < 2].index.tolist()\n",
    "\n",
    "print(f\"‚úÖ Joints to KEEP: {len(joints_to_keep)} / {len(remaining_joints)}\")\n",
    "print(f\"‚ùå Joints to REMOVE: {len(joints_to_remove)}\")\n",
    "\n",
    "# Save to file for easy import in preprocessing\n",
    "with open('joints_to_remove.txt', 'w') as f:\n",
    "    f.write('# Joints identified for removal by data cleaning analysis\\n')\n",
    "    f.write('# Based on: low variance, non-discriminative, or train/test mismatch\\n')\n",
    "    f.write(f\"joints_to_remove = {joints_to_remove}\\n\")\n",
    "\n",
    "print(\"\\nüíæ Saved to 'joints_to_remove.txt'\")\n",
    "print(\"   Import in preprocessing.py:\")\n",
    "print(\"   exec(open('joints_to_remove.txt').read())\")\n",
    "print(\"   df = df.drop(columns=joints_to_remove)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
