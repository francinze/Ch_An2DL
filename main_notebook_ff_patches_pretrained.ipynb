{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0ffa1054","cell_type":"markdown","source":"# Kaggle & Colab Imports","metadata":{}},{"id":"6d3e4c0e","cell_type":"code","source":"%%capture\n# KAGGLE IMPORTS\n# Clone repo\n!git clone https://github.com/francinze/Ch_An2DL.git /kaggle/working/ch2\n\n# Install kaggle API\n!pip install -q kaggle\n\n# Configure kaggle.json\n!mkdir -p /root/.config/kaggle\n\n# Copy your kaggle.json there\n!cp /kaggle/working/ch2/kaggle.json /root/.config/kaggle/\n\n# Set correct permissions\n!chmod 600 /root/.config/kaggle/kaggle.json\n\n# Move into the working directory\n%cd /kaggle/working/ch2/\n\n!mkdir -p data\n!mkdir -p models\n\n# Download competition files WITH CORRECT PATH\n!kaggle competitions download -c an2dl2526c2v2 -p ./data/\n\n# Unzip dataset WITH CORRECT PATH\n!unzip -o ./data/an2dl2526c2v2.zip -d ./data/\n\n# Verify download\n!ls -la ./data/\n!echo \"Download complete!\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"93b1cef6","cell_type":"code","source":"'''\n%%capture\n# COLAB IMPORTS\n!git clone https://github.com/francinze/Ch_An2DL.git\n! pip install -q kaggle\n! mkdir ~/.kaggle\n! cp Ch_An2DL/kaggle.json ~/.kaggle/\n! chmod 600 ~/.kaggle/kaggle.json\n%cd /content/Ch_An2DL/\n!mkdir data\n!mkdir models\n!kaggle competitions download -c an2dl2526c2v2 -p /data\n!unzip -o /data/an2dl2526c2v2.zip -d /data/\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"687201fe","cell_type":"markdown","source":"#  Import data","metadata":{}},{"id":"8f8eea66","cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"da2ccb0d","cell_type":"markdown","source":"## Organize Data by Type","metadata":{}},{"id":"d5a05d7a","cell_type":"code","source":"import os\nimport shutil\n\n# Detect environment and set appropriate path prefix\nif os.path.exists('./data/train_data'):\n    PATH_PREFIX = './'\n    print(\"✓ Found ./data/train_data (Local or Colab)\")\nelif os.path.exists('/data/train_data'):\n    PATH_PREFIX = '/'\n    print(\"✓ Found /data/train_data (Kaggle)\")\nelif os.path.exists('data/train_data'):\n    PATH_PREFIX = ''\n    print(\"✓ Found data/train_data (Current directory)\")\nelse:\n    print(\"✗ Data not found in expected locations!\")\n    PATH_PREFIX = '/'\n\nprint(f\"Using PATH_PREFIX: {PATH_PREFIX}\")\n\nprint(\"=\"*80)\nprint(\"ORGANIZING DATA INTO SEPARATE DIRECTORIES BY TYPE\")\nprint(\"=\"*80)\n\n# Define source directories\ntrain_data_dir = PATH_PREFIX + 'data/train_data/'\ntest_data_dir = PATH_PREFIX + 'data/test_data/'\n\n# Define target directories for organized data\ntrain_img_dir = PATH_PREFIX + 'data/train_img/'\ntrain_mask_dir = PATH_PREFIX + 'data/train_mask/'\ntest_img_dir = PATH_PREFIX + 'data/test_img/'\ntest_mask_dir = PATH_PREFIX + 'data/test_mask/'\n\ntrain_labels = pd.read_csv(PATH_PREFIX + 'data/train_labels.csv')\n\n# Create target directories if they don't exist\nfor directory in [train_img_dir, train_mask_dir, test_img_dir, test_mask_dir]:\n    os.makedirs(directory, exist_ok=True)\n\n# Function to organize files by type\ndef organize_data_by_type(source_dir, img_dir, mask_dir):\n    \"\"\"\n    Move image and mask files from source directory to separate directories.\n    Only moves files if they don't already exist in the target directory.\n    \"\"\"\n    if not os.path.exists(source_dir):\n        print(f\"⚠ Warning: Source directory not found: {source_dir}\")\n        return 0, 0\n    \n    files = os.listdir(source_dir)\n    img_count = 0\n    mask_count = 0\n    \n    for filename in files:\n        source_path = os.path.join(source_dir, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(source_path):\n            continue\n        \n        # Determine target directory based on filename prefix\n        if filename.startswith('img_'):\n            target_path = os.path.join(img_dir, filename)\n            if not os.path.exists(target_path):\n                shutil.copy2(source_path, target_path)\n                img_count += 1\n        elif filename.startswith('mask_'):\n            target_path = os.path.join(mask_dir, filename)\n            if not os.path.exists(target_path):\n                shutil.copy2(source_path, target_path)\n                mask_count += 1\n    \n    return img_count, mask_count\n\n# Organize training data\nprint(\"\\nOrganizing training data...\")\ntrain_img_moved, train_mask_moved = organize_data_by_type(\n    train_data_dir, train_img_dir, train_mask_dir\n)\nprint(f\"  Images: {train_img_moved} files copied to {train_img_dir}\")\nprint(f\"  Masks: {train_mask_moved} files copied to {train_mask_dir}\")\n\n# Organize test data\nprint(\"\\nOrganizing test data...\")\ntest_img_moved, test_mask_moved = organize_data_by_type(\n    test_data_dir, test_img_dir, test_mask_dir\n)\nprint(f\"  Images: {test_img_moved} files copied to {test_img_dir}\")\nprint(f\"  Masks: {test_mask_moved} files copied to {test_mask_dir}\")\n\n# Verify organization\nprint(\"\\n\" + \"=\"*80)\nprint(\"DATA ORGANIZATION SUMMARY\")\nprint(\"=\"*80)\nprint(f\"Train images: {len(os.listdir(train_img_dir)) if os.path.exists(train_img_dir) else 0} files in {train_img_dir}\")\nprint(f\"Train masks: {len(os.listdir(train_mask_dir)) if os.path.exists(train_mask_dir) else 0} files in {train_mask_dir}\")\nprint(f\"Test images: {len(os.listdir(test_img_dir)) if os.path.exists(test_img_dir) else 0} files in {test_img_dir}\")\nprint(f\"Test masks: {len(os.listdir(test_mask_dir)) if os.path.exists(test_mask_dir) else 0} files in {test_mask_dir}\")\nprint(\"=\"*80)\nprint(\"Data organization complete!\")\nprint(\"  - Organized copies are in train_img/, train_mask/, test_img/, test_mask/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4af21e5c","cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"id":"63af5838","cell_type":"markdown","source":"## Remove Shrek & Slimes","metadata":{}},{"id":"d8ef36cc","cell_type":"code","source":"# Parse the contaminated indices from the text file\ncontaminated_indices = []\nwith open('shrek_and_slimes.txt', 'r') as f:\n    for line in f:\n        line = line.strip()\n        if line and line.isdigit():\n            contaminated_indices.append(int(line))\n\nprint(f\"Found {len(contaminated_indices)} contaminated samples to remove\")\n\n# Define directories to clean (both img and mask directories)\ntrain_img_dir_clean = PATH_PREFIX + 'data/train_img/'\ntrain_mask_dir_clean = PATH_PREFIX + 'data/train_mask/'\n\n# Remove corresponding image and mask files from both directories\nremoved_count = 0\nfor idx in contaminated_indices:\n    img_name = f'img_{idx:04d}.png'\n    mask_name = f'mask_{idx:04d}.png'\n    \n    # Remove from train_img directory\n    img_path = os.path.join(train_img_dir_clean, img_name)\n    if os.path.exists(img_path):\n        os.remove(img_path)\n        removed_count += 1\n    \n    # Remove from train_mask directory\n    mask_path = os.path.join(train_mask_dir_clean, mask_name)\n    if os.path.exists(mask_path):\n        os.remove(mask_path)\n        removed_count += 1\n\nprint(f\"Removed {removed_count} files from organized directories\")\n\n# Update train_labels by removing contaminated indices\ntrain_labels = train_labels[~train_labels['sample_index'].str.extract(r'(\\d+)')[0].astype(int).isin(contaminated_indices)]\nprint(f\"Training labels updated: {len(train_labels)} samples remaining\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"63f4e14e-b5c7-4134-a63a-108cccf383df","cell_type":"markdown","source":"## Masks as Focus Filters","metadata":{}},{"id":"c4165b80-ee19-4d1f-80e0-c61896dc68fb","cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# === FUNZIONE CHE APPLICA LA MASK ALL’IMMAGINE ===\ndef apply_mask(image, mask):\n    \"\"\"\n    image: PIL RGB image\n    mask: PIL grayscale mask (0 = nero, 255 = bianco)\n    return: masked image (PIL)\n    \"\"\"\n\n    # Converti in numpy\n    img_np = np.array(image).astype(np.uint8)\n    mask_np = np.array(mask).astype(np.uint8)\n\n    # Normalizza la mask a 0–1\n    mask_np = mask_np / 255.0\n\n    # Se l’immagine ha 3 canali, estendi la mask\n    if img_np.ndim == 3:\n        mask_np = np.expand_dims(mask_np, axis=-1)\n\n    # Moltiplica → le zone nere diventano 0 (nero)\n    masked_img_np = (img_np * mask_np).astype(np.uint8)\n\n    return Image.fromarray(masked_img_np)\n\n# === VISUALIZZA A VIDEO ALCUNI ESEMPI ===\nsamples = sorted(os.listdir(train_img_dir))[:4]  # primi 4 esempi\n\nfig, axes = plt.subplots(len(samples), 2, figsize=(12, 2 * len(samples)))\n\nfor i, img_name in enumerate(samples):\n\n    # Carica immagine e mask corrispondente\n    img_path = os.path.join(train_img_dir, img_name)\n    mask_path = os.path.join(train_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n\n    image = Image.open(img_path).convert(\"RGB\")\n    mask = Image.open(mask_path).convert(\"L\")\n\n    # Applica la mask\n    masked_image = apply_mask(image, mask)\n\n    # --- Plot ---\n    axes[i, 0].imshow(image)\n    axes[i, 0].set_title(\"Original Image\")\n    axes[i, 0].axis(\"off\")\n\n    axes[i, 1].imshow(masked_image)\n    axes[i, 1].set_title(\"Masked Image\")\n    axes[i, 1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"aed302cc","cell_type":"code","source":"from tqdm import tqdm\n\n# Save masked images to a new directory\nmasked_train_img_dir = PATH_PREFIX + 'data/train_img_masked/'\nos.makedirs(masked_train_img_dir, exist_ok=True)\nmasked_test_img_dir = PATH_PREFIX + 'data/test_img_masked/'\nos.makedirs(masked_test_img_dir, exist_ok=True)\n\n# Apply masking to all training images and save\ntrain_img_files = sorted(os.listdir(train_img_dir))\n\nfor img_name in tqdm(train_img_files, desc=\"Processing training images\"):\n    img_path = os.path.join(train_img_dir, img_name)\n    mask_path = os.path.join(train_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n    image = Image.open(img_path).convert(\"RGB\")\n    mask = Image.open(mask_path).convert(\"L\")\n\n    masked_image = apply_mask(image, mask)\n    masked_image.save(os.path.join(masked_train_img_dir, img_name))\nprint(\"Masked training images saved.\")\n\n# Apply masking to all test images and save\ntest_img_files = sorted(os.listdir(test_img_dir))\nfor img_name in tqdm(test_img_files, desc=\"Processing test images\"):\n    img_path = os.path.join(test_img_dir, img_name)\n    mask_path = os.path.join(test_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n    image = Image.open(img_path).convert(\"RGB\")\n    mask = Image.open(mask_path).convert(\"L\")\n    masked_image = apply_mask(image, mask)\n    masked_image.save(os.path.join(masked_test_img_dir, img_name))\n\nprint(\"Masked test images saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d80f213e","cell_type":"markdown","source":"## Augmentation","metadata":{}},{"id":"3d8b4216","cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Analyze class distribution after removal\nclass_distribution = train_labels['label'].value_counts().sort_index()\nprint(\"\\n\" + \"=\"*60)\nprint(\"Class Distribution After Removal of Contaminated Images\")\nprint(\"=\"*60)\nprint(class_distribution)\nprint(f\"\\nTotal samples: {len(train_labels)}\")\n\n# Calculate statistics\nprint(\"\\n\" + \"=\"*60)\nprint(\"STATISTICS FOR AUGMENTATION\")\nprint(\"=\"*60)\n\n# Class with the most samples (majority)\nmax_class = class_distribution.max()\nmax_class_name = class_distribution.idxmax()\nprint(f\"\\nClass with the most samples (Majority): {max_class_name} ({max_class} samples)\")\n\n# Class with the fewest samples (minority)\nmin_class = class_distribution.min()\nmin_class_name = class_distribution.idxmin()\nprint(f\"Class with the fewest samples (Minority): {min_class_name} ({min_class} samples)\")\n\n# Imbalance ratio\nimbalance_ratio = max_class / min_class\nprint(f\"\\nImbalance ratio (Max/Min): {imbalance_ratio:.2f}x\")\n\n# Augmentation proposal\nprint(\"\\n\" + \"=\"*60)\nprint(\"RECOMMENDED AUGMENTATION STRATEGY\")\nprint(\"=\"*60)\nprint(\"\\nAugmentations to apply (as suggested by the professor):\")\nprint(\"  1. Horizontal Flip (p=0.5)\")\nprint(\"  2. Vertical Flip (p=0.5)\")\nprint(\"  3. Random Translation (0.2, 0.2)\")\nprint(\"  4. Random Zoom/Scale (0.8, 1.2)\")\nprint(\"  [EXCLUDE: Random Rotation - would change dimensions]\\n\")\n\n# STRATEGY: All classes grow until reaching the same target number for ALL\nprint(\"\\n\" + \"=\"*80)\nprint(\"BALANCED STRATEGY: ALL CLASSES GROW TO A FIXED AND EQUAL NUMBER\")\nprint(\"=\"*80)\n\n# ===== MODIFY THE TARGET NUMBER OF SAMPLES HERE =====\ntarget_samples = 250  # Desired number of samples for EACH class\n# =====================================================\n\nprint(f\"\\nTarget: {target_samples} samples for EACH class\")\n\naugmentation_strategy_balanced = {}\ntotal_to_generate = 0\n\nfor class_name in class_distribution.index:\n    n_samples = class_distribution[class_name]\n    n_needed = target_samples - n_samples\n    n_augmentations = max(0, n_needed)  # We cannot have negative augmentations\n    \n    augmentation_strategy_balanced[class_name] = {\n        'original': n_samples,\n        'target': target_samples,\n        'augment_count': n_augmentations,\n        'ratio_multiplier': n_augmentations / n_samples if n_samples > 0 else 0\n    }\n    \n    total_to_generate += n_augmentations\n\n# Projection of the dataset after augmentation\nprint(\"\\n\" + \"=\"*80)\nprint(\"DATASET AFTER BALANCED AUGMENTATION\")\nprint(\"=\"*80)\nprint(f\"{'Class':<20} {'Original':<15} {'New Augment':<15} {'Augmentations per image':<25} {'Total':<15}\")\nprint(\"-\" * 80)\n\ntotal_original = 0\ntotal_augmented = 0\nfor class_name in class_distribution.index:\n    n_original = class_distribution[class_name]\n    n_aug = augmentation_strategy_balanced[class_name]['augment_count']\n    n_total = n_original + n_aug\n    \n    total_original += n_original\n    total_augmented += n_total\n    \n    print(f\"{class_name:<20} {n_original:<15} {n_aug:<15} {augmentation_strategy_balanced[class_name]['ratio_multiplier']:<25.2f} {n_total:<15}\")\n\nprint(\"-\" * 80)\nprint(f\"{'TOTAL':<20} {total_original:<15} {total_to_generate:<15} {np.mean([augmentation_strategy_balanced[class_name]['ratio_multiplier'] for class_name in class_distribution.index]):<25.2f} {total_augmented:<15}\")\n\n# Visualize the distribution before and after\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Before\nclass_distribution.plot(kind='bar', ax=axes[0], color='steelblue')\naxes[0].set_title('Class Distribution - BEFORE Augmentation', fontsize=12, fontweight='bold')\naxes[0].set_ylabel('Number of samples')\naxes[0].set_xlabel('Class')\naxes[0].axhline(y=target_samples, color='red', linestyle='--', linewidth=2, label=f'Target: {target_samples}')\naxes[0].legend()\naxes[0].grid(axis='y', alpha=0.3)\n\n# After\nafter_augmentation_balanced = {}\nfor class_name in class_distribution.index:\n    after_augmentation_balanced[class_name] = augmentation_strategy_balanced[class_name]['target']\n\nafter_series = pd.Series(after_augmentation_balanced)\nafter_series.plot(kind='bar', ax=axes[1], color='seagreen')\naxes[1].set_title('Class Distribution - AFTER Balanced Augmentation', fontsize=12, fontweight='bold')\naxes[1].set_ylabel('Number of samples')\naxes[1].set_xlabel('Class')\naxes[1].axhline(y=target_samples, color='red', linestyle='--', linewidth=2, label=f'Target: {target_samples}')\naxes[1].set_ylim([0, max_class * 1.1])\naxes[1].legend()\naxes[1].grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f722ec8b","cell_type":"code","source":"from torchvision import transforms\n\n# Create folder for augmented data if it doesn't exist\naugmented_dir = PATH_PREFIX + f'data/train_data_augmented/'\ntrain_dir = PATH_PREFIX + 'data/train_img_cropped/'\nif not os.path.exists(augmented_dir):\n    os.makedirs(augmented_dir)\n    print(f\"Created directory: {augmented_dir}\")\nelse:\n    existing_files = len(os.listdir(augmented_dir))\n    print(f\"Directory already exists: {augmented_dir}\")\n    print(f\"Found {existing_files} existing augmented files\")\n\n# Define augmentations for each class\n# Using expand=True in RandomRotation to preserve aspect ratio and prevent cropping\naugmentation_transforms = {\n    'flip': transforms.Compose([\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.RandomRotation(180, expand=True, fill=0)  # expand=True prevents cropping, fill=0 for black padding\n    ]),\n}\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STARTING AUGMENTATION PROCESS\")\nprint(\"=\"*80)\n\n# Loop through each class and generate augmentations\ntotal_augmented = 0\n\nfor class_name in sorted(augmentation_strategy_balanced.keys()):\n    info = augmentation_strategy_balanced[class_name]\n    n_augment = info['augment_count']\n    \n    if n_augment == 0:\n        print(f\"\\n{class_name}: No augmentation needed (already at target)\")\n        continue\n    \n    print(f\"\\n{'-'*80}\")\n    print(f\"Class: {class_name}\")\n    print(f\"Augmentations to generate: {n_augment}\")\n    print(f\"{'-'*80}\")\n    \n    # Get original images of this class\n    class_samples = train_labels[train_labels['label'] == class_name]['sample_index'].tolist()\n    n_original = len(class_samples)\n    \n    # Calculate how many augmentations per original image\n    aug_per_img = n_augment / n_original\n    \n    # For each original image\n    aug_count = 0\n    for img_idx, img_name in enumerate(class_samples):\n        # Determine which file to load based on DATA_TYPE\n        file_name = img_name\n        \n        img_path = os.path.join(masked_train_img_dir, file_name)\n        \n        if not os.path.exists(img_path):\n            print(f\"  File not found: {file_name}\")\n            continue\n        \n        # Load the original image/mask\n        img = Image.open(img_path).convert('RGB')\n        img_pil = img.copy()\n        \n        # Generate augmentations for this image\n        n_to_generate = int(np.ceil(aug_per_img)) if img_idx < n_augment % n_original else int(np.floor(aug_per_img))\n        \n        for aug_num in range(n_to_generate):\n            if aug_count <= n_augment:\n                base_name = file_name.replace('.png', '')\n\n                # Choose an augmentation type cyclically\n                aug_types = list(augmentation_transforms.keys())\n                aug_type = aug_types[aug_count % len(aug_types)]\n                transform = augmentation_transforms[aug_type]\n                img_augmented = transform(img_pil)\n                augmented_img_name = f\"{base_name}_aug_{aug_num}_{aug_type}.png\"\n                \n                # Save augmented image\n                augmented_img_path = os.path.join(augmented_dir, augmented_img_name)\n                img_augmented.save(augmented_img_path)\n                \n            aug_count += 1\n        \n        # Progress update\n        if (img_idx + 1) % max(1, n_original // 5) == 0 or img_idx == n_original - 1:\n            print(f\"  Processed {img_idx + 1}/{n_original} original samples ({aug_count} augmentations generated)\")\n    \n    total_augmented += aug_count\n    print(f\"  {class_name}: Completed! {aug_count} augmentations generated\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(f\"AUGMENTATION COMPLETED!\")\nprint(f\"Total augmented images generated: {total_augmented}\")\nprint(f\"Save directory: {augmented_dir}\")\nprint(\"=\"*80)\n\n# Verify file countprint(f\"First 5 files: {augmented_files[:5]}\")\naugmented_files = os.listdir(augmented_dir)\nprint(f\"\\nFiles in augmented folder: {len(augmented_files)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"01c11365","cell_type":"markdown","source":"## Crop Masked Images","metadata":{}},{"id":"0dc3ab60","cell_type":"code","source":"# Crop masked images to the extent of information\n# Now that we have applied the masks, we can crop the images to the bounding box of the non-zero regions in the masks.\n\n# Define minimum image size (will be used later in preprocessing)\nMIN_IMG_SIZE = (256, 256)\ndef crop_to_mask(image, min_size=MIN_IMG_SIZE):\n    \"\"\"\n    Crop the image to the bounding box of the non-zero regions.\n    Since the mask has already been applied to the image, we detect non-zero pixels directly from the image.\n    If the cropped image is smaller than min_size, add zero-padding to reach min_size.\n    \n    image: PIL RGB image (already masked, with black background)\n    mask: PIL grayscale mask (not used, kept for compatibility)\n    min_size: tuple (width, height) - minimum size for the output image\n    return: cropped and padded image (PIL)\n    \"\"\"\n    # Convert image to numpy array\n    img_np = np.array(image).astype(np.uint8)\n    \n    # Find non-zero pixels in any channel (R, G, or B)\n    # Sum across color channels and check where sum > 0\n    non_zero_mask = np.sum(img_np, axis=2) > 0\n    coords = np.column_stack(np.where(non_zero_mask))\n    \n    if coords.size == 0:\n        return image  # No cropping if image is completely black\n    \n    y_min, x_min = coords.min(axis=0)\n    y_max, x_max = coords.max(axis=0) + 1  # add 1 to include the max pixel\n    \n    cropped = image.crop((x_min, y_min, x_max, y_max))\n    \n    # Check if padding is needed\n    width, height = cropped.size\n    min_width, min_height = min_size\n    \n    if width < min_width or height < min_height:\n        # Calculate padding needed\n        pad_width = max(0, min_width - width)\n        pad_height = max(0, min_height - height)\n        \n        # Create new image with black padding\n        padded = Image.new('RGB', (max(width, min_width), max(height, min_height)), (0, 0, 0))\n        \n        # Paste cropped image in the center\n        paste_x = pad_width // 2\n        paste_y = pad_height // 2\n        padded.paste(cropped, (paste_x, paste_y))\n        \n        return padded\n    \n    return cropped\n\n# === VISUALIZZA A VIDEO ALCUNI ESEMPI DI CROP ===\nsamples = sorted(os.listdir(masked_train_img_dir))[:4]  # primi 4 esempi\nfig, axes = plt.subplots(len(samples), 3, figsize=(12, 3 * len(samples)))\n\nfor i, img_name in enumerate(samples):\n    # Carica immagine e mask corrispondente\n    img_path = os.path.join(masked_train_img_dir, img_name)\n    mask_path = os.path.join(train_mask_dir, img_name.replace(\"img_\", \"mask_\"))\n\n    image = Image.open(img_path).convert(\"RGB\")\n    mask = Image.open(mask_path).convert(\"L\")\n\n    # Applica la mask\n    masked_image = apply_mask(image, mask)\n\n    # Esegui il crop con padding se necessario\n    cropped_image = crop_to_mask(masked_image)\n\n    # --- Plot ---\n    axes[i, 0].imshow(masked_image)\n    axes[i, 0].set_title(\"Masked Image\")\n    axes[i, 0].axis(\"off\")\n\n    axes[i, 1].imshow(mask, cmap=\"gray\")\n    axes[i, 1].set_title(\"Mask\")\n    axes[i, 1].axis(\"off\")\n\n    axes[i, 2].imshow(cropped_image)\n    axes[i, 2].set_title(f\"Cropped Image\\n{cropped_image.size[0]}x{cropped_image.size[1]}\")\n    axes[i, 2].axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6473d6cb","cell_type":"code","source":"from tqdm import tqdm\n# Crop all images and save to new directories\n# Define new directories for cropped images\ncropped_train_img_dir = PATH_PREFIX + 'data/train_img_cropped/'\ncropped_test_img_dir = PATH_PREFIX + 'data/test_img_cropped/'\nos.makedirs(cropped_train_img_dir, exist_ok=True)\nos.makedirs(cropped_test_img_dir, exist_ok=True)\n\n# Process training images\nfor img_name in tqdm(os.listdir(masked_train_img_dir), desc=\"Processing training images\"):\n    img_path = os.path.join(masked_train_img_dir, img_name)\n\n    image = Image.open(img_path).convert(\"RGB\")\n\n    cropped_image = crop_to_mask(image)\n\n    cropped_image.save(os.path.join(cropped_train_img_dir, img_name))\nprint(\"\\nCropped training images saved.\")\n# Process test images\nfor img_name in tqdm(os.listdir(masked_test_img_dir), desc=\"Processing test images\"):\n    img_path = os.path.join(masked_test_img_dir, img_name)\n\n    image = Image.open(img_path).convert(\"RGB\")\n\n    cropped_image = crop_to_mask(image)\n\n    cropped_image.save(os.path.join(cropped_test_img_dir, img_name))\nprint(\"\\nCropped test images saved.\")\n\n# Crop also all augmented images\nfor img_name in tqdm(os.listdir(augmented_dir), desc=\"Processing augmented images\"):\n    img_path = os.path.join(augmented_dir, img_name)\n\n    image = Image.open(img_path).convert(\"RGB\")\n\n    cropped_image = crop_to_mask(image)\n\n    cropped_image.save(os.path.join(cropped_train_img_dir, img_name))\nprint(\"\\nCropped augmented images saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cca46753","cell_type":"markdown","source":"# Dataloaders","metadata":{}},{"id":"fe745789","cell_type":"code","source":"from torch.utils.data import TensorDataset, Dataset, DataLoader\n\n# ===== PATCH-BASED PROCESSING SETTINGS =====\n# Instead of resizing images to 224x224 (which loses detail),\n# we extract patches at full resolution and process them separately\nPATCH_SIZE = 256  # Size of each patch (256x256)\nPATCH_STRIDE = 64\n# ==========================================\n\n# For backward compatibility when not using patches\nIMG_SIZE = (224, 224) if not PATCH_SIZE and not PATCH_STRIDE else (PATCH_SIZE, PATCH_SIZE)\n\n# Create DataLoaders\n# Larger batch size for multi-GPU: DataParallel splits batch across GPUs\n# With 2 GPUs: effective batch per GPU = BATCH_SIZE / 2\nBATCH_SIZE = 128  # 64 per GPU with DataParallel\n\n# ===== GPU OPTIMIZATION SETTINGS =====\n# Kaggle T4 x2 optimization: maximize data loading throughput\n# Each T4 GPU can handle 4 workers efficiently\nimport torch\nif torch.cuda.is_available():\n    NUM_WORKERS = 2 * torch.cuda.device_count()  # 2 workers per GPU = 4 total for T4 x2\n    PIN_MEMORY = True  # CRITICAL: Enables async CPU-to-GPU transfer while GPU computes\n    PERSISTENT_WORKERS = True  # Keeps workers alive between epochs (faster)\n    print(f\"GPU detected: {torch.cuda.device_count()} GPU(s) - using {NUM_WORKERS} workers\")\nelse:\n    # CPU-only environment\n    NUM_WORKERS = 0\n    PIN_MEMORY = False\n    PERSISTENT_WORKERS = False\n# ======================================\n\n# Load original + augmented images into tensors\nprint(\"\\n\" + \"=\"*80)\nprint(\"LOADING CROPPED ORIGINAL + AUGMENTED IMAGES\")\nprint(\"=\"*80)\nprint(f\"Cropped directory: {cropped_train_img_dir}\")\n\n# Check if augmented directory exists and validate files\nif not os.path.exists(augmented_dir):\n    print(f\"\\nWARNING: Augmented directory does not exist!\")\n    print(f\"Expected: {augmented_dir}\")\n    print(f\"No augmented data will be loaded. Only original images will be used.\")\n    augmented_files = []\nelse:\n    # Create list of augmented images\n    augmented_files = os.listdir(augmented_dir)\n    print(f\"Augmented images found: {len(augmented_files)}\")\n\n# Create new dataframe with all images (original + augmented)\ntrain_labels_augmented = train_labels.copy()\n\n# Add augmented images\naugmented_rows = []\nfor aug_img_name in augmented_files:\n    # Extract original file name (works for both img_ and mask_ prefixes)\n    # Format: {prefix}_{number}_aug_{aug_num}_{aug_type}.png\n    base_name = aug_img_name.split('_aug_')[0] + '.png'\n    \n    original_row = train_labels[train_labels['sample_index'] == base_name]\n    if not original_row.empty:\n        class_label = original_row.iloc[0]['label']\n        augmented_rows.append({'sample_index': aug_img_name, 'label': class_label})\n\naugmented_df = pd.DataFrame(augmented_rows)\ntrain_labels_augmented = pd.concat([train_labels_augmented, augmented_df], ignore_index=True)\n\nprint(f\"\\nOriginal dataset: {len(train_labels)} samples\")\nprint(f\"Augmented dataset: {len(train_labels_augmented)} samples\")\nprint(f\"\\nDistribution in augmented dataset:\")\nprint(train_labels_augmented['label'].value_counts().sort_index())\n\n# ===== PRE-EXTRACT PATCHES TO DISK (RUN ONCE) =====\nprint(\"\\n\" + \"=\"*80)\nprint(\"PATCH PRE-EXTRACTION (Multi-File Strategy)\")\nprint(\"=\"*80)\n\n# Directory to save pre-extracted patches\npatches_dir = os.path.join(PATH_PREFIX, \"data\", \"patches_cache\")\nos.makedirs(patches_dir, exist_ok=True)\n\n# Metadata file to track all batch files\nmetadata_file = os.path.join(patches_dir, f\"metadata_ps{PATCH_SIZE}_stride{PATCH_STRIDE}.json\")\n\nif os.path.exists(metadata_file):\n    print(f\"✓ Found pre-extracted patches!\")\n    print(f\"  Loading metadata from: {metadata_file}\")\n    \n    import json\n    with open(metadata_file, 'r') as f:\n        metadata = json.load(f)\n    \n    print(f\"✓ Found {metadata['num_batches']} batch files with {metadata['total_patches']} total patches\")\n    print(f\"✓ Found {metadata['num_batches']} batch files with {metadata['total_patches']} total patches\")\n    \nelse:\n    print(f\"⚠ No pre-extracted patches found. Extracting now...\")\n    print(f\"  Strategy: Save each batch to separate file (no huge concatenation!)\")\n    \n    # Helper function to extract patches from a single image\n    def extract_patches_from_image(img_path, patch_size, stride, min_variance_threshold=0.001):\n        \"\"\"\n        Extract all patches from a single image, filtering out blank patches.\n        \n        Args:\n            img_path: Path to the image file\n            patch_size: Size of each patch\n            stride: Stride for patch extraction\n            min_variance_threshold: Minimum variance to consider patch non-blank (default: 0.001)\n        \n        Returns:\n            List of patch tensors (excluding blank patches)\n        \"\"\"\n        img = Image.open(img_path).convert('RGB')\n        \n        img_array = np.array(img, dtype=np.float32)\n        h, w, c = img_array.shape\n        patches = []\n        \n        if h < patch_size or w < patch_size:\n            pad_h = max(0, patch_size - h)\n            pad_w = max(0, patch_size - w)\n            img_array = np.pad(img_array, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n            patch = img_array[:patch_size, :patch_size, :]\n            \n            # Check if patch is not blank\n            patch_normalized = patch / 255.0\n            if np.var(patch_normalized) > min_variance_threshold:\n                patch_tensor = torch.from_numpy(patch.transpose(2, 0, 1) / 255.0).float()\n                patches.append(patch_tensor)\n        else:\n            n_patches_h = max(1, (h - patch_size) // stride + 1)\n            n_patches_w = max(1, (w - patch_size) // stride + 1)\n            \n            for row_idx in range(n_patches_h):\n                for col_idx in range(n_patches_w):\n                    start_h = min(row_idx * stride, h - patch_size)\n                    start_w = min(col_idx * stride, w - patch_size)\n                    patch = img_array[start_h:start_h+patch_size, start_w:start_w+patch_size, :]\n                    \n                    # Check if patch is not blank (has sufficient variance)\n                    patch_normalized = patch / 255.0\n                    if np.var(patch_normalized) > min_variance_threshold:\n                        patch_tensor = torch.from_numpy(patch.transpose(2, 0, 1) / 255.0).float()\n                        patches.append(patch_tensor)\n        \n        return patches\n    \n    label_map = {'Triple negative': 0, 'Luminal A': 1, 'Luminal B': 2, 'HER2(+)': 3}\n    IMAGES_PER_BATCH = 50  # Process 50 images at a time (stride=256 = 4x fewer patches)\n    \n    print(f\"\\nExtracting and saving patches in batches of {IMAGES_PER_BATCH} images...\")\n    \n    batch_patches = []\n    batch_labels = []\n    batch_num = 0\n    total_patches = 0\n    batch_files = []\n    \n    for idx, row in train_labels_augmented.iterrows():\n        if idx % 10 == 0:\n            print(f\"  Progress: {idx}/{len(train_labels_augmented)} images...\")\n        \n        img_name = row['sample_index']\n        label = label_map[row['label']]\n        \n        img_path = os.path.join(cropped_train_img_dir, img_name)\n        \n        if os.path.exists(img_path):\n            patches = extract_patches_from_image(img_path, PATCH_SIZE, PATCH_STRIDE)\n            batch_patches.extend(patches)\n            batch_labels.extend([label] * len(patches))\n        \n        # Save batch to disk when full (DON'T accumulate in memory!)\n        if (idx + 1) % IMAGES_PER_BATCH == 0 or idx == len(train_labels_augmented) - 1:\n            if len(batch_patches) > 0:\n                batch_tensor = torch.stack(batch_patches)\n                batch_labels_tensor = torch.tensor(batch_labels, dtype=torch.long)\n                \n                # Save this batch to its own file\n                batch_file = os.path.join(patches_dir, f\"batch_{batch_num:03d}.pt\")\n                torch.save({'patches': batch_tensor, 'labels': batch_labels_tensor}, batch_file)\n                \n                batch_files.append(batch_file)\n                total_patches += len(batch_patches)\n                print(f\"    ✓ Saved batch {batch_num}: {len(batch_patches)} patches → {batch_file}\")\n                \n                # CRITICAL: Clear memory immediately!\n                batch_patches = []\n                batch_labels = []\n                batch_num += 1\n    \n    # Save metadata\n    import json\n    metadata = {\n        'num_batches': batch_num,\n        'total_patches': total_patches,\n        'batch_files': batch_files,\n        'patch_size': PATCH_SIZE,\n        'stride': PATCH_STRIDE\n    }\n    with open(metadata_file, 'w') as f:\n        json.dump(metadata, f, indent=2)\n    \n    print(f\"\\nExtraction complete!\")\n    print(f\"  Saved {batch_num} batch files with {total_patches} total patches\")\n    print(f\"  Metadata saved to: {metadata_file}\")\n\n# Load metadata for dataset creation\nimport json\nwith open(metadata_file, 'r') as f:\n    metadata = json.load(f)\n\nprint(f\"\\n{'='*80}\")\nprint(\"Creating multi-file dataset...\")\nprint(f\"Total patches: {metadata['total_patches']}\")\n\n# Custom Dataset that loads from multiple batch files\nclass MultiFilePatchDataset(Dataset):\n    \"\"\"Dataset that loads patches from multiple batch files with caching\"\"\"\n    \n    def __init__(self, metadata):\n        self.batch_files = metadata['batch_files']\n        self.total_patches = metadata['total_patches']\n        \n        # Cache for loaded batches (key: batch_idx, value: batch_data)\n        self._cache = {}\n        \n        # Load all batches to build index (loads only metadata, not actual tensors)\n        self.batch_sizes = []\n        self.cumulative_sizes = [0]\n        \n        print(f\"Loading batch metadata from {len(self.batch_files)} files...\")\n        for batch_file in self.batch_files:\n            batch_data = torch.load(batch_file)\n            batch_size = len(batch_data['patches'])\n            self.batch_sizes.append(batch_size)\n            self.cumulative_sizes.append(self.cumulative_sizes[-1] + batch_size)\n            \n            # CRITICAL: Pre-load all batches into cache (only ~22K patches total, manageable)\n            # This avoids repeated torch.load() calls which are extremely slow\n            self._cache[len(self.batch_sizes) - 1] = batch_data\n        \n        print(f\"Dataset ready with {self.total_patches} patches (all batches cached in RAM)\")\n    \n    def __len__(self):\n        return self.total_patches\n    \n    def __getitem__(self, idx):\n        # Find which batch file contains this index\n        batch_idx = np.searchsorted(self.cumulative_sizes[1:], idx, side='right')\n        local_idx = idx - self.cumulative_sizes[batch_idx]\n        \n        # Use cached batch data (no disk I/O!)\n        batch_data = self._cache[batch_idx]\n        \n        return batch_data['patches'][local_idx], batch_data['labels'][local_idx]\n\n# Load metadata for dataset creation\nimport json\nwith open(metadata_file, 'r') as f:\n    metadata = json.load(f)\n\nprint(f\"\\n{'='*80}\")\nprint(\"Creating multi-file dataset...\")\nprint(f\"Total patches: {metadata['total_patches']}\")\n\n# Create dataset\nfull_dataset = MultiFilePatchDataset(metadata)\n\n# ===== CREATE TRAIN/VAL SPLIT =====\nprint(\"\\n\" + \"=\"*80)\nprint(\"CREATING TRAIN/VAL SPLIT\")\nprint(\"=\"*80)\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\n# Create indices for train/val split\n# Need to load all labels to stratify properly\nprint(\"Loading all labels for stratified split...\")\nall_labels_list = []\nfor batch_file in metadata['batch_files']:\n    batch_data = torch.load(batch_file)\n    all_labels_list.extend(batch_data['labels'].tolist())\n\nall_labels_array = np.array(all_labels_list)\n\n# Split indices (not data!)\ntrain_indices, val_indices = train_test_split(\n    np.arange(len(all_labels_array)), test_size=0.2, random_state=42, \n    stratify=all_labels_array\n)\n\nprint(f\"Train patches: {len(train_indices)}\")\nprint(f\"Val patches: {len(val_indices)}\")\n\n# Create subset datasets\ntrain_dataset = Subset(full_dataset, train_indices)\nval_dataset = Subset(full_dataset, val_indices)\n\nlabel_map = {'Triple negative': 0, 'Luminal A': 1, 'Luminal B': 2, 'HER2(+)': 3}\n\n# Create DataLoaders with GPU optimizations\ntrain_loader_kwargs = {\n    'batch_size': BATCH_SIZE,\n    'shuffle': True,\n    'num_workers': NUM_WORKERS,\n    'pin_memory': PIN_MEMORY\n}\nval_loader_kwargs = {\n    'batch_size': BATCH_SIZE,\n    'shuffle': False,\n    'num_workers': NUM_WORKERS,\n    'pin_memory': PIN_MEMORY\n}\nif NUM_WORKERS > 0:\n    train_loader_kwargs['persistent_workers'] = PERSISTENT_WORKERS\n    val_loader_kwargs['persistent_workers'] = PERSISTENT_WORKERS\n\ntrain_loader = DataLoader(train_dataset, **train_loader_kwargs)\nval_loader = DataLoader(val_dataset, **val_loader_kwargs)\n\nprint(f\"\\nOptimization: {NUM_WORKERS} workers, pin_memory={PIN_MEMORY}, persistent_workers={PERSISTENT_WORKERS}\")\nprint(f\"\\nCreated DataLoaders:\")\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Val batches: {len(val_loader)}\")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f7d4a0a5","cell_type":"code","source":"# Memory-efficient test dataset\nclass TestPatchDataset(Dataset):\n    \"\"\"Memory-efficient test dataset that extracts patches on-the-fly\"\"\"\n    \n    def __init__(self, data_dir, patch_size=PATCH_SIZE, stride=PATCH_STRIDE, min_variance_threshold=0.001):\n        self.data_dir = data_dir\n        self.patch_size = patch_size\n        self.stride = stride\n        self.min_variance_threshold = min_variance_threshold\n        \n        # Get image files\n        self.image_files = sorted([f for f in os.listdir(data_dir) if f.startswith('img_')])\n        \n        # Pre-compute patch counts and mapping (if using patches)\n        if PATCH_SIZE and PATCH_STRIDE:\n            self.patch_counts = []\n            self.cumulative_patches = [0]\n            self.patch_to_image = []\n            \n            print(f\"Computing patch counts for {len(self.image_files)} test images (filtering blank patches)...\")\n            for i, img_name in enumerate(self.image_files):\n                if i % 100 == 0 and i > 0:\n                    print(f\"  Processed {i}/{len(self.image_files)} images...\")\n                \n                img_path = os.path.join(data_dir, img_name)\n                img = Image.open(img_path)\n                img_array = np.array(img, dtype=np.float32)\n                h, w = img.size[1], img.size[0]\n                \n                # Count non-blank patches\n                if h < patch_size or w < patch_size:\n                    n_patches = 1  # Single padded patch\n                else:\n                    n_patches_h = max(1, (h - patch_size) // stride + 1)\n                    n_patches_w = max(1, (w - patch_size) // stride + 1)\n                    \n                    # Count only non-blank patches\n                    valid_patches = 0\n                    for row_idx in range(n_patches_h):\n                        for col_idx in range(n_patches_w):\n                            start_h = min(row_idx * stride, h - patch_size)\n                            start_w = min(col_idx * stride, w - patch_size)\n                            patch = img_array[start_h:start_h+patch_size, start_w:start_w+patch_size, :]\n                            \n                            # Check if patch has sufficient variance\n                            if np.var(patch / 255.0) > min_variance_threshold:\n                                valid_patches += 1\n                    \n                    n_patches = max(1, valid_patches)  # Ensure at least 1 patch per image\n                \n                self.patch_counts.append(n_patches)\n                self.cumulative_patches.append(self.cumulative_patches[-1] + n_patches)\n                \n                # Track which image each patch belongs to\n                for _ in range(n_patches):\n                    self.patch_to_image.append(i)\n            \n            self.total_patches = self.cumulative_patches[-1]\n            print(f\"Total test patches (excluding blank): {self.total_patches}\")\n        else:\n            self.total_patches = len(self.image_files)\n            self.patch_to_image = list(range(len(self.image_files)))\n    \n    def __len__(self):\n        return self.total_patches\n    \n    def _get_valid_patches_for_image(self, img_array, img_h, img_w):\n        \"\"\"Extract all valid (non-blank) patches from an image and return their positions\"\"\"\n        valid_patches_info = []\n        \n        if img_h < self.patch_size or img_w < self.patch_size:\n            # For small images, use padded single patch\n            valid_patches_info.append((0, 0))  # placeholder position\n        else:\n            n_patches_h = max(1, (img_h - self.patch_size) // self.stride + 1)\n            n_patches_w = max(1, (img_w - self.patch_size) // self.stride + 1)\n            \n            for row_idx in range(n_patches_h):\n                for col_idx in range(n_patches_w):\n                    start_h = min(row_idx * self.stride, img_h - self.patch_size)\n                    start_w = min(col_idx * self.stride, img_w - self.patch_size)\n                    patch = img_array[start_h:start_h+self.patch_size, start_w:start_w+self.patch_size, :]\n                    \n                    # Check if patch has sufficient variance\n                    if np.var(patch / 255.0) > self.min_variance_threshold:\n                        valid_patches_info.append((start_h, start_w))\n            \n            # Ensure at least one patch per image\n            if len(valid_patches_info) == 0:\n                valid_patches_info.append((0, 0))\n        \n        return valid_patches_info\n    \n    def __getitem__(self, idx):\n        if PATCH_SIZE and PATCH_STRIDE:\n            # Find image for this patch\n            img_idx = np.searchsorted(self.cumulative_patches[1:], idx, side='right')\n            patch_idx = idx - self.cumulative_patches[img_idx]\n            img_name = self.image_files[img_idx]\n        else:\n            img_name = self.image_files[idx]\n        \n        img_path = os.path.join(self.data_dir, img_name)\n        \n        # Load image\n        img = Image.open(img_path).convert('RGB')\n        img_array = np.array(img, dtype=np.float32)\n        \n        # Extract patch or resize\n        if PATCH_SIZE and PATCH_STRIDE:\n            h, w, c = img_array.shape\n            \n            if h < self.patch_size or w < self.patch_size:\n                pad_h = max(0, self.patch_size - h)\n                pad_w = max(0, self.patch_size - w)\n                img_array = np.pad(img_array, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n                patch = img_array[:self.patch_size, :self.patch_size, :]\n            else:\n                # Get valid patch positions for this image\n                valid_patches = self._get_valid_patches_for_image(img_array, h, w)\n                start_h, start_w = valid_patches[patch_idx]\n                patch = img_array[start_h:start_h+self.patch_size, start_w:start_w+self.patch_size, :]\n            \n            final_img = patch\n        else:\n            img_pil = Image.fromarray(img_array.astype('uint8'))\n            img_resized = img_pil.resize((self.patch_size, self.patch_size), Image.BILINEAR)\n            final_img = np.array(img_resized)\n        \n        # Convert to tensor\n        img_tensor = torch.from_numpy(final_img).permute(2, 0, 1).float() / 255.0\n        \n        return img_tensor\n\n# Load test data\nprint(f\"\\nLoading test data\")\nif PATCH_SIZE and PATCH_STRIDE:\n    print(f\"Using PATCH-BASED processing for test data\")\n    \ntest_dataset = TestPatchDataset(cropped_test_img_dir, PATCH_SIZE, PATCH_STRIDE)\ntest_filenames = test_dataset.image_files\ntest_patch_to_image = test_dataset.patch_to_image\n\nprint(f\"Test {'patches' if PATCH_SIZE and PATCH_STRIDE else 'images'}: {len(test_dataset)}\")\nprint(f\"Test image files: {len(test_filenames)}\")\n\n# Create DataLoader with GPU optimizations\ntest_loader_kwargs = {\n    'batch_size': BATCH_SIZE,\n    'shuffle': False,\n    'num_workers': NUM_WORKERS,\n    'pin_memory': PIN_MEMORY\n}\nif NUM_WORKERS > 0:\n    test_loader_kwargs['persistent_workers'] = PERSISTENT_WORKERS\n\ntest_loader = DataLoader(test_dataset, **test_loader_kwargs)\n\nprint(f\"\\nDataLoader created:\")\nprint(f\"Test batches: {len(test_loader)}\")\nprint(f\"Optimization: {NUM_WORKERS} workers, pin_memory={PIN_MEMORY}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5ad6b729","cell_type":"code","source":"input_shape = (3, PATCH_SIZE, PATCH_SIZE) if PATCH_SIZE else (3, IMG_SIZE[0], IMG_SIZE[1])\nnum_classes = len(label_map)\n\n# ===== MULTI-GPU SETUP =====\n# Check for multiple GPUs and set up DataParallel\nif torch.cuda.is_available():\n    num_gpus = torch.cuda.device_count()\n    device = torch.device('cuda:0')\n    print(f\"Found {num_gpus} GPU(s) available:\")\n    for i in range(num_gpus):\n        print(f\" GPU {i}: {torch.cuda.get_device_name(i)}\")\n    if num_gpus > 1:\n        print(f\"Multi-GPU training enabled: Will use {num_gpus} GPUs with DataParallel\")\n    else:\n        print(f\"Single GPU training\")\nelse:\n    device = torch.device('cpu')\n    num_gpus = 0\n    print(\"No GPU available, using CPU\")\n# ===========================","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a85cd3aa","cell_type":"markdown","source":"# Parameters","metadata":{}},{"id":"00880a6d","cell_type":"code","source":"import torch.nn as nn\n\n# Number of training epochs\nLEARNING_RATE = 1e-3\nEPOCHS = 300\nPATIENCE = 50\n\n# Regularisation\nDROPOUT_RATE = 0.5       # Dropout probability\nL1_LAMBDA = 0.001           # L1 penalty\nL2_LAMBDA = 0.01         # L2 penalty\n\n# Set up loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\n\n# Print the defined parameters\nprint(\"Epochs:\", EPOCHS)\nprint(\"Batch Size:\", BATCH_SIZE)\nprint(\"Learning Rate:\", LEARNING_RATE)\nprint(\"Dropout Rate:\", DROPOUT_RATE)\nprint(\"L1 Penalty:\", L1_LAMBDA)\nprint(\"L2 Penalty:\", L2_LAMBDA)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9dfb42d0","cell_type":"markdown","source":"# Model Builder","metadata":{}},{"id":"e76b60f5-a3e4-4214-843d-a0c2eec77448","cell_type":"code","source":"import torchvision.models as models\nfrom torchvision.models import ResNet18_Weights, ResNet50_Weights, EfficientNet_B0_Weights, EfficientNet_B3_Weights, VGG16_Weights\n\n# ===== UNCOMMENT THE MODEL YOU WANT TO USE =====\n\n# ResNet-18 (Smaller, faster)\nmodel_pretrained = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\nMODEL_NAME = \"resnet18\"\n\n# ResNet-50 (Deeper, more powerful)\n# model_pretrained = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n# MODEL_NAME = \"resnet50\"\n\n# EfficientNet-B0 (Efficient, good balance)\n# model_pretrained = models.efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n# MODEL_NAME = \"efficientnet_b0\"\n\n# EfficientNet-B3 (More powerful EfficientNet)\n# model_pretrained = models.efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n# MODEL_NAME = \"efficientnet_b3\"\n\n# VGG-16 (Classic architecture)\n# model_pretrained = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n# MODEL_NAME = \"vgg16\"\n\n# ===============================================\n\nprint(f\"Loaded pretrained model: {MODEL_NAME}\")\nprint(f\"Model architecture:\\n{model_pretrained}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9d67a91a-dd4b-452c-930b-8fa213a01a69","cell_type":"code","source":"import torch.nn as nn\n\n# ===== STEP 1: Freeze all layers in the feature extractor =====\nfor param in model_pretrained.parameters():\n    param.requires_grad = False\n\nprint(\"\\nAll feature extractor weights frozen\")\n\n# ===== STEP 2: Replace the classifier head =====\n# Different models have different classifier layer names\nif MODEL_NAME.startswith('resnet'):\n    # ResNet has 'fc' as final layer\n    num_features = model_pretrained.fc.in_features\n    model_pretrained.fc = nn.Sequential(\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(num_features, 256),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(256, num_classes)\n    )\n    print(f\"Replaced ResNet classifier: {num_features} -> 256 -> {num_classes}\")\n    \nelif MODEL_NAME.startswith('efficientnet'):\n    # EfficientNet has 'classifier' as final layer\n    num_features = model_pretrained.classifier[1].in_features\n    model_pretrained.classifier = nn.Sequential(\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(num_features, 256),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(256, num_classes)\n    )\n    print(f\"Replaced EfficientNet classifier: {num_features} -> 256 -> {num_classes}\")\n    \nelif MODEL_NAME.startswith('vgg'):\n    # VGG has 'classifier' as a sequential module\n    num_features = model_pretrained.classifier[0].in_features\n    model_pretrained.classifier = nn.Sequential(\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(num_features, 512),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(512, 256),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(256, num_classes)\n    )\n    print(f\"Replaced VGG classifier: {num_features} -> 512 -> 256 -> {num_classes}\")\n\n# Move model to device FIRST\nmodel_pretrained = model_pretrained.to(device)\n\n# Then wrap with DataParallel if multiple GPUs are available\nif num_gpus > 1:\n    model_pretrained = nn.DataParallel(model_pretrained)\n    print(f\"Model wrapped with DataParallel for {num_gpus} GPUs\")\n\nprint(f\"\\nModel ready for transfer learning on {device} ({num_gpus} GPU(s))\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ea301fbb","cell_type":"code","source":"from torchsummary import summary\n\n# Display model architecture summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL SUMMARY\")\nprint(\"=\"*80)\nsummary(model_pretrained, input_size=input_shape)\n\n# Count trainable vs frozen parameters\ntotal_params = sum(p.numel() for p in model_pretrained.parameters())\ntrainable_params = sum(p.numel() for p in model_pretrained.parameters() if p.requires_grad)\nfrozen_params = total_params - trainable_params\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PARAMETER STATISTICS\")\nprint(\"=\"*80)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters (classifier only): {trainable_params:,}\")\nprint(f\"Frozen parameters (feature extractor): {frozen_params:,}\")\nprint(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"074d3174","cell_type":"code","source":"# Define optimizer - ONLY train classifier parameters (feature extractor is frozen)\n# Filter to get only parameters that require gradients (classifier layers)\ntrainable_params = filter(lambda p: p.requires_grad, model_pretrained.parameters())\noptimizer = torch.optim.AdamW(trainable_params, lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n\n# Enable mixed precision training for GPU acceleration\nscaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n\nprint(f\"✓ Optimizer configured to train only classifier layers\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Weight decay (L2): {L2_LAMBDA}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4268bf50","cell_type":"code","source":"# GPU Memory and Utilization Monitoring\nif torch.cuda.is_available():\n    print(\"\\n\" + \"=\"*80)\n    print(\"GPU STATUS BEFORE TRAINING\")\n    print(\"=\"*80)\n    for i in range(torch.cuda.device_count()):\n        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n        print(f\"  Memory Reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n        print(f\"  Max Memory Allocated: {torch.cuda.max_memory_allocated(i) / 1024**3:.2f} GB\")\n    print(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0d9f5f56","cell_type":"markdown","source":"# Training","metadata":{}},{"id":"d65a4724","cell_type":"code","source":"# Initialize best model tracking variables\nbest_model = None\nbest_performance = float('-inf')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ce7e4dd6","cell_type":"code","source":"def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n    \"\"\"\n    Perform one complete training epoch through the entire training dataset.\n\n    Args:\n        model (nn.Module): The neural network model to train\n        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n        l1_lambda (float): Lambda for L1 regularization\n        l2_lambda (float): Lambda for L2 regularization\n\n    Returns:\n        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n    \"\"\"\n    model.train()  # Set model to training mode\n\n    running_loss = 0.0\n    all_predictions = []\n    all_targets = []\n\n    # Iterate through training batches\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        # Move data to device (GPU/CPU)\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        # Clear gradients from previous step\n        optimizer.zero_grad(set_to_none=True)\n\n        # Forward pass with mixed precision (if CUDA available)\n        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n            logits = model(inputs)\n            loss = criterion(logits, targets)\n\n            # Add L1 and L2 regularization\n            l1_norm = sum(p.abs().sum() for p in model.parameters())\n            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n\n\n        # Backward pass with gradient scaling\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # Accumulate metrics\n        running_loss += loss.item() * inputs.size(0)\n        predictions = logits.argmax(dim=1)\n        all_predictions.append(predictions.cpu().numpy())\n        all_targets.append(targets.cpu().numpy())\n\n    # Calculate epoch metrics\n    epoch_loss = running_loss / len(train_loader.dataset)\n    epoch_f1 = f1_score(\n        np.concatenate(all_targets),\n        np.concatenate(all_predictions),\n        average='weighted'\n    )\n\n    return epoch_loss, epoch_f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"20b99110","cell_type":"code","source":"def validate_one_epoch(model, val_loader, criterion, device):\n    \"\"\"\n    Perform one complete validation epoch through the entire validation dataset.\n\n    Args:\n        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n        criterion (nn.Module): Loss function used to calculate validation loss\n        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n\n    Returns:\n        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n\n    Note:\n        This function automatically sets the model to evaluation mode and disables\n        gradient computation for efficiency during validation.\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n\n    running_loss = 0.0\n    all_predictions = []\n    all_targets = []\n\n    # Disable gradient computation for validation\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            # Move data to device\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            # Forward pass with mixed precision (if CUDA available)\n            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n                logits = model(inputs)\n                loss = criterion(logits, targets)\n\n            # Accumulate metrics\n            running_loss += loss.item() * inputs.size(0)\n            predictions = logits.argmax(dim=1)\n            all_predictions.append(predictions.cpu().numpy())\n            all_targets.append(targets.cpu().numpy())\n\n    # Calculate epoch metrics\n    epoch_loss = running_loss / len(val_loader.dataset)\n    epoch_accuracy = f1_score(\n        np.concatenate(all_targets),\n        np.concatenate(all_predictions),\n        average='weighted'\n    )\n\n    return epoch_loss, epoch_accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"05834891","cell_type":"code","source":"def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n    \"\"\"\n    Train the neural network model on the training data and validate on the validation data.\n\n    Args:\n        model (nn.Module): The neural network model to train\n        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n        epochs (int): Number of training epochs\n        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n        l1_lambda (float): L1 regularization coefficient (default: 0)\n        l2_lambda (float): L2 regularization coefficient (default: 0)\n        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n        verbose (int, optional): Frequency of printing training progress (default: 10)\n        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n\n    Returns:\n        tuple: (model, training_history) - Trained model and metrics history\n    \"\"\"\n\n    # Initialize metrics tracking\n    training_history = {\n        'train_loss': [], 'val_loss': [],\n        'train_f1': [], 'val_f1': []\n    }\n\n    # Configure early stopping if patience is set\n    if patience > 0:\n        patience_counter = 0\n        best_metric = float('-inf') if mode == 'max' else float('inf')\n        best_epoch = 0\n\n    print(f\"Training {epochs} epochs...\")\n\n    # Main training loop: iterate through epochs\n    for epoch in range(1, epochs + 1):\n\n        # Forward pass through training data, compute gradients, update weights\n        train_loss, train_f1 = train_one_epoch(\n            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n        )\n\n        # Evaluate model on validation data without updating weights\n        val_loss, val_f1 = validate_one_epoch(\n            model, val_loader, criterion, device\n        )\n\n        # Store metrics for plotting and analysis\n        training_history['train_loss'].append(train_loss)\n        training_history['val_loss'].append(val_loss)\n        training_history['train_f1'].append(train_f1)\n        training_history['val_f1'].append(val_f1)\n\n        # Print progress every N epochs or on first epoch\n        if verbose > 0:\n            if epoch % verbose == 0 or epoch == 1:\n                print(f\"Epoch {epoch:3d}/{epochs} | \"\n                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n\n        # Early stopping logic: monitor metric and save best model\n        if patience > 0:\n            current_metric = training_history[evaluation_metric][-1]\n            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n\n            if is_improvement:\n                best_metric = current_metric\n                best_epoch = epoch\n                torch.save(model.state_dict(),\"models/\"+experiment_name+'_model.pt')\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping triggered after {epoch} epochs.\")\n                    break\n\n    # Restore best model weights if early stopping was used\n    if restore_best_weights and patience > 0:\n        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n\n    # Save final model if no early stopping\n    if patience == 0:\n        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n\n    # Close TensorBoard writer\n    if writer is not None:\n        writer.close()\n\n    return model, training_history, best_metric","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"61d8b6f7","cell_type":"markdown","source":"## Fitting","metadata":{}},{"id":"50b0c487","cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# Set experiment name for this run\nEXPERIMENT_NAME = f\"pretrained_{MODEL_NAME}_augmented\"\n\n# Train with augmented (balanced) dataset\nprint(\"\\n\" + \"=\"*80)\nprint(f\"TRAINING WITH PRETRAINED {MODEL_NAME.upper()} - TRANSFER LEARNING\")\nprint(\"=\"*80)\nprint(f\"Train loader: {len(train_loader)} batches\")\nprint(f\"Val loader: {len(val_loader)} batches\")\nprint(f\"Strategy: Frozen feature extractor + Trainable classifier\")\nprint(\"=\"*80 + \"\\n\")\n\n# Train model and track training history using AUGMENTED dataset\nmodel_pretrained, history, best_perf = fit(\n    model=model_pretrained,\n    train_loader=train_loader,  # ← USE AUGMENTED LOADER\n    val_loader=val_loader,      # ← USE AUGMENTED LOADER\n    epochs=EPOCHS,\n    criterion=criterion,\n    optimizer=optimizer,\n    scaler=scaler,\n    device=device,\n    verbose=10,\n    experiment_name=EXPERIMENT_NAME,\n    patience=PATIENCE\n)\n\n# Update best model if current performance is superior\nif best_perf > best_performance:\n    best_model = model_pretrained    \n    best_performance = best_perf\n    print(f\"\\nNew best model saved with F1 Score: {best_performance:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bd146d25","cell_type":"markdown","source":"# Identify High-Loss Samples (Data Quality Check)","metadata":{}},{"id":"67724fa1","cell_type":"code","source":"def calculate_per_sample_loss(model, dataset, criterion, device):\n    \"\"\"\n    Calculate loss for each individual sample in the dataset.\n    \n    Returns:\n        losses: numpy array of per-sample losses\n        predictions: numpy array of predicted labels\n        targets: numpy array of true labels\n    \"\"\"\n    model.eval()\n    \n    losses = []\n    predictions = []\n    targets = []\n    \n    with torch.no_grad():\n        for i in range(len(dataset)):\n            inputs, target = dataset[i]\n            inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension\n            target_tensor = torch.tensor([target]).to(device)\n            \n            # Forward pass\n            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n                logits = model(inputs)\n                loss = criterion(logits, target_tensor)\n            \n            losses.append(loss.item())\n            predictions.append(logits.argmax(dim=1).cpu().item())\n            targets.append(target)\n    \n    return np.array(losses), np.array(predictions), np.array(targets)\n\nprint(\"Calculating per-sample losses on training set...\")\ntrain_losses, train_preds, train_targets = calculate_per_sample_loss(\n    model_pretrained, train_dataset, criterion, device\n)\n\nprint(f\"\\nLoss statistics:\")\nprint(f\"Mean loss: {train_losses.mean():.4f}\")\nprint(f\"Median loss: {np.median(train_losses):.4f}\")\nprint(f\"Max loss: {train_losses.max():.4f}\")\nprint(f\"Min loss: {train_losses.min():.4f}\")\nprint(f\"Std loss: {train_losses.std():.4f}\")\n\n# Identify high-loss samples\ntop_k = 50  # Number of worst samples to examine\nworst_indices = np.argsort(train_losses)[-top_k:][::-1]  # Highest losses first\n\nprint(f\"\\n{'='*80}\")\nprint(f\"TOP {top_k} HIGHEST LOSS SAMPLES (Potential Data Quality Issues)\")\nprint(f\"{'='*80}\")\nprint(f\"{'Index':<10} {'Loss':<12} {'True Label':<20} {'Predicted':<20} {'Correct':<10}\")\nprint('-' * 80)\n\nreverse_label_map = {v: k for k, v in label_map.items()}\nproblematic_samples = []\n\nfor rank, idx in enumerate(worst_indices, 1):\n    loss = train_losses[idx]\n    true_label = reverse_label_map[train_targets[idx]]\n    pred_label = reverse_label_map[train_preds[idx]]\n    is_correct = train_targets[idx] == train_preds[idx]\n    \n    problematic_samples.append({\n        'dataset_index': idx,\n        'loss': loss,\n        'true_label': true_label,\n        'predicted_label': pred_label,\n        'correct': is_correct\n    })\n    \n    if rank <= 20:  # Print top 20\n        print(f\"{idx:<10} {loss:<12.4f} {true_label:<20} {pred_label:<20} {str(is_correct):<10}\")\n\nprint('=' * 80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f773e6da","cell_type":"code","source":"# Visualize the worst samples\nfig, axes = plt.subplots(5, 5, figsize=(20, 20))\naxes = axes.flatten()\n\nprint(f\"\\nVisualizing top 25 highest-loss samples...\")\n\nfor i in range(min(25, len(worst_indices))):\n    idx = worst_indices[i]\n    loss = train_losses[idx]\n    true_label = reverse_label_map[train_targets[idx]]\n    pred_label = reverse_label_map[train_preds[idx]]\n    \n    # Get the image tensor\n    img_tensor, _ = train_dataset[idx]\n    \n    # Convert tensor to displayable image (C, H, W) -> (H, W, C)\n    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n    \n    # Display image\n    axes[i].imshow(img)\n    axes[i].set_title(\n        f\"Rank {i+1}: Loss={loss:.3f}\\n\"\n        f\"True: {true_label}\\n\"\n        f\"Pred: {pred_label}\",\n        fontsize=9,\n        color='red' if true_label != pred_label else 'green'\n    )\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.suptitle('Top 25 Highest Loss Training Samples', fontsize=16, y=1.001)\nplt.show()\n\n# Plot loss distribution\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\n# Histogram of losses\naxes[0].hist(train_losses, bins=100, color='steelblue', alpha=0.7, edgecolor='black')\naxes[0].axvline(train_losses.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {train_losses.mean():.3f}')\naxes[0].axvline(np.median(train_losses), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(train_losses):.3f}')\naxes[0].set_xlabel('Loss')\naxes[0].set_ylabel('Number of Samples')\naxes[0].set_title('Distribution of Per-Sample Losses')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# Sorted losses\nsorted_losses = np.sort(train_losses)\naxes[1].plot(sorted_losses, color='steelblue', linewidth=2)\naxes[1].axhline(train_losses.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {train_losses.mean():.3f}')\naxes[1].set_xlabel('Sample Rank (sorted)')\naxes[1].set_ylabel('Loss')\naxes[1].set_title('Sorted Per-Sample Losses')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a94029c4","cell_type":"markdown","source":"## Remove High-Loss Samples (Data Cleaning)","metadata":{}},{"id":"d53e0d33","cell_type":"code","source":"# Define threshold for removing high-loss samples\n# Option 1: Remove top N samples with highest loss\nREMOVE_TOP_N = 100  # Adjust this value based on visual inspection\n\n# Option 2: Remove samples above a certain loss percentile\nLOSS_PERCENTILE_THRESHOLD = 95  # Remove top 5% highest losses\n\n# Choose method (uncomment one)\nMETHOD = \"top_n\"  # Remove top N samples\n# METHOD = \"percentile\"  # Remove by percentile\n\nprint(f\"{'='*80}\")\nprint(f\"REMOVING HIGH-LOSS SAMPLES\")\nprint(f\"{'='*80}\")\n\nif METHOD == \"top_n\":\n    # First, we need to get the actual worst indices (not limited by top_k)\n    # Re-calculate worst_indices for removal (up to REMOVE_TOP_N)\n    all_worst_indices = np.argsort(train_losses)[::-1]  # All samples sorted by loss (highest first)\n    n_to_remove = min(REMOVE_TOP_N, len(train_losses))  # Don't try to remove more than available\n    samples_to_remove = all_worst_indices[:n_to_remove]\n    threshold_loss = train_losses[samples_to_remove[-1]]\n    print(f\"Method: Remove top {n_to_remove} samples\")\n    print(f\"Loss threshold: {threshold_loss:.4f}\")\nelse:\n    # Remove samples above percentile threshold\n    threshold_loss = np.percentile(train_losses, LOSS_PERCENTILE_THRESHOLD)\n    samples_to_remove = np.where(train_losses > threshold_loss)[0]\n    print(f\"Method: Remove samples above {LOSS_PERCENTILE_THRESHOLD}th percentile\")\n    print(f\"Loss threshold: {threshold_loss:.4f}\")\n\nprint(f\"Samples to remove: {len(samples_to_remove)}\")\nprint(f\"Original training set size: {len(train_dataset)}\")\nprint(f\"New training set size: {len(train_dataset) - len(samples_to_remove)}\")\n\n# Create mask for samples to keep\nkeep_mask = np.ones(len(train_dataset), dtype=bool)\nkeep_mask[samples_to_remove] = False\n\n# Filter the datasets\n# Create new indices list excluding samples to remove\nkeep_indices = [idx for idx in train_indices if idx not in samples_to_remove]\n\n# Filter using the keep_indices\ntrain_dataset_cleaned = Subset(full_dataset, keep_indices)\n\n# For creating TensorDataset (if needed for compatibility)\n# Load all kept samples into tensors\nX_train_cleaned = []\ny_train_cleaned = []\n\nprint(f\"Loading {len(keep_indices)} cleaned samples into memory...\")\nfor idx in keep_indices:\n    X, y = full_dataset[idx]\n    X_train_cleaned.append(X)\n    y_train_cleaned.append(y)\n\nX_train_cleaned = torch.stack(X_train_cleaned)\ny_train_cleaned = torch.tensor(y_train_cleaned, dtype=torch.long)\n\nprint(f\"\\nCleaned dataset shapes:\")\nprint(f\"X_train: {X_train_cleaned.shape}\")\nprint(f\"y_train: {y_train_cleaned.shape}\")\n# Check class distribution after cleaning\nprint(f\"\\nClass distribution after cleaning:\")\nunique, counts = np.unique(y_train_cleaned.cpu().numpy(), return_counts=True)\nfor label_idx, count in zip(unique, counts):\n    label_name = reverse_label_map[label_idx]\n    print(f\"  {label_name}: {count} samples\")\n\n# Create new cleaned DataLoader\ntrain_dataset_cleaned = TensorDataset(X_train_cleaned, y_train_cleaned)\n\ntrain_loader_kwargs = {\n    'batch_size': BATCH_SIZE,\n    'shuffle': True,\n    'num_workers': NUM_WORKERS,\n    'pin_memory': PIN_MEMORY\n}\nif NUM_WORKERS > 0:\n    train_loader_kwargs['persistent_workers'] = PERSISTENT_WORKERS\n\ntrain_loader_cleaned = DataLoader(train_dataset_cleaned, **train_loader_kwargs)\n\nprint(f\"\\nNew DataLoader created:\")\nprint(f\"Train batches: {len(train_loader_cleaned)}\")\nprint(f\"{'='*80}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"eec31b2a","cell_type":"markdown","source":"## Retrain with Cleaned Data","metadata":{}},{"id":"e11035fa","cell_type":"code","source":"import torch.nn as nn\n\n# Helper: ottieni il modello \"base\" anche se è DataParallel\nis_parallel = isinstance(model_pretrained, nn.DataParallel)\nbase_model = model_pretrained.module if is_parallel else model_pretrained\n\n# ===== STEP 1: Freeze all layers in the feature extractor =====\nfor param in base_model.parameters():\n    param.requires_grad = False\n\nprint(\"\\nAll feature extractor weights frozen\")\n\n# ===== STEP 2: Replace the classifier head =====\nif MODEL_NAME.startswith('resnet') or hasattr(base_model, \"fc\"):\n    num_features = base_model.fc.in_features\n    base_model.fc = nn.Sequential(\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(num_features, 256),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(256, num_classes)\n    )\n    print(f\"Replaced ResNet classifier: {num_features} -> 256 -> {num_classes}\")\n\nelif MODEL_NAME.startswith('efficientnet'):\n    num_features = base_model.classifier[1].in_features\n    base_model.classifier = nn.Sequential(\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(num_features, 256),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(256, num_classes)\n    )\n    print(f\"Replaced EfficientNet classifier: {num_features} -> 256 -> {num_classes}\")\n\nelif MODEL_NAME.startswith('vgg'):\n    num_features = base_model.classifier[0].in_features\n    base_model.classifier = nn.Sequential(\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(num_features, 512),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(512, 256),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(256, num_classes)\n    )\n    print(f\"Replaced VGG classifier: {num_features} -> 512 -> 256 -> {num_classes}\")\n\nelse:\n    raise ValueError(f\"Unsupported MODEL_NAME or unknown head structure: {MODEL_NAME}\")\n\n# ===== STEP 3: Move model to device =====\nbase_model = base_model.to(device)\n\n# ===== STEP 4: Re-wrap if needed =====\nif num_gpus > 1:\n    model_pretrained = nn.DataParallel(base_model)\n    print(f\"Model wrapped with DataParallel for {num_gpus} GPUs\")\nelse:\n    model_pretrained = base_model\n\nprint(f\"\\nModel ready for transfer learning on {device} ({num_gpus} GPU(s))\")\n\n\n# Reinitialize optimizer and scaler\noptimizer_cleaned = torch.optim.AdamW(model_pretrained.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\nscaler_cleaned = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n\n# Set experiment name for cleaned model\nEXPERIMENT_NAME_CLEANED = f\"{EXPERIMENT_NAME}_cleaned\"\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING WITH CLEANED DATASET (High-loss samples removed)\")\nprint(\"=\"*80)\nprint(f\"Train loader: {len(train_loader_cleaned)} batches\")\nprint(f\"Val loader: {len(val_loader)} batches (unchanged)\")\nprint(\"=\"*80 + \"\\n\")\n\n# Train model with cleaned data\nmodel_pretrained_cleaned, history_cleaned, best_perf = fit(\n    model=model_pretrained_cleaned,\n    train_loader=train_loader_cleaned,  # ← CLEANED LOADER\n    val_loader=val_loader,              # Validation set unchanged\n    epochs=EPOCHS,\n    criterion=criterion,\n    optimizer=optimizer_cleaned,\n    scaler=scaler_cleaned,\n    device=device,\n    verbose=10,\n    experiment_name=EXPERIMENT_NAME_CLEANED,\n    patience=PATIENCE\n)\n\n# Update best model if current performance is superior\nif best_perf > best_performance:\n    best_model = model_pretrained_cleaned\n    best_performance = best_perf\n    print(f\"\\n New best model saved with F1 Score: {best_performance:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c4f41e21","cell_type":"code","source":"# Compare original vs cleaned training\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Loss comparison\naxes[0, 0].plot(history['train_loss'], label='Original - Train', alpha=0.6, linestyle='--', color='#1f77b4')\naxes[0, 0].plot(history['val_loss'], label='Original - Val', alpha=0.8, color='#1f77b4')\naxes[0, 0].plot(history_cleaned['train_loss'], label='Cleaned - Train', alpha=0.6, linestyle='--', color='#ff7f0e')\naxes[0, 0].plot(history_cleaned['val_loss'], label='Cleaned - Val', alpha=0.8, color='#ff7f0e')\naxes[0, 0].set_title('Loss Comparison: Original vs Cleaned Data')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(alpha=0.3)\n\n# F1 Score comparison\naxes[0, 1].plot(history['train_f1'], label='Original - Train', alpha=0.6, linestyle='--', color='#1f77b4')\naxes[0, 1].plot(history['val_f1'], label='Original - Val', alpha=0.8, color='#1f77b4')\naxes[0, 1].plot(history_cleaned['train_f1'], label='Cleaned - Train', alpha=0.6, linestyle='--', color='#ff7f0e')\naxes[0, 1].plot(history_cleaned['val_f1'], label='Cleaned - Val', alpha=0.8, color='#ff7f0e')\naxes[0, 1].set_title('F1 Score Comparison: Original vs Cleaned Data')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('F1 Score')\naxes[0, 1].legend()\naxes[0, 1].grid(alpha=0.3)\n\n# Training loss only (zoomed)\naxes[1, 0].plot(history['train_loss'], label='Original', alpha=0.8, color='#1f77b4')\naxes[1, 0].plot(history_cleaned['train_loss'], label='Cleaned', alpha=0.8, color='#ff7f0e')\naxes[1, 0].set_title('Training Loss: Original vs Cleaned Data')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Training Loss')\naxes[1, 0].legend()\naxes[1, 0].grid(alpha=0.3)\n\n# Validation F1 only (zoomed)\naxes[1, 1].plot(history['val_f1'], label='Original', alpha=0.8, color='#1f77b4', marker='o')\naxes[1, 1].plot(history_cleaned['val_f1'], label='Cleaned', alpha=0.8, color='#ff7f0e', marker='s')\naxes[1, 1].set_title('Validation F1 Score: Original vs Cleaned Data')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Validation F1 Score')\naxes[1, 1].legend()\naxes[1, 1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print summary comparison\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING COMPARISON SUMMARY\")\nprint(\"=\"*80)\nprint(f\"\\nOriginal Dataset:\")\nprint(f\"  Best Val F1: {max(history['val_f1']):.4f}\")\nprint(f\"  Final Val F1: {history['val_f1'][-1]:.4f}\")\nprint(f\"  Final Train Loss: {history['train_loss'][-1]:.4f}\")\nprint(f\"  Final Val Loss: {history['val_loss'][-1]:.4f}\")\n\nprint(f\"\\nCleaned Dataset (removed {len(samples_to_remove)} high-loss samples):\")\nprint(f\"  Best Val F1: {max(history_cleaned['val_f1']):.4f}\")\nprint(f\"  Final Val F1: {history_cleaned['val_f1'][-1]:.4f}\")\nprint(f\"  Final Train Loss: {history_cleaned['train_loss'][-1]:.4f}\")\nprint(f\"  Final Val Loss: {history_cleaned['val_loss'][-1]:.4f}\")\n\nimprovement = max(history_cleaned['val_f1']) - max(history['val_f1'])\nprint(f\"\\nImprovement: {improvement:+.4f} ({improvement*100:+.2f}%)\")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5829b683","cell_type":"markdown","source":"## Plotting","metadata":{}},{"id":"fbe5e407","cell_type":"code","source":"import seaborn as sns\n\n# Get validation predictions\nval_preds = []\nval_targets = []\nbest_model.eval()\n\nwith torch.no_grad():\n    for inputs, targets in val_loader:\n        inputs = inputs.to(device)\n        logits = best_model(inputs)\n        preds = logits.argmax(dim=1).cpu().numpy()\n        \n        val_preds.append(preds)\n        val_targets.append(targets.numpy())\n\nval_preds = np.concatenate(val_preds)\nval_targets = np.concatenate(val_targets)\n\n# Calculate overall validation set metrics\nval_acc = accuracy_score(val_targets, val_preds)\nval_prec = precision_score(val_targets, val_preds, average='weighted')\nval_rec = recall_score(val_targets, val_preds, average='weighted')\nval_f1 = f1_score(val_targets, val_preds, average='weighted')\n\nprint(f\"Accuracy over the validation set: {val_acc:.4f}\")\nprint(f\"Precision over the validation set: {val_prec:.4f}\")\nprint(f\"Recall over the validation set: {val_rec:.4f}\")\nprint(f\"F1 score over the validation set: {val_f1:.4f}\")\n\n# Generate confusion matrix\ncm = confusion_matrix(val_targets, val_preds)\nlabels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n\n# Visualize confusion matrix\nplt.figure(figsize=(8, 7))\nsns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.title('Confusion Matrix — Validation Set')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"90c0b71c","cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Create a figure with two side-by-side subplots (two columns)\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n\n# Plot of training and validation loss on the first axis\nax1.plot(history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\nax1.plot(history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\nax1.set_title('Loss')\nax1.legend()\nax1.grid(alpha=0.3)\n\n# Plot of training and validation accuracy on the second axis\nax2.plot(history['train_f1'], label='Training f1', alpha=0.3, color='#ff7f0e', linestyle='--')\nax2.plot(history['val_f1'], label='Validation f1', alpha=0.9, color='#ff7f0e')\nax2.set_title('F1 Score')\nax2.legend()\nax2.grid(alpha=0.3)\n\n# Adjust the layout and display the plot\nplt.tight_layout()\nplt.subplots_adjust(right=0.85)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ea326ad9","cell_type":"markdown","source":"# Inference","metadata":{}},{"id":"e336b736-e6db-4b1f-aae1-8b68c9c2e8a6","cell_type":"markdown","source":"## Emergency load weights if lost variables","metadata":{}},{"id":"d0032ab7-9d6b-43dc-ac5a-1657313b11a8","cell_type":"code","source":"model_weights_name = 'pretrained_resnet18_augmented_model.pt'\nimport torchvision.models as models\nfrom torchvision.models import ResNet18_Weights, ResNet50_Weights, EfficientNet_B0_Weights, EfficientNet_B3_Weights, VGG16_Weights\nimport torch.nn as nn\n\n# Number of training epochs\nLEARNING_RATE = 1e-3\nEPOCHS = 300\nPATIENCE = 50\n\n# Regularisation\nDROPOUT_RATE = 0.5       # Dropout probability\nL1_LAMBDA = 0.001           # L1 penalty\nL2_LAMBDA = 0.01         # L2 penalty\n\n# Set up loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\n\ncheckpoint = torch.load(f\"models/{model_weights_name}\", map_location=device)\n\n# Remove \"module.\" prefix if present\nfrom collections import OrderedDict\nnew_state_dict = OrderedDict()\n\nfor k, v in checkpoint.items():\n    name = k.replace(\"module.\", \"\")  # remove module.\n    new_state_dict[name] = v\n# ===== UNCOMMENT THE MODEL YOU WANT TO USE =====\n\n# ResNet-18 (Smaller, faster)\nbest_model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\nMODEL_NAME = \"resnet18\"\n\n# ResNet-50 (Deeper, more powerful)\n# model_pretrained = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n# MODEL_NAME = \"resnet50\"\n\n# EfficientNet-B0 (Efficient, good balance)\n# model_pretrained = models.efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n# MODEL_NAME = \"efficientnet_b0\"\n\n# EfficientNet-B3 (More powerful EfficientNet)\n# model_pretrained = models.efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n# MODEL_NAME = \"efficientnet_b3\"\n\n# VGG-16 (Classic architecture)\n# model_pretrained = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n# MODEL_NAME = \"vgg16\"\n\n# ===============================================\n\nprint(f\"Loaded pretrained model: {MODEL_NAME}\")\nprint(f\"Model architecture:\\n{best_model}\")\n\n# ===== STEP 1: Freeze all layers in the feature extractor =====\nfor param in best_model.parameters():\n    param.requires_grad = False\n\nprint(\"\\nAll feature extractor weights frozen\")\n\n# ===== STEP 2: Replace the classifier head =====\n# Different models have different classifier layer names\nif MODEL_NAME.startswith('resnet'):\n    # ResNet has 'fc' as final layer\n    num_features = best_model.fc.in_features\n    best_model.fc = nn.Sequential(\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(num_features, 256),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(256, num_classes)\n    )\n    print(f\"Replaced ResNet classifier: {num_features} -> 256 -> {num_classes}\")\n    \nelif MODEL_NAME.startswith('efficientnet'):\n    # EfficientNet has 'classifier' as final layer\n    num_features = best_model.classifier[1].in_features\n    best_model.classifier = nn.Sequential(\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(num_features, 256),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(256, num_classes)\n    )\n    print(f\"Replaced EfficientNet classifier: {num_features} -> 256 -> {num_classes}\")\n    \nelif MODEL_NAME.startswith('vgg'):\n    # VGG has 'classifier' as a sequential module\n    num_features = best_model.classifier[0].in_features\n    best_model.classifier = nn.Sequential(\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(num_features, 512),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(512, 256),\n        nn.ReLU(),\n        nn.Dropout(DROPOUT_RATE),\n        nn.Linear(256, num_classes)\n    )\n    print(f\"Replaced VGG classifier: {num_features} -> 512 -> 256 -> {num_classes}\")\n\nbest_model.load_state_dict(new_state_dict)\n\n# Move model to device FIRST\nbest_model = best_model.to(device)\n\n# Then wrap with DataParallel if multiple GPUs are available\nif num_gpus > 1:\n    best_model = nn.DataParallel(best_model)\n    print(f\"Model wrapped with DataParallel for {num_gpus} GPUs\")\n\nEXPERIMENT_NAME = MODEL_NAME+'_recovered'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9229c1f4","cell_type":"code","source":"# Collect patch predictions and aggregate to image-level predictions\npatch_preds = []\npatch_probs = []  # Store probabilities for soft voting\nbest_model.eval()\n\nprint(f\"Running inference on {len(test_dataset)} test patches...\")\nprint(f\"Will aggregate to {len(test_filenames)} images\")\n\nwith torch.no_grad():\n    for batch in test_loader:\n        xb = batch.to(device)\n        \n        # Forward pass\n        logits = best_model(xb)\n        probs = torch.softmax(logits, dim=1).cpu().numpy()  # Get probabilities\n        preds = logits.argmax(dim=1).cpu().numpy()  # Get hard predictions\n        \n        patch_preds.append(preds)\n        patch_probs.append(probs)\n\n# Combine all patch predictions\npatch_preds = np.concatenate(patch_preds)\npatch_probs = np.concatenate(patch_probs)\n\nprint(f\"Got {len(patch_preds)} patch predictions\")\n\n# Aggregate patches to image-level predictions using soft voting (average probabilities)\nimage_preds = []\nfor img_idx in range(len(test_filenames)):\n    # Find all patches belonging to this image\n    patch_indices = [i for i, img_id in enumerate(test_patch_to_image) if img_id == img_idx]\n    \n    # Get probabilities for all patches of this image\n    image_patch_probs = patch_probs[patch_indices]\n    \n    # Average probabilities across patches (soft voting)\n    avg_probs = image_patch_probs.mean(axis=0)\n    \n    # Final prediction is class with highest average probability\n    final_pred = avg_probs.argmax()\n    image_preds.append(final_pred)\n\ntest_preds = np.array(image_preds)\nprint(f\"Aggregated to {len(test_preds)} image predictions\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"443b3d5d","cell_type":"code","source":"# Create reverse label mapping\nreverse_label_map = {v: k for k, v in label_map.items()}\n\ntest_filenames = [fn.replace('mask', 'img') for fn in test_filenames]\n\n# Create submission dataframe\nsubmission_df = pd.DataFrame({\n    'sample_index': test_filenames,\n    'label': [reverse_label_map[pred] for pred in test_preds]\n})\n\n# Create descriptive filename with all hyperparameters\nfilename_parts = [\n    f\"submission_{EXPERIMENT_NAME}\",\n    f\"focus_filter\",\n    f\"bs_{BATCH_SIZE}\",\n    f\"lr_{LEARNING_RATE}\",\n    f\"drop_{DROPOUT_RATE}\",\n    f\"l1_{L1_LAMBDA}\",\n    f\"l2_{L2_LAMBDA}\",\n    f\"epochs_{EPOCHS}\",\n    f\"patience_{PATIENCE}\",\n    f\"imgsize_{IMG_SIZE[0]}x{IMG_SIZE[1]}\"\n]\nsubmission_filename = \"_\".join(filename_parts) + \".csv\"\n\n# Save to CSV\nsubmission_df.to_csv(submission_filename, index=False)\nprint(f\"Submission file created: {submission_filename}\")\nprint(f\"Total predictions: {len(submission_df)}\")\nprint(\"\\nFirst few predictions:\")\nprint(submission_df.head(10))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}